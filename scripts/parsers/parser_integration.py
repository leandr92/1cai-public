#!/usr/bin/env python3
"""
Parser Integration Module
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã –≤ –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
- OptimizedXMLParser –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
- BSLASTParser –¥–ª—è BSL –∫–æ–¥–∞
- –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Redis
- –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥

–í–µ—Ä—Å–∏—è: 1.0.0
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Iterator
from datetime import datetime
import asyncio

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    from scripts.parsers.optimized_xml_parser import OptimizedXMLParser
    from scripts.parsers.bsl_ast_parser import BSLASTParser, BSLLanguageServerClient
    from src.services.configuration_knowledge_base import get_knowledge_base
except ImportError as e:
    print(f"[ERROR] Import error: {e}")
    sys.exit(1)

# Optional: Redis –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("[WARN] Redis not available, caching disabled")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class IntegratedParser:
    """
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –≤—Å–µ–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
    
    Features:
    - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π XML –ø–∞—Ä—Å–∏–Ω–≥ (lxml streaming)
    - AST –ø–∞—Ä—Å–∏–Ω–≥ BSL –∫–æ–¥–∞
    - –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
    - Redis –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    """
    
    def __init__(
        self,
        use_ast: bool = True,
        use_redis: bool = True,
        incremental: bool = True,
        redis_url: str = "redis://localhost:6380"
    ):
        """
        Args:
            use_ast: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AST –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è BSL
            use_redis: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Redis –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
            incremental: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
            redis_url: URL Redis —Å–µ—Ä–≤–µ—Ä–∞
        """
        # XML Parser
        self.xml_parser = OptimizedXMLParser(enable_incremental=incremental)
        
        # BSL AST Parser
        self.use_ast = use_ast
        if use_ast:
            try:
                self.bsl_parser = BSLASTParser(use_language_server=True)
                logger.info("‚úÖ AST –ø–∞—Ä—Å–∏–Ω–≥ –≤–∫–ª—é—á–µ–Ω")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è AST –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                logger.warning("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback regex parser")
                self.use_ast = False
                from scripts.parsers.improve_bsl_parser import ImprovedBSLParser
                self.bsl_parser = ImprovedBSLParser()
        else:
            from scripts.parsers.improve_bsl_parser import ImprovedBSLParser
            self.bsl_parser = ImprovedBSLParser()
        
        # Redis Cache
        self.use_redis = use_redis and REDIS_AVAILABLE
        self.redis_client = None
        
        if self.use_redis:
            try:
                self.redis_client = redis.from_url(redis_url, decode_responses=True)
                self.redis_client.ping()
                logger.info("‚úÖ Redis –∫–µ—à –¥–æ—Å—Ç—É–ø–µ–Ω")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self.use_redis = False
        
        # Stats
        self.stats = {
            'total_configs': 0,
            'total_modules': 0,
            'total_functions': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'parse_time': 0.0
        }
        
        # Knowledge Base
        self.kb = get_knowledge_base()
    
    async def parse_all_configurations(
        self,
        config_dir: Path = None,
        parallel: bool = True
    ) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
        
        Args:
            config_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏
            parallel: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
        """
        if config_dir is None:
            config_dir = Path("./1c_configurations")
        
        logger.info("=" * 70)
        logger.info("INTEGRATED PARSER - OPTIMIZED MODE")
        logger.info("=" * 70)
        logger.info(f"AST Parsing: {'‚úÖ Enabled' if self.use_ast else '‚ùå Disabled'}")
        logger.info(f"Redis Cache: {'‚úÖ Enabled' if self.use_redis else '‚ùå Disabled'}")
        logger.info(f"Incremental: {'‚úÖ Enabled' if self.xml_parser.enable_incremental else '‚ùå Disabled'}")
        logger.info(f"Parallel: {'‚úÖ Enabled' if parallel else '‚ùå Disabled'}")
        logger.info("=" * 70)
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_files = list(config_dir.rglob("config.xml"))
        logger.info(f"\nüìÅ –ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {len(config_files)}")
        
        start_time = datetime.now()
        
        if parallel and len(config_files) > 1:
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            results = await self._parse_configs_parallel(config_files)
        else:
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            results = await self._parse_configs_sequential(config_files)
        
        total_time = (datetime.now() - start_time).total_seconds()
        self.stats['parse_time'] = total_time
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö–µ—à–∏ –¥–ª—è incremental parsing
        if self.xml_parser.enable_incremental:
            self.xml_parser._save_hashes()
        
        # –ò—Ç–æ–≥–∏
        logger.info("\n" + "=" * 70)
        logger.info("–ò–¢–û–ì–ò –ü–ê–†–°–ò–ù–ì–ê:")
        logger.info("=" * 70)
        logger.info(f"–í—Ä–µ–º—è: {total_time:.1f} —Å–µ–∫ ({total_time/60:.1f} –º–∏–Ω)")
        logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {self.stats['total_configs']}")
        logger.info(f"–ú–æ–¥—É–ª–µ–π: {self.stats['total_modules']}")
        logger.info(f"–§—É–Ω–∫—Ü–∏–π: {self.stats['total_functions']}")
        
        if self.use_redis:
            logger.info(f"Cache hits: {self.stats['cache_hits']}")
            logger.info(f"Cache misses: {self.stats['cache_misses']}")
            hit_rate = self.stats['cache_hits'] / max(1, self.stats['cache_hits'] + self.stats['cache_misses']) * 100
            logger.info(f"Cache hit rate: {hit_rate:.1f}%")
        
        logger.info(f"–°–∫–æ—Ä–æ—Å—Ç—å: {self.stats['total_modules']/total_time:.1f} –º–æ–¥—É–ª–µ–π/—Å–µ–∫")
        logger.info("=" * 70)
        
        return {
            'status': 'success',
            'stats': self.stats,
            'results': results
        }
    
    async def _parse_configs_parallel(
        self,
        config_files: List[Path]
    ) -> List[Dict[str, Any]]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
        
        logger.info("üîÑ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞...")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
        tasks = []
        for config_file in config_files:
            config_name = config_file.parent.name.upper()
            task = self._parse_single_config(config_name, config_file)
            tasks.append(task)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –æ—à–∏–±–∫–∏
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {config_files[i].name}: {result}")
            else:
                valid_results.append(result)
        
        return valid_results
    
    async def _parse_configs_sequential(
        self,
        config_files: List[Path]
    ) -> List[Dict[str, Any]]:
        """–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
        
        logger.info("‚û°Ô∏è  –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞...")
        
        results = []
        for config_file in config_files:
            config_name = config_file.parent.name.upper()
            result = await self._parse_single_config(config_name, config_file)
            results.append(result)
        
        return results
    
    async def _parse_single_config(
        self,
        config_name: str,
        config_file: Path
    ) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        
        logger.info(f"\n{'='*70}")
        logger.info(f"üì¶ {config_name}")
        logger.info(f"{'='*70}")
        
        modules_saved = 0
        modules_skipped = 0
        
        # Streaming –ø–∞—Ä—Å–∏–Ω–≥ XML
        for module in self.xml_parser.parse_configuration_streaming(config_name, config_file):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
            if self.use_redis:
                cached = self._get_from_cache(module['name'])
                if cached:
                    self.stats['cache_hits'] += 1
                    modules_skipped += 1
                    continue
                else:
                    self.stats['cache_misses'] += 1
            
            # –ü–∞—Ä—Å–∏–º BSL —Å AST (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
            if self.use_ast and module.get('code'):
                try:
                    ast_result = self.bsl_parser.parse(module['code'])
                    
                    # –û–±–æ–≥–∞—â–∞–µ–º –º–æ–¥—É–ª—å AST –¥–∞–Ω–Ω—ã–º–∏
                    module['ast'] = ast_result.get('ast')
                    module['control_flow'] = ast_result.get('control_flow')
                    module['data_flow'] = ast_result.get('data_flow')
                    module['complexity'] = ast_result.get('complexity')
                    module['diagnostics'] = ast_result.get('diagnostics', [])
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ —Å AST
                    module['functions'] = ast_result.get('functions', module.get('functions', []))
                    module['procedures'] = ast_result.get('procedures', module.get('procedures', []))
                    
                except Exception as e:
                    logger.warning(f"AST –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è –¥–ª—è {module['name']}: {e}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
            try:
                self.xml_parser.save_module_to_kb(module, config_name)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
                if self.use_redis:
                    self._save_to_cache(module['name'], module)
                
                modules_saved += 1
                self.stats['total_modules'] += 1
                self.stats['total_functions'] += module.get('functions_count', 0)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥—É–ª—è {module['name']}: {e}")
        
        self.stats['total_configs'] += 1
        
        logger.info(f"‚úÖ {config_name}: {modules_saved} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ, {modules_skipped} –ø—Ä–æ–ø—É—â–µ–Ω–æ")
        
        return {
            'config_name': config_name,
            'modules_saved': modules_saved,
            'modules_skipped': modules_skipped
        }
    
    def _get_from_cache(self, module_name: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑ Redis –∫–µ—à–∞"""
        if not self.use_redis or not self.redis_client:
            return None
        
        try:
            cached = self.redis_client.get(f"module:{module_name}")
            if cached:
                return json.loads(cached)
        except Exception as e:
            logger.warning(f"Cache read error: {e}")
        
        return None
    
    def _save_to_cache(self, module_name: str, module_data: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Redis –∫–µ—à"""
        if not self.use_redis or not self.redis_client:
            return
        
        try:
            # –£–±–∏—Ä–∞–µ–º AST –∏–∑ –∫–µ—à–∞ (—Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π)
            cache_data = module_data.copy()
            cache_data.pop('ast', None)
            cache_data.pop('control_flow', None)
            cache_data.pop('data_flow', None)
            
            # TTL 1 –¥–µ–Ω—å
            self.redis_client.setex(
                f"module:{module_name}",
                86400,
                json.dumps(cache_data, ensure_ascii=False)
            )
        except Exception as e:
            logger.warning(f"Cache write error: {e}")


async def main():
    """Main entry point"""
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä
    parser = IntegratedParser(
        use_ast=True,
        use_redis=True,
        incremental=True
    )
    
    # –ü–∞—Ä—Å–∏–º –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    result = await parser.parse_all_configurations(parallel=True)
    
    if result['status'] == 'success':
        print("\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ")
    else:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {result.get('error')}")


if __name__ == "__main__":
    asyncio.run(main())




