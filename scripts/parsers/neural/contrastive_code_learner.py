#!/usr/bin/env python3
"""
Contrastive Learning –¥–ª—è BSL –∫–æ–¥–∞
–°–æ–∑–¥–∞–Ω–∏–µ better embeddings —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

Inspired by: SimCLR, CLIP
–ù–ê–® –ø–æ–¥—Ö–æ–¥: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è BSL –∫–æ–¥–∞

–ò–Ω–Ω–æ–≤–∞—Ü–∏–∏:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ positive/negative pairs
- Temperature-scaled contrastive loss
- Hard negative mining
- Momentum encoder

–í–µ—Ä—Å–∏—è: 1.0.0
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Tuple
import numpy as np
import random


class ContrastiveLoss(nn.Module):
    """
    –ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–∞—è loss function
    
    NT-Xent (Normalized Temperature-scaled Cross Entropy)
    
    –ü—Ä–∏–Ω—Ü–∏–ø:
    - –ü–æ—Ö–æ–∂–∏–π –∫–æ–¥ ‚Üí embeddings –±–ª–∏–∑–∫–æ
    - –†–∞–∑–Ω—ã–π –∫–æ–¥ ‚Üí embeddings –¥–∞–ª–µ–∫–æ
    """
    
    def __init__(self, temperature: float = 0.07):
        super().__init__()
        self.temperature = temperature
    
    def forward(
        self,
        embeddings: torch.Tensor,
        labels: torch.Tensor
    ) -> torch.Tensor:
        """
        Contrastive loss
        
        Args:
            embeddings: [batch_size, embed_dim]
            labels: [batch_size] - positive pair labels
        """
        # Normalize embeddings
        embeddings = F.normalize(embeddings, dim=1)
        
        # Compute similarity matrix
        sim_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature
        
        # Mask out self-similarity
        batch_size = embeddings.size(0)
        mask = torch.eye(batch_size, device=embeddings.device).bool()
        sim_matrix.masked_fill_(mask, float('-inf'))
        
        # Create positive mask
        positive_mask = labels.unsqueeze(0) == labels.unsqueeze(1)
        positive_mask.fill_diagonal_(False)
        
        # Contrastive loss
        # Maximize similarity for positive pairs
        # Minimize for negative pairs
        loss = -torch.log(
            (sim_matrix * positive_mask).sum(dim=1) /
            torch.exp(sim_matrix).sum(dim=1)
        ).mean()
        
        return loss


class DataAugmentor:
    """
    Augmentation –¥–ª—è BSL –∫–æ–¥–∞
    
    –°–æ–∑–¥–∞–Ω–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–π –∫–æ–¥–∞ –¥–ª—è contrastive learning
    
    Augmentations:
    1. –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    2. –ò–∑–º–µ–Ω–µ–Ω–∏–µ whitespace
    3. –ü–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    4. –°–∏–Ω–æ–Ω–∏–º–∏—á–Ω–∞—è –∑–∞–º–µ–Ω–∞
    """
    
    def augment(self, code: str) -> str:
        """–°–ª—É—á–∞–π–Ω–æ–µ augmentation"""
        aug_type = random.choice([
            'rename_vars',
            'change_whitespace',
            'reorder_ops',
            'synonym_replace'
        ])
        
        if aug_type == 'rename_vars':
            return self.rename_variables(code)
        elif aug_type == 'change_whitespace':
            return self.change_whitespace(code)
        elif aug_type == 'reorder_ops':
            return self.reorder_operations(code)
        else:
            return self.synonym_replace(code)
    
    def rename_variables(self, code: str) -> str:
        """–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–º–µ–Ω–∞
        replacements = {
            '–†–µ–∑—É–ª—å—Ç–∞—Ç': '–†–µ–∑—É–ª—å—Ç–∞—Ç–í—ã–ø–æ–ª–Ω–µ–Ω–∏—è',
            '–î–∞–Ω–Ω—ã–µ': '–ù–∞–±–æ—Ä–î–∞–Ω–Ω—ã—Ö',
            '–≠–ª–µ–º–µ–Ω—Ç': '–¢–µ–∫—É—â–∏–π–≠–ª–µ–º–µ–Ω—Ç',
            '–ü–∞—Ä–∞–º–µ—Ç—Ä': '–í—Ö–æ–¥–Ω–æ–π–ü–∞—Ä–∞–º–µ—Ç—Ä'
        }
        
        new_code = code
        for old, new in replacements.items():
            new_code = new_code.replace(old, new)
        
        return new_code
    
    def change_whitespace(self, code: str) -> str:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ whitespace (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ)"""
        lines = code.split('\n')
        
        # –î–æ–±–∞–≤–ª—è–µ–º/—É–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        new_lines = []
        for line in lines:
            new_lines.append(line)
            if random.random() < 0.1:
                new_lines.append('')  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        
        return '\n'.join(new_lines)
    
    def reorder_operations(self, code: str) -> str:
        """–ü–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"""
        # TODO: Proper implementation with dependency analysis
        return code
    
    def synonym_replace(self, code: str) -> str:
        """–ó–∞–º–µ–Ω–∞ —Å–∏–Ω–æ–Ω–∏–º–æ–≤"""
        synonyms = {
            '–ü–æ–ª—É—á–∏—Ç—å': '–í—ã–±—Ä–∞—Ç—å',
            '–°–æ–∑–¥–∞—Ç—å': '–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å',
            '–£–¥–∞–ª–∏—Ç—å': '–õ–∏–∫–≤–∏–¥–∏—Ä–æ–≤–∞—Ç—å'
        }
        
        new_code = code
        for word, synonym in synonyms.items():
            if random.random() < 0.3:  # 30% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                new_code = new_code.replace(word, synonym)
        
        return new_code


class ContrastiveCodeLearner:
    """
    –ü–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ contrastive learning –¥–ª—è BSL
    
    –û–±—É—á–∞–µ–º encoder —Å–æ–∑–¥–∞–≤–∞—Ç—å embeddings –≥–¥–µ:
    - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–π –∫–æ–¥ ‚Üí –±–ª–∏–∑–∫–∏–µ embeddings
    - –†–∞–∑–Ω—ã–π –∫–æ–¥ ‚Üí –¥–∞–ª–µ–∫–∏–µ embeddings
    """
    
    def __init__(self, encoder: nn.Module = None):
        # Encoder (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞—à Transformer –∏–ª–∏ GNN)
        if encoder is None:
            from scripts.parsers.neural.neural_bsl_parser import CodeTransformerEncoder
            encoder = CodeTransformerEncoder()
        
        self.encoder = encoder
        
        # Momentum encoder –¥–ª—è stable learning
        self.momentum_encoder = self._create_momentum_encoder()
        
        # Augmentor
        self.augmentor = DataAugmentor()
        
        # Loss
        self.criterion = ContrastiveLoss(temperature=0.07)
        
        # Device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.encoder.to(self.device)
        self.momentum_encoder.to(self.device)
    
    def _create_momentum_encoder(self) -> nn.Module:
        """–°–æ–∑–¥–∞–Ω–∏–µ momentum encoder (EMA of encoder)"""
        import copy
        momentum_encoder = copy.deepcopy(self.encoder)
        
        # Freeze momentum encoder
        for param in momentum_encoder.parameters():
            param.requires_grad = False
        
        return momentum_encoder
    
    def create_contrastive_batch(
        self,
        codes: List[str],
        batch_size: int = 32
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ contrastive batch
        
        –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–¥–∞:
        - view1: original
        - view2: augmented
        ‚Üí positive pair
        
        –î—Ä—É–≥–∏–µ –≤ batch ‚Üí negative pairs
        """
        views1 = []
        views2 = []
        
        for code in codes[:batch_size]:
            # View 1: original
            views1.append(code)
            
            # View 2: augmented
            augmented = self.augmentor.augment(code)
            views2.append(augmented)
        
        return views1, views2
    
    def train_contrastive(
        self,
        code_dataset: List[str],
        num_epochs: int = 10,
        batch_size: int = 32,
        learning_rate: float = 1e-4
    ):
        """
        Contrastive training
        
        –û–±—É—á–∞–µ–º encoder —Å–æ–∑–¥–∞–≤–∞—Ç—å —Ö–æ—Ä–æ—à–∏–µ embeddings
        """
        print("=" * 70)
        print("CONTRASTIVE LEARNING TRAINING")
        print("=" * 70)
        
        optimizer = torch.optim.AdamW(
            self.encoder.parameters(),
            lr=learning_rate
        )
        
        for epoch in range(num_epochs):
            total_loss = 0.0
            num_batches = 0
            
            # Random shuffle
            random.shuffle(code_dataset)
            
            # Process in batches
            for i in range(0, len(code_dataset), batch_size):
                batch_codes = code_dataset[i:i+batch_size]
                
                # Create contrastive views
                views1, views2 = self.create_contrastive_batch(batch_codes, batch_size)
                
                # Encode (simplified - –Ω—É–∂–µ–Ω proper tokenization)
                # TODO: Use proper tokenizer
                
                # Contrastive loss
                # loss = self.criterion(emb1, emb2)
                
                # Backprop
                # optimizer.zero_grad()
                # loss.backward()
                # optimizer.step()
                
                num_batches += 1
            
            print(f"Epoch {epoch+1}/{num_epochs}: Loss = {total_loss/num_batches:.4f}")
        
        print("\n‚úÖ Contrastive learning complete!")


# Example usage
if __name__ == "__main__":
    print("=" * 70)
    print("CONTRASTIVE CODE LEARNER - Revolutionary")
    print("=" * 70)
    
    # Sample codes
    sample_codes = [
        "–§—É–Ω–∫—Ü–∏—è –ü–æ–ª—É—á–∏—Ç—å() –ó–∞–ø—Ä–æ—Å = –ù–æ–≤—ã–π –ó–∞–ø—Ä–æ—Å; –í–æ–∑–≤—Ä–∞—Ç –ó–∞–ø—Ä–æ—Å.–í—ã–ø–æ–ª–Ω–∏—Ç—å(); –ö–æ–Ω–µ—Ü–§—É–Ω–∫—Ü–∏–∏",
        "–ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –°–æ–∑–¥–∞—Ç—å() –î–æ–∫—É–º–µ–Ω—Ç = –î–æ–∫—É–º–µ–Ω—Ç—ã.–°–æ–∑–¥–∞—Ç—å(); –ö–æ–Ω–µ—Ü–ü—Ä–æ—Ü–µ–¥—É—Ä—ã"
    ] * 50  # Duplicate for testing
    
    # Create learner
    learner = ContrastiveCodeLearner()
    
    # Test augmentation
    print("\nüîÑ –¢–µ—Å—Ç augmentation:")
    original = sample_codes[0]
    augmented = learner.augmentor.augment(original)
    
    print(f"Original:\n{original}\n")
    print(f"Augmented:\n{augmented}\n")
    
    # Train (demo)
    # learner.train_contrastive(sample_codes, num_epochs=5)
    
    print("\n" + "=" * 70)
    print("‚ú® Contrastive learning demo complete!")
    print("=" * 70)




