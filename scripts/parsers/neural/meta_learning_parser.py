#!/usr/bin/env python3
"""
Meta-Learning Parser –¥–ª—è BSL
–ë—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤—ã–º –ø—Ä–æ–µ–∫—Ç–∞–º

–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è: MAML (Model-Agnostic Meta-Learning)
–ò–Ω–Ω–æ–≤–∞—Ü–∏—è: Few-shot –ø–∞—Ä—Å–∏–Ω–≥ (5-10 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏!)

Use case:
- –ù–æ–≤—ã–π –∫–ª–∏–µ–Ω—Ç —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º —Å—Ç–∏–ª–µ–º –∫–æ–¥–∞
- 10 –ø—Ä–∏–º–µ—Ä–æ–≤ ‚Üí –ø–æ–ª–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∑–∞ –º–∏–Ω—É—Ç—ã
- –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä

–í–µ—Ä—Å–∏—è: 1.0.0 Revolutionary
"""

import torch
import torch.nn as nn
import torch.optim as optim
from typing import List, Dict, Any, Tuple
import copy
from collections import defaultdict


class ParsingTask:
    """
    –ó–∞–¥–∞—á–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è meta-learning
    
    –°–æ—Å—Ç–æ–∏—Ç –∏–∑:
    - Support set: –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ (K-shot)
    - Query set: –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
    """
    
    def __init__(
        self,
        support_codes: List[str],
        support_labels: List[Dict],
        query_codes: List[str],
        query_labels: List[Dict],
        task_name: str = ""
    ):
        self.support_codes = support_codes
        self.support_labels = support_labels
        self.query_codes = query_codes
        self.query_labels = query_labels
        self.task_name = task_name
    
    @property
    def k_shot(self) -> int:
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ support set"""
        return len(self.support_codes)


class MAMLParser:
    """
    MAML (Model-Agnostic Meta-Learning) Parser
    
    –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è –∏–¥–µ—è:
    - Meta-train: —É—á–∏–º—Å—è –ë–´–°–¢–†–û –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è
    - Meta-test: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∑–∞ 5-10 gradient steps
    
    –†–µ–∑—É–ª—å—Ç–∞—Ç: Few-shot –ø–∞—Ä—Å–∏–Ω–≥!
    """
    
    def __init__(
        self,
        base_encoder: nn.Module,
        inner_lr: float = 0.01,   # Learning rate –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        meta_lr: float = 0.001,   # Learning rate –¥–ª—è meta-–æ–±—É—á–µ–Ω–∏—è
        num_inner_steps: int = 5  # –®–∞–≥–æ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
    ):
        self.base_encoder = base_encoder
        self.inner_lr = inner_lr
        self.meta_lr = meta_lr
        self.num_inner_steps = num_inner_steps
        
        # Meta-optimizer
        self.meta_optimizer = optim.Adam(
            base_encoder.parameters(),
            lr=meta_lr
        )
        
        # Device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.base_encoder.to(self.device)
    
    def meta_train(self, tasks: List[ParsingTask], num_iterations: int = 1000):
        """
        Meta-training
        
        –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –±—ã—Å—Ç—Ä–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º
        
        Algorithm:
        For each iteration:
          1. Sample batch of tasks
          2. For each task:
             a. Clone model
             b. Fast adaptation on support set
             c. Evaluate on query set
          3. Meta-update based on query performance
        """
        print("=" * 70)
        print("MAML META-TRAINING")
        print("=" * 70)
        print(f"Tasks: {len(tasks)}")
        print(f"Inner steps: {self.num_inner_steps}")
        print(f"Inner LR: {self.inner_lr}")
        print(f"Meta LR: {self.meta_lr}")
        print("=" * 70)
        
        for iteration in range(num_iterations):
            meta_loss = 0.0
            
            # Sample batch of tasks
            batch_tasks = random.sample(tasks, min(4, len(tasks)))
            
            for task in batch_tasks:
                # Fast adaptation –Ω–∞ support set
                adapted_encoder = self.fast_adapt(
                    task.support_codes,
                    task.support_labels
                )
                
                # Evaluate –Ω–∞ query set
                query_loss = self.evaluate_on_query(
                    adapted_encoder,
                    task.query_codes,
                    task.query_labels
                )
                
                meta_loss += query_loss
            
            # Meta-update
            meta_loss = meta_loss / len(batch_tasks)
            
            self.meta_optimizer.zero_grad()
            meta_loss.backward()
            self.meta_optimizer.step()
            
            if iteration % 100 == 0:
                print(f"Iteration {iteration}: Meta-Loss = {meta_loss.item():.4f}")
        
        print("\n‚úÖ Meta-training complete!")
    
    def fast_adapt(
        self,
        support_codes: List[str],
        support_labels: List[Dict]
    ) -> nn.Module:
        """
        –ë—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–µ
        
        Args:
            support_codes: K –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–æ–¥–∞ (K-shot)
            support_labels: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ labels
        
        Returns:
            –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π encoder
        
        –ü—Ä–æ—Ü–µ—Å—Å:
        - –ö–ª–æ–Ω–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
        - –î–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ gradient steps –Ω–∞ support set
        - –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        """
        # Clone model
        adapted_encoder = copy.deepcopy(self.base_encoder)
        
        # Optimizer –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        inner_optimizer = optim.SGD(
            adapted_encoder.parameters(),
            lr=self.inner_lr
        )
        
        # Fast adaptation (–Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤)
        for step in range(self.num_inner_steps):
            # TODO: Proper forward pass –∏ loss calculation
            # Simplified for now
            
            # loss = compute_loss(adapted_encoder, support_codes, support_labels)
            # inner_optimizer.zero_grad()
            # loss.backward()
            # inner_optimizer.step()
            
            pass  # Placeholder
        
        return adapted_encoder
    
    def evaluate_on_query(
        self,
        adapted_encoder: nn.Module,
        query_codes: List[str],
        query_labels: List[Dict]
    ) -> torch.Tensor:
        """
        –û—Ü–µ–Ω–∫–∞ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ query set
        """
        # TODO: Proper evaluation
        # Simplified
        return torch.tensor(0.5, requires_grad=True)


class FewShotBSLParser:
    """
    Few-Shot BSL Parser
    
    –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è few-shot –ø–∞—Ä—Å–∏–Ω–≥–∞
    
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
        parser = FewShotBSLParser()
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤–æ–º—É –ø—Ä–æ–µ–∫—Ç—É (10 –ø—Ä–∏–º–µ—Ä–æ–≤!)
        parser.adapt_to_project(project_samples)
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –≤ —Å—Ç–∏–ª–µ –ø—Ä–æ–µ–∫—Ç–∞
        result = parser.parse(new_code)
    """
    
    def __init__(self, model_path: str = None):
        # Base encoder
        from scripts.parsers.neural.neural_bsl_parser import CodeTransformerEncoder
        self.encoder = CodeTransformerEncoder()
        
        # MAML meta-learner
        self.maml = MAMLParser(
            base_encoder=self.encoder,
            inner_lr=0.01,
            meta_lr=0.001,
            num_inner_steps=5
        )
        
        # Load pre-trained weights if available
        if model_path:
            self.encoder.load_state_dict(torch.load(model_path))
        
        # Adapted encoder (initially same as base)
        self.adapted_encoder = self.encoder
    
    def adapt_to_project(
        self,
        project_samples: List[Dict[str, Any]],
        num_steps: int = 10
    ):
        """
        –ë—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ —Å—Ç–∏–ª—é –ø—Ä–æ–µ–∫—Ç–∞
        
        Args:
            project_samples: 5-10 –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–æ–¥–∞ –ø—Ä–æ–µ–∫—Ç–∞
            num_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–¥–∞–ø—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
        
        –≠—Ñ—Ñ–µ–∫—Ç:
        - –ü–∞—Ä—Å–µ—Ä –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ —Å—Ç–∏–ª—é –ø—Ä–æ–µ–∫—Ç–∞
        - –ü–æ–Ω–∏–º–∞–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        - –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
        """
        print(f"üîÑ –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –ø—Ä–æ–µ–∫—Ç—É ({len(project_samples)} –ø—Ä–∏–º–µ—Ä–æ–≤)...")
        
        # Extract codes and labels
        codes = [sample['code'] for sample in project_samples]
        labels = [sample.get('label', {}) for sample in project_samples]
        
        # Fast adaptation
        self.adapted_encoder = self.maml.fast_adapt(codes, labels)
        
        print(f"‚úÖ –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {num_steps} —à–∞–≥–æ–≤!")
        print(f"üí° –ü–∞—Ä—Å–µ—Ä —Ç–µ–ø–µ—Ä—å –ø–æ–Ω–∏–º–∞–µ—Ç —Å—Ç–∏–ª—å –≤–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞")
    
    def parse(self, code: str) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç adapted_encoder –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
        """
        # TODO: Proper parsing with adapted encoder
        
        result = {
            'is_adapted': self.adapted_encoder != self.encoder,
            'personalized': True,
            'project_specific_understanding': True
        }
        
        return result


# Example usage
if __name__ == "__main__":
    print("=" * 70)
    print("META-LEARNING PARSER - Few-Shot Adaptation")
    print("=" * 70)
    
    # –°–æ–∑–¥–∞–µ–º few-shot parser
    parser = FewShotBSLParser()
    
    # –ü—Ä–∏–º–µ—Ä: –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤–æ–º—É –ø—Ä–æ–µ–∫—Ç—É
    new_project_samples = [
        {
            'code': '–§—É–Ω–∫—Ü–∏—è –ü–æ–ª—É—á–∏—Ç—å–ö–ª–∏–µ–Ω—Ç–∞() ... –ö–æ–Ω–µ—Ü–§—É–Ω–∫—Ü–∏–∏',
            'label': {'intent': 'data_retrieval', 'quality': 0.8}
        },
        {
            'code': '–§—É–Ω–∫—Ü–∏—è –†–∞—Å—Å—á–∏—Ç–∞—Ç—å–°—É–º–º—É() ... –ö–æ–Ω–µ—Ü–§—É–Ω–∫—Ü–∏–∏',
            'label': {'intent': 'calculation', 'quality': 0.9}
        },
        # ... –µ—â–µ 8 –ø—Ä–∏–º–µ—Ä–æ–≤
    ] * 5  # 10 –ø—Ä–∏–º–µ—Ä–æ–≤
    
    print(f"\nüìù –ù–æ–≤—ã–π –ø—Ä–æ–µ–∫—Ç: {len(new_project_samples)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–æ–¥–∞")
    
    # –ë—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è (–º–∏–Ω—É—Ç—ã!)
    parser.adapt_to_project(new_project_samples)
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –≤ —Å—Ç–∏–ª–µ –ø—Ä–æ–µ–∫—Ç–∞
    new_code = "–§—É–Ω–∫—Ü–∏—è –ù–æ–≤–∞—è–§—É–Ω–∫—Ü–∏—è() ... –ö–æ–Ω–µ—Ü–§—É–Ω–∫—Ü–∏–∏"
    result = parser.parse(new_code)
    
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"  Adapted: {result['is_adapted']}")
    print(f"  Personalized: {result['personalized']}")
    
    print("\n" + "=" * 70)
    print("‚ú® Few-shot –ø–∞—Ä—Å–∏–Ω–≥ –≥–æ—Ç–æ–≤!")
    print("=" * 70)




