#!/usr/bin/env python3
"""
Fine-tune Qwen-Coder on SmolTalk Dataset
–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –∏ –¥–∏–∞–ª–æ–≥–æ–≤
"""

import os
import sys
import logging
from pathlib import Path
from typing import Optional
import json

import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import bitsandbytes as bnb

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QwenFineTuner:
    """Fine-tuner –¥–ª—è Qwen-Coder –Ω–∞ SmolTalk"""
    
    def __init__(
        self,
        base_model: str = "Qwen/Qwen2.5-Coder-7B-Instruct",
        output_dir: str = "./models/qwen-coder-smoltalk-ru",
        use_4bit: bool = True,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è fine-tuner
        
        Args:
            base_model: –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å Qwen
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
            use_4bit: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 4-bit quantization (–¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
        """
        self.base_model_name = base_model
        self.output_dir = output_dir
        self.use_4bit = use_4bit
        
        self.model = None
        self.tokenizer = None
        self.dataset = None
        
        logger.info(f"Initializing QwenFineTuner")
        logger.info(f"  Base model: {base_model}")
        logger.info(f"  Output dir: {output_dir}")
        logger.info(f"  4-bit quantization: {use_4bit}")
    
    def load_base_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Qwen"""
        logger.info("Loading base model...")
        
        # Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_name,
            trust_remote_code=True
        )
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Model —Å quantization (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
        if self.use_4bit:
            from transformers import BitsAndBytesConfig
            
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
            )
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True
            )
            
            # Prepare for training
            self.model = prepare_model_for_kbit_training(self.model)
            
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
        
        logger.info("‚úì Base model loaded")
        logger.info(f"  Model size: {self.model.num_parameters() / 1e9:.2f}B parameters")
    
    def apply_lora(
        self,
        r: int = 16,
        lora_alpha: int = 32,
        target_modules: Optional[list] = None,
        lora_dropout: float = 0.05,
    ):
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
        
        Args:
            r: –†–∞–Ω–≥ LoRA –º–∞—Ç—Ä–∏—Ü
            lora_alpha: LoRA alpha parameter
            target_modules: –ú–æ–¥—É–ª–∏ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA
            lora_dropout: Dropout –¥–ª—è LoRA layers
        """
        logger.info("Applying LoRA adapter...")
        
        if target_modules is None:
            # Default –¥–ª—è Qwen
            target_modules = [
                "q_proj",
                "k_proj",
                "v_proj",
                "o_proj",
                "gate_proj",
                "up_proj",
                "down_proj",
            ]
        
        lora_config = LoraConfig(
            r=r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type="CAUSAL_LM",
        )
        
        self.model = get_peft_model(self.model, lora_config)
        
        # –ü–µ—á–∞—Ç–∞–µ–º trainable parameters
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        
        logger.info("‚úì LoRA applied")
        logger.info(f"  Trainable params: {trainable_params / 1e6:.2f}M")
        logger.info(f"  Total params: {total_params / 1e6:.2f}M")
        logger.info(f"  Trainable %: {100 * trainable_params / total_params:.2f}%")
    
    def load_smoltalk_dataset(
        self,
        language: str = "ru",
        max_samples: Optional[int] = None,
        train_split: float = 0.95
    ):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ SmolTalk –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Args:
            language: –Ø–∑—ã–∫ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (ru, en, etc.)
            max_samples: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (–¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞)
            train_split: –ü—Ä–æ—Ü–µ–Ω—Ç –¥–ª—è train split
        """
        logger.info("Loading SmolTalk dataset...")
        logger.info(f"  Language filter: {language}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        dataset = load_dataset("HuggingFaceFW/smoltalk")
        
        logger.info(f"  Total samples: {len(dataset['train'])}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —è–∑—ã–∫—É (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
        if language:
            dataset = dataset.filter(
                lambda x: x.get('language', '').lower() == language.lower()
            )
            logger.info(f"  After language filter: {len(dataset['train'])} samples")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ)
        if max_samples:
            dataset['train'] = dataset['train'].select(range(min(max_samples, len(dataset['train']))))
            logger.info(f"  Limited to: {len(dataset['train'])} samples")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ train/val
        dataset = dataset['train'].train_test_split(test_size=1-train_split)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        def format_example(example):
            """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
            # SmolTalk —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∏–∞–ª–æ–≥–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ messages
            messages = example.get('messages', [])
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π
            if messages:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
                text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=False
                )
            else:
                # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ text –ø–æ–ª–µ
                text = example.get('text', '')
            
            return {"text": text}
        
        dataset = dataset.map(format_example)
        
        logger.info("‚úì SmolTalk dataset loaded")
        logger.info(f"  Train samples: {len(dataset['train'])}")
        logger.info(f"  Val samples: {len(dataset['test'])}")
        
        self.dataset = dataset
        return dataset
    
    def tokenize_dataset(self, max_length: int = 2048):
        """
        –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Args:
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        logger.info("Tokenizing dataset...")
        
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                truncation=True,
                max_length=max_length,
                padding="max_length",
                return_tensors="pt"
            )
        
        self.dataset = self.dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=self.dataset['train'].column_names
        )
        
        logger.info("‚úì Dataset tokenized")
    
    def train(
        self,
        num_epochs: int = 3,
        batch_size: int = 4,
        learning_rate: float = 2e-4,
        gradient_accumulation_steps: int = 4,
        warmup_steps: int = 100,
        save_steps: int = 500,
        logging_steps: int = 50,
    ):
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Args:
            num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            learning_rate: Learning rate
            gradient_accumulation_steps: Gradient accumulation
            warmup_steps: Warmup steps
            save_steps: –®–∞–≥–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è checkpoint
            logging_steps: –®–∞–≥–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        logger.info("Starting training...")
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            warmup_steps=warmup_steps,
            logging_steps=logging_steps,
            save_steps=save_steps,
            evaluation_strategy="steps",
            eval_steps=save_steps,
            save_total_limit=3,
            fp16=torch.cuda.is_available(),
            gradient_checkpointing=True,
            optim="paged_adamw_8bit" if self.use_4bit else "adamw_torch",
            report_to=["tensorboard"],
            logging_dir=f"{self.output_dir}/logs",
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.dataset['train'],
            eval_dataset=self.dataset['test'],
            data_collator=data_collator,
        )
        
        # Train!
        logger.info("üöÄ Training started...")
        trainer.train()
        
        logger.info("‚úì Training completed!")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        self.save_model()
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        logger.info(f"Saving model to {self.output_dir}...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        self.model.save_pretrained(self.output_dir)
        self.tokenizer.save_pretrained(self.output_dir)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = {
            "base_model": self.base_model_name,
            "dataset": "HuggingFaceFW/smoltalk",
            "language": "ru",
            "timestamp": str(Path.ctime(Path(self.output_dir))),
        }
        
        config_path = Path(self.output_dir) / "training_config.json"
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        logger.info("‚úì Model saved")
        logger.info(f"  Location: {self.output_dir}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë  Qwen-Coder Fine-tuning on SmolTalk Dataset              ‚ïë
    ‚ïë  –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞                        ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    BASE_MODEL = os.getenv("BASE_MODEL", "Qwen/Qwen2.5-Coder-7B-Instruct")
    OUTPUT_DIR = os.getenv("OUTPUT_DIR", "./models/qwen-coder-smoltalk-ru")
    USE_4BIT = os.getenv("USE_4BIT", "true").lower() == "true"
    NUM_EPOCHS = int(os.getenv("NUM_EPOCHS", "3"))
    MAX_SAMPLES = os.getenv("MAX_SAMPLES", None)
    MAX_SAMPLES = int(MAX_SAMPLES) if MAX_SAMPLES else None
    
    logger.info("Configuration:")
    logger.info(f"  BASE_MODEL: {BASE_MODEL}")
    logger.info(f"  OUTPUT_DIR: {OUTPUT_DIR}")
    logger.info(f"  USE_4BIT: {USE_4BIT}")
    logger.info(f"  NUM_EPOCHS: {NUM_EPOCHS}")
    logger.info(f"  MAX_SAMPLES: {MAX_SAMPLES or 'All'}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    finetuner = QwenFineTuner(
        base_model=BASE_MODEL,
        output_dir=OUTPUT_DIR,
        use_4bit=USE_4BIT
    )
    
    # –≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    finetuner.load_base_model()
    
    # –≠—Ç–∞–ø 2: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA
    finetuner.apply_lora(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05
    )
    
    # –≠—Ç–∞–ø 3: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    finetuner.load_smoltalk_dataset(
        language="ru",
        max_samples=MAX_SAMPLES,
        train_split=0.95
    )
    
    # –≠—Ç–∞–ø 4: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    finetuner.tokenize_dataset(max_length=2048)
    
    # –≠—Ç–∞–ø 5: –û–±—É—á–µ–Ω–∏–µ
    finetuner.train(
        num_epochs=NUM_EPOCHS,
        batch_size=4,
        learning_rate=2e-4,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        save_steps=500,
        logging_steps=50
    )
    
    print("\n" + "="*60)
    print("‚úÖ FINE-TUNING –ó–ê–í–ï–†–®–ï–ù!")
    print("="*60)
    print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {OUTPUT_DIR}")
    print("\n–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å:")
    print(f"  export COPILOT_MODEL_PATH={OUTPUT_DIR}")
    print(f"  python src/api/copilot_api_perfect.py")
    print("="*60)


if __name__ == "__main__":
    main()





