#!/usr/bin/env python3
"""
Test Kimi-Linear-48B –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–æ–ª—å—à–∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π 1–°
–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ—Å–æ–æ–±—Ä–∞–∑–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å 200K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
"""

import os
import sys
import logging
import time
from pathlib import Path
from typing import Dict, List, Optional
import json

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class KimiLinear48BTest:
    """–¢–µ—Å—Ç–µ—Ä –¥–ª—è Kimi-Linear-48B –º–æ–¥–µ–ª–∏"""
    
    def __init__(
        self,
        model_name: str = "moonshotai/Kimi-Linear-48B-A3B-Instruct",
        use_4bit: bool = True
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–µ—Ä–∞
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ HuggingFace
            use_4bit: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 4-bit quantization
        """
        self.model_name = model_name
        self.use_4bit = use_4bit
        
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"Initializing KimiLinear48BTest")
        logger.info(f"  Model: {model_name}")
        logger.info(f"  Device: {self.device}")
        logger.info(f"  4-bit quantization: {use_4bit}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Kimi-Linear-48B"""
        logger.info("Loading Kimi-Linear-48B model...")
        start_time = time.time()
        
        try:
            # Tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # Model —Å quantization (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
            if self.use_4bit:
                from transformers import BitsAndBytesConfig
                
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                )
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    device_map="auto",
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    trust_remote_code=True
                )
            
            load_time = time.time() - start_time
            
            logger.info(f"‚úì Model loaded in {load_time:.2f}s")
            logger.info(f"  Model size: {self.model.num_parameters() / 1e9:.2f}B parameters")
            logger.info(f"  Max context: 200K tokens")
            
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            raise
    
    def load_1c_configuration(self, config_path: str) -> str:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ 1–°
        
        Args:
            config_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        
        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∫–æ–¥ –≤—Å–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        logger.info(f"Loading 1C configuration from: {config_path}")
        
        config_path = Path(config_path)
        
        if not config_path.exists():
            raise FileNotFoundError(f"Configuration not found: {config_path}")
        
        # –ò—â–µ–º –≤—Å–µ BSL —Ñ–∞–π–ª—ã
        bsl_files = list(config_path.rglob("*.bsl"))
        logger.info(f"Found {len(bsl_files)} BSL files")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–¥
        full_code = []
        total_lines = 0
        
        for bsl_file in bsl_files[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª—è —Ç–µ—Å—Ç–∞ –ø–µ—Ä–≤—ã–º–∏ 50 —Ñ–∞–π–ª–∞–º–∏
            try:
                with open(bsl_file, 'r', encoding='utf-8-sig') as f:
                    content = f.read()
                    lines = len(content.split('\n'))
                    total_lines += lines
                    
                    full_code.append(f"// File: {bsl_file.name}")
                    full_code.append(content)
                    full_code.append("")
                    
            except Exception as e:
                logger.warning(f"Failed to read {bsl_file}: {e}")
        
        combined_code = "\n".join(full_code)
        
        logger.info(f"‚úì Configuration loaded")
        logger.info(f"  Files: {len(bsl_files)}")
        logger.info(f"  Total lines: {total_lines}")
        logger.info(f"  Total chars: {len(combined_code)}")
        
        return combined_code
    
    def analyze_configuration(
        self,
        code: str,
        tasks: List[str],
        max_new_tokens: int = 2048
    ) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é Kimi-Linear-48B
        
        Args:
            code: –ö–æ–¥ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            tasks: –°–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            max_new_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        logger.info("Analyzing configuration...")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        tasks_str = "\n".join(f"{i+1}. {task}" for i, task in enumerate(tasks))
        
        prompt = f"""–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ 1–°:–ü—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ

–ö–æ–¥ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:

{code}

–ó–∞–¥–∞—á–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
{tasks_str}

–ü—Ä–æ–≤–µ–¥–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–µ."""
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=200000  # Kimi –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ 200K
        ).to(self.device)
        
        input_tokens = inputs['input_ids'].shape[1]
        logger.info(f"Input tokens: {input_tokens:,}")
        
        if input_tokens > 200000:
            logger.warning(f"‚ö†Ô∏è Input exceeds 200K tokens, truncating...")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        logger.info("Generating analysis...")
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
            )
        
        generation_time = time.time() - start_time
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç (–±–µ–∑ –ø—Ä–æ–º–ø—Ç–∞)
        response = output_text[len(prompt):].strip()
        
        output_tokens = outputs.shape[1] - input_tokens
        tokens_per_sec = output_tokens / generation_time if generation_time > 0 else 0
        
        logger.info(f"‚úì Analysis complete")
        logger.info(f"  Generation time: {generation_time:.2f}s")
        logger.info(f"  Output tokens: {output_tokens}")
        logger.info(f"  Speed: {tokens_per_sec:.2f} tokens/s")
        
        return {
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "generation_time": generation_time,
            "tokens_per_sec": tokens_per_sec,
            "response": response,
            "tasks": tasks,
        }
    
    def run_benchmark(self, config_path: str) -> Dict:
        """
        –ü–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ 1–°
        
        Args:
            config_path: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ 1–°
        
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞
        """
        logger.info("Running benchmark...")
        
        results = {
            "model": self.model_name,
            "device": self.device,
            "use_4bit": self.use_4bit,
            "config_path": config_path,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        try:
            code = self.load_1c_configuration(config_path)
            results["code_length"] = len(code)
            results["code_lines"] = len(code.split('\n'))
        except Exception as e:
            logger.error(f"Failed to load configuration: {e}")
            results["error"] = str(e)
            return results
        
        # –ó–∞–¥–∞—á–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—Ç–∏–ø–∏—á–Ω—ã–µ –¥–ª—è 1–°)
        tasks = [
            "–ù–∞–π–¥–∏ –≤—Å–µ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –º–µ–∂–¥—É –º–æ–¥—É–ª—è–º–∏",
            "–í—ã—è–≤–∏ —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏",
            "–ù–∞–π–¥–∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞",
            "–ü—Ä–µ–¥–ª–æ–∂–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
            "–ù–∞–π–¥–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
        ]
        
        # –ê–Ω–∞–ª–∏–∑
        try:
            analysis = self.analyze_configuration(code, tasks)
            results.update(analysis)
            results["status"] = "success"
        except Exception as e:
            logger.error(f"Analysis failed: {e}")
            results["error"] = str(e)
            results["status"] = "failed"
        
        return results
    
    def evaluate_viability(self, benchmark_results: Dict) -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ —Ü–µ–ª–µ—Å–æ–æ–±—Ä–∞–∑–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Kimi-Linear-48B
        
        Args:
            benchmark_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞
        
        Returns:
            Dict —Å –æ—Ü–µ–Ω–∫–æ–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        logger.info("Evaluating model viability...")
        
        evaluation = {
            "model": self.model_name,
            "verdict": "unknown",
            "scores": {},
            "pros": [],
            "cons": [],
            "recommendations": [],
        }
        
        if benchmark_results.get("status") != "success":
            evaluation["verdict"] = "failed"
            evaluation["recommendations"].append("–ú–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–≥–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")
            return evaluation
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏
        
        # 1. –°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        tokens_per_sec = benchmark_results.get("tokens_per_sec", 0)
        if tokens_per_sec > 10:
            evaluation["scores"]["speed"] = "excellent"
            evaluation["pros"].append(f"–ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {tokens_per_sec:.1f} tokens/s")
        elif tokens_per_sec > 5:
            evaluation["scores"]["speed"] = "good"
            evaluation["pros"].append(f"–ü—Ä–∏–µ–º–ª–µ–º–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å: {tokens_per_sec:.1f} tokens/s")
        else:
            evaluation["scores"]["speed"] = "poor"
            evaluation["cons"].append(f"–ú–µ–¥–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {tokens_per_sec:.1f} tokens/s")
        
        # 2. –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        input_tokens = benchmark_results.get("input_tokens", 0)
        if input_tokens > 100000:
            evaluation["scores"]["context"] = "excellent"
            evaluation["pros"].append(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: {input_tokens:,} tokens")
        elif input_tokens > 50000:
            evaluation["scores"]["context"] = "good"
            evaluation["pros"].append(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: {input_tokens:,} tokens")
        else:
            evaluation["scores"]["context"] = "average"
        
        # 3. –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        generation_time = benchmark_results.get("generation_time", 0)
        if generation_time < 30:
            evaluation["scores"]["latency"] = "excellent"
            evaluation["pros"].append(f"–ù–∏–∑–∫–∞—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {generation_time:.1f}s")
        elif generation_time < 60:
            evaluation["scores"]["latency"] = "good"
        else:
            evaluation["scores"]["latency"] = "poor"
            evaluation["cons"].append(f"–í—ã—Å–æ–∫–∞—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {generation_time:.1f}s")
        
        # 4. –ö–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
        response_length = len(benchmark_results.get("response", ""))
        if response_length > 500:
            evaluation["scores"]["quality"] = "good"
            evaluation["pros"].append("–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç")
        elif response_length > 100:
            evaluation["scores"]["quality"] = "average"
        else:
            evaluation["scores"]["quality"] = "poor"
            evaluation["cons"].append("–ö—Ä–∞—Ç–∫–∏–π/–Ω–µ–ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        scores_values = {
            "excellent": 3,
            "good": 2,
            "average": 1,
            "poor": 0,
        }
        
        total_score = sum(scores_values.get(v, 0) for v in evaluation["scores"].values())
        max_score = len(evaluation["scores"]) * 3
        
        score_percentage = (total_score / max_score * 100) if max_score > 0 else 0
        
        if score_percentage >= 75:
            evaluation["verdict"] = "recommended"
            evaluation["recommendations"].append("‚úÖ –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–æ–ª—å—à–∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π")
        elif score_percentage >= 50:
            evaluation["verdict"] = "conditional"
            evaluation["recommendations"].append("‚ö†Ô∏è –£–°–õ–û–í–ù–û –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø - —Ç–æ–ª—å–∫–æ –¥–ª—è specific use cases")
        else:
            evaluation["verdict"] = "not_recommended"
            evaluation["recommendations"].append("‚ùå –ù–ï –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–µ–∫—É—â–∏–π —Å—Ç–µ–∫")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if self.device == "cpu":
            evaluation["recommendations"].append("üí° GPU –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä–∏—Ç —Ä–∞–±–æ—Ç—É")
        
        if input_tokens < 50000:
            evaluation["recommendations"].append("üí° –î–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π <50K —Ç–æ–∫–µ–Ω–æ–≤ Qwen-Coder –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ")
        
        return evaluation


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë  Kimi-Linear-48B Test for 1C Configurations              ‚ïë
    ‚ïë  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –±–æ–ª—å—à–∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö 1–°                 ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    CONFIG_PATH = os.getenv("CONFIG_PATH", "./1c_configurations/ERP")
    USE_4BIT = os.getenv("USE_4BIT", "true").lower() == "true"
    OUTPUT_FILE = os.getenv("OUTPUT_FILE", "./kimi_test_results.json")
    
    logger.info("Configuration:")
    logger.info(f"  CONFIG_PATH: {CONFIG_PATH}")
    logger.info(f"  USE_4BIT: {USE_4BIT}")
    logger.info(f"  OUTPUT_FILE: {OUTPUT_FILE}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if not Path(CONFIG_PATH).exists():
        logger.error(f"Configuration path not found: {CONFIG_PATH}")
        logger.info("Please provide a valid 1C configuration path via CONFIG_PATH env variable")
        sys.exit(1)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–µ—Ä–∞
    tester = KimiLinear48BTest(use_4bit=USE_4BIT)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    try:
        tester.load_model()
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        logger.info("This may be due to:")
        logger.info("  - Model not available on HuggingFace")
        logger.info("  - Insufficient memory")
        logger.info("  - Missing dependencies")
        sys.exit(1)
    
    # –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞
    logger.info("\n" + "="*60)
    logger.info("RUNNING BENCHMARK")
    logger.info("="*60 + "\n")
    
    benchmark_results = tester.run_benchmark(CONFIG_PATH)
    
    # –û—Ü–µ–Ω–∫–∞ —Ü–µ–ª–µ—Å–æ–æ–±—Ä–∞–∑–Ω–æ—Å—Ç–∏
    evaluation = tester.evaluate_viability(benchmark_results)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    final_results = {
        "benchmark": benchmark_results,
        "evaluation": evaluation
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open(OUTPUT_FILE, 'w') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\n‚úì Results saved to: {OUTPUT_FILE}")
    
    # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏
    print("\n" + "="*60)
    print("EVALUATION RESULTS")
    print("="*60)
    print(f"\nModel: {evaluation['model']}")
    print(f"Verdict: {evaluation['verdict'].upper()}")
    print(f"\nScores:")
    for criterion, score in evaluation['scores'].items():
        print(f"  - {criterion}: {score}")
    
    print(f"\nPros:")
    for pro in evaluation['pros']:
        print(f"  ‚úÖ {pro}")
    
    print(f"\nCons:")
    for con in evaluation['cons']:
        print(f"  ‚ùå {con}")
    
    print(f"\nRecommendations:")
    for rec in evaluation['recommendations']:
        print(f"  {rec}")
    
    print("="*60)
    
    # Exit code
    if evaluation['verdict'] in ['recommended', 'conditional']:
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == "__main__":
    main()





