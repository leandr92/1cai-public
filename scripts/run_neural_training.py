#!/usr/bin/env python3
"""
–ü–æ–ª–Ω—ã–π pipeline –æ–±—É—á–µ–Ω–∏—è Neural BSL Parser

–®–∞–≥–∏:
1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ dataset –∏–∑ PostgreSQL
2. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
3. –í–∞–ª–∏–¥–∞—Ü–∏—è
4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/run_neural_training.py
    python scripts/run_neural_training.py --epochs 20 --batch-size 32

–í–µ—Ä—Å–∏—è: 1.0.0
"""

import argparse
import asyncio
import sys
from pathlib import Path

# Add paths
sys.path.insert(0, str(Path(__file__).parent.parent))


async def prepare_dataset():
    """Step 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ dataset"""
    print("\n" + "=" * 70)
    print("STEP 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ Training Dataset")
    print("=" * 70)
    
    from scripts.dataset.prepare_neural_training_data import NeuralDatasetPreparer
    
    preparer = NeuralDatasetPreparer()
    await preparer.prepare_from_postgres()
    preparer.save_dataset()
    
    return preparer.output_dir


def train_model(dataset_dir: Path, epochs: int, batch_size: int):
    """Step 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    print("\n" + "=" * 70)
    print("STEP 2: –û–±—É—á–µ–Ω–∏–µ Neural Parser")
    print("=" * 70)
    
    from scripts.parsers.neural.train_neural_parser import (
        NeuralParserTrainer,
        BSLCodeDataset
    )
    
    # –°–æ–∑–¥–∞–µ–º trainer
    trainer = NeuralParserTrainer(
        learning_rate=1e-4,
        batch_size=batch_size
    )
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º datasets
    train_dataset = BSLCodeDataset(
        data_path=str(dataset_dir / 'train.json'),
        tokenizer=trainer.tokenizer
    )
    
    val_dataset = BSLCodeDataset(
        data_path=str(dataset_dir / 'val.json'),
        tokenizer=trainer.tokenizer
    )
    
    # –û–±—É—á–∞–µ–º
    trainer.train(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        num_epochs=epochs,
        save_path='./models/neural_parser'
    )
    
    return trainer


def test_model():
    """Step 3: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    print("\n" + "=" * 70)
    print("STEP 3: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Neural Parser")
    print("=" * 70)
    
    from scripts.parsers.neural.neural_bsl_parser import NeuralBSLParser
    
    parser = NeuralBSLParser()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    test_codes = [
        """
        –§—É–Ω–∫—Ü–∏—è –ü–æ–ª—É—á–∏—Ç—å–°–ø–∏—Å–æ–∫–ö–ª–∏–µ–Ω—Ç–æ–≤() –≠–∫—Å–ø–æ—Ä—Ç
            –ó–∞–ø—Ä–æ—Å = –ù–æ–≤—ã–π –ó–∞–ø—Ä–æ—Å;
            –ó–∞–ø—Ä–æ—Å.–¢–µ–∫—Å—Ç = "–í–´–ë–†–ê–¢–¨ * –ò–ó –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫.–ö–ª–∏–µ–Ω—Ç—ã";
            –í–æ–∑–≤—Ä–∞—Ç –ó–∞–ø—Ä–æ—Å.–í—ã–ø–æ–ª–Ω–∏—Ç—å();
        –ö–æ–Ω–µ—Ü–§—É–Ω–∫—Ü–∏–∏
        """,
        """
        –§—É–Ω–∫—Ü–∏—è –†–∞—Å—Å—á–∏—Ç–∞—Ç—å–ù–î–°(–°—É–º–º–∞, –°—Ç–∞–≤–∫–∞ = 20) –≠–∫—Å–ø–æ—Ä—Ç
            –ï—Å–ª–∏ –°—Ç–∞–≤–∫–∞ <= 0 –¢–æ–≥–¥–∞
                –í—ã–∑–≤–∞—Ç—å–ò—Å–∫–ª—é—á–µ–Ω–∏–µ "–ù–µ–≤–µ—Ä–Ω–∞—è —Å—Ç–∞–≤–∫–∞";
            –ö–æ–Ω–µ—Ü–ï—Å–ª–∏;
            –í–æ–∑–≤—Ä–∞—Ç –°—É–º–º–∞ * –°—Ç–∞–≤–∫–∞ / 100;
        –ö–æ–Ω–µ—Ü–§—É–Ω–∫—Ü–∏–∏
        """
    ]
    
    for i, code in enumerate(test_codes, 1):
        print(f"\n--- –¢–µ—Å—Ç {i} ---")
        result = parser.parse(code)
        print(f"Intent: {result.intent.value}")
        print(f"Quality: {result.quality_score:.2f}")
        print(f"Complexity: {result.complexity_score:.2f}")
        print(f"Suggestions: {len(result.suggestions)}")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Neural BSL Parser Training Pipeline"
    )
    parser.add_argument(
        '--epochs',
        type=int,
        default=10,
        help='Number of training epochs (default: 10)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=16,
        help='Batch size (default: 16)'
    )
    parser.add_argument(
        '--skip-dataset',
        action='store_true',
        help='Skip dataset preparation (use existing)'
    )
    parser.add_argument(
        '--dataset-dir',
        type=Path,
        default=Path('./data/neural_training'),
        help='–ö–∞—Ç–∞–ª–æ–≥ —Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º dataset' 
    )
    return parser.parse_args()


async def main():
    """Main pipeline"""
    args = parse_args()

    print("=" * 70)
    print("üöÄ NEURAL BSL PARSER - TRAINING PIPELINE")
    print("=" * 70)
    print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"  Epochs: {args.epochs}")
    print(f"  Batch Size: {args.batch_size}")
    print("=" * 70)

    # Step 1: Dataset
    if not args.skip_dataset:
        dataset_dir = await prepare_dataset()
    else:
        dataset_dir = args.dataset_dir
        if not dataset_dir.exists():
            raise FileNotFoundError(f"–ö–∞—Ç–∞–ª–æ–≥ —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º {dataset_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        print(f"\n‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ dataset, –∏—Å–ø–æ–ª—å–∑—É–µ–º: {dataset_dir}")

    # Step 2: Training
    trainer = train_model(dataset_dir, args.epochs, args.batch_size)

    # Step 3: Testing
    test_model()

    print("\n" + "=" * 70)
    print("üéâ PIPELINE –ó–ê–í–ï–†–®–ï–ù!")
    print("=" * 70)
    print("\n‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
    print(f"\nüìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: ./models/neural_parser/")
    print(f"\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
    print(f"  from scripts.parsers.neural.neural_bsl_parser import NeuralBSLParser")
    print(f"  parser = NeuralBSLParser()")
    print(f"  result = parser.parse(code)")


if __name__ == "__main__":
    asyncio.run(main())





