#!/usr/bin/env python3
"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ training dataset –¥–ª—è Neural BSL Parser
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL

–°–æ–∑–¥–∞–µ—Ç:
- train.json (40,000+ –ø—Ä–∏–º–µ—Ä–æ–≤)
- val.json (5,000+)
- test.json (5,000+)

–° –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏:
- Intent labels
- Quality scores
- Complexity metrics
- Best practices tags

–í–µ—Ä—Å–∏—è: 1.0.0
"""

import argparse
import asyncio
import json
import logging
from pathlib import Path
from typing import List, Dict, Any
from collections import defaultdict
import re

try:
    import asyncpg
except ImportError:
    print("[ERROR] asyncpg not installed: pip install asyncpg")
    exit(1)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NeuralDatasetPreparer:
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ dataset –¥–ª—è Neural Parser
    
    –ü—Ä–æ—Ü–µ—Å—Å:
    1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ PostgreSQL (50k+ —Ñ—É–Ω–∫—Ü–∏–π)
    2. Auto-labeling (intent, quality)
    3. Data augmentation
    4. Train/val/test split
    """
    
    def __init__(self, output_dir: str | Path = "./data/neural_training"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.examples = []
        self.stats = defaultdict(int)
    
    async def prepare_from_postgres(
        self,
        db_url: str = "postgresql://parser_user:parser_pass_2024@localhost:5433/1c_ai_db"
    ):
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL
        """
        logger.info("üîÑ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL...")
        
        try:
            conn = await asyncpg.connect(db_url)
            
            # –ó–∞–ø—Ä–æ—Å –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π —Å –∫–æ–¥–æ–º
            query = """
                SELECT 
                    function_name,
                    code,
                    description,
                    parameters,
                    is_exported,
                    config_name,
                    module_type
                FROM knowledge_base.module_details
                WHERE code IS NOT NULL
                AND LENGTH(code) > 50
                AND LENGTH(code) < 5000
                LIMIT 50000
            """
            
            logger.info("üìä –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π...")
            rows = await conn.fetch(query)
            
            logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(rows)} —Ñ—É–Ω–∫—Ü–∏–π")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Ñ—É–Ω–∫—Ü–∏—é
            for row in rows:
                example = self._prepare_example(row)
                if example:
                    self.examples.append(example)
                    self.stats['total'] += 1
                    self.stats[f"intent_{example['intent']}"] += 1
            
            await conn.close()
            
            logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(self.examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL: {e}")
            logger.warning("‚ö†Ô∏è  –°–æ–∑–¥–∞–µ–º sample dataset...")
            self._create_sample_dataset()
    
    def _prepare_example(self, row: asyncpg.Record) -> Dict[str, Any]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–¥–Ω–æ–≥–æ –æ–±—É—á–∞—é—â–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
        
        Auto-labeling:
        - Intent detection (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ –∏ –∫–æ–¥–∞)
        - Quality score (—ç–≤—Ä–∏—Å—Ç–∏–∫–∏)
        - Complexity (–ø–æ–¥—Å—á–µ—Ç —Ü–∏–∫–ª–æ–≤, —É—Å–ª–æ–≤–∏–π)
        """
        code = row['code']
        function_name = row['function_name'] or ''
        
        # 1. Auto-label intent
        intent = self._detect_intent(function_name, code)
        
        # 2. Calculate quality score
        quality = self._calculate_quality(code, row)
        
        # 3. Calculate complexity
        complexity = self._calculate_complexity(code)
        
        # 4. Calculate maintainability
        maintainability = self._calculate_maintainability(code)
        
        return {
            'code': code,
            'function_name': function_name,
            'intent': intent,
            'quality': quality,
            'complexity': complexity,
            'maintainability': maintainability,
            'is_exported': row.get('is_exported', False),
            'config_name': row.get('config_name', ''),
            'module_type': row.get('module_type', '')
        }
    
    def _detect_intent(self, function_name: str, code: str) -> str:
        """
        Auto-labeling –Ω–∞–º–µ—Ä–µ–Ω–∏–π —Ñ—É–Ω–∫—Ü–∏–∏
        
        –ù–∞ –æ—Å–Ω–æ–≤–µ:
        - –ò–º–µ–Ω–∏ —Ñ—É–Ω–∫—Ü–∏–∏
        - –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –∫–æ–¥–µ
        - –ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        """
        fname_lower = function_name.lower()
        code_lower = code.lower()
        
        # Data Retrieval
        if any(kw in fname_lower for kw in ['–ø–æ–ª—É—á–∏—Ç—å', '–Ω–∞–π—Ç–∏', '–≤—ã–±—Ä–∞—Ç—å', 'get', 'find', 'select']):
            if '–∑–∞–ø—Ä–æ—Å' in code_lower or 'query' in code_lower:
                return 'data_retrieval'
        
        # Data Creation
        if any(kw in fname_lower for kw in ['—Å–æ–∑–¥–∞—Ç—å', '–¥–æ–±–∞–≤–∏—Ç—å', '–Ω–æ–≤—ã–π', 'create', 'add', 'new']):
            return 'data_creation'
        
        # Data Update
        if any(kw in fname_lower for kw in ['–æ–±–Ω–æ–≤–∏—Ç—å', '–∏–∑–º–µ–Ω–∏—Ç—å', 'update', 'modify']):
            return 'data_update'
        
        # Data Deletion
        if any(kw in fname_lower for kw in ['—É–¥–∞–ª–∏—Ç—å', 'delete', 'remove']):
            return 'data_deletion'
        
        # Calculation
        if any(kw in fname_lower for kw in ['—Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å', '–≤—ã—á–∏—Å–ª–∏—Ç—å', 'calc', 'calculate', 'compute']):
            return 'calculation'
        
        # Validation
        if any(kw in fname_lower for kw in ['–ø—Ä–æ–≤–µ—Ä–∏—Ç—å', '–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å', 'validate', 'check']):
            return 'validation'
        
        # Transformation
        if any(kw in fname_lower for kw in ['–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å', '–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å', 'transform', 'convert']):
            return 'transformation'
        
        # Integration
        if 'http' in code_lower or 'rest' in code_lower or 'soap' in code_lower:
            return 'integration'
        
        # UI Interaction
        if '—Ñ–æ—Ä–º–∞' in code_lower or 'form' in code_lower or '—ç–ª–µ–º–µ–Ω—Ç' in code_lower:
            return 'ui_interaction'
        
        # Default
        return 'utility'
    
    def _calculate_quality(self, code: str, row: asyncpg.Record) -> float:
        """
        –†–∞—Å—á–µ—Ç quality score (0-1)
        
        –ö—Ä–∏—Ç–µ—Ä–∏–∏:
        - –ù–∞–ª–∏—á–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
        - –î–ª–∏–Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏
        - –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å
        """
        score = 0.5  # Base score
        
        # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (+0.15)
        if '//' in code or '/*' in code:
            score += 0.15
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ (+0.15)
        if '–ü–æ–ø—ã—Ç–∫–∞' in code or 'Try' in code:
            score += 0.15
        
        # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ (+0.1)
        lines = len(code.split('\n'))
        if 10 <= lines <= 100:
            score += 0.1
        
        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º–∞—è —Ñ—É–Ω–∫—Ü–∏—è (+0.05)
        if row.get('is_exported'):
            score += 0.05
        
        # –û–ø–∏—Å–∞–Ω–∏–µ (+0.05)
        if row.get('description'):
            score += 0.05
        
        return min(score, 1.0)
    
    def _calculate_complexity(self, code: str) -> float:
        """
        –¶–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è 0-1)
        
        –ü–æ–¥—Å—á–µ—Ç:
        - –£—Å–ª–æ–≤–∏—è (–ï—Å–ª–∏, –ò–Ω–∞—á–µ)
        - –¶–∏–∫–ª—ã (–î–ª—è, –ü–æ–∫–∞)
        - Try-Catch –±–ª–æ–∫–∏
        """
        complexity = 1  # Base complexity
        
        # –£—Å–ª–æ–≤–∏—è
        complexity += code.count('–ï—Å–ª–∏')
        complexity += code.count('–ò–Ω–∞—á–µ')
        
        # –¶–∏–∫–ª—ã
        complexity += code.count('–î–ª—è')
        complexity += code.count('–ü–æ–∫–∞')
        complexity += code.count('–¶–∏–∫–ª')
        
        # Exception handling
        complexity += code.count('–ü–æ–ø—ã—Ç–∫–∞')
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (max expected = 20)
        normalized = min(complexity / 20.0, 1.0)
        
        return normalized
    
    def _calculate_maintainability(self, code: str) -> float:
        """
        Maintainability index (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π, 0-1)
        
        –§–∞–∫—Ç–æ—Ä—ã:
        - –î–ª–∏–Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏
        - –í–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å
        - –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        """
        score = 1.0
        
        lines = len(code.split('\n'))
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –¥–ª–∏–Ω—É
        if lines > 100:
            score -= 0.3
        elif lines > 50:
            score -= 0.15
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        if '//' not in code and '/*' not in code:
            score -= 0.2
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –≤—ã—Å–æ–∫—É—é –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å
        max_indent = max(
            (len(line) - len(line.lstrip())) // 4 
            for line in code.split('\n') 
            if line.strip()
        )
        if max_indent > 4:
            score -= 0.2
        
        return max(score, 0.0)
    
    def _create_sample_dataset(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ sample dataset –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        samples = [
            {
                'code': '–§—É–Ω–∫—Ü–∏—è –ü–æ–ª—É—á–∏—Ç—å–î–∞–Ω–Ω—ã–µ() –ó–∞–ø—Ä–æ—Å = –ù–æ–≤—ã–π –ó–∞–ø—Ä–æ—Å; –í–æ–∑–≤—Ä–∞—Ç –ó–∞–ø—Ä–æ—Å.–í—ã–ø–æ–ª–Ω–∏—Ç—å(); –ö–æ–Ω–µ—Ü–§—É–Ω–∫—Ü–∏–∏',
                'function_name': '–ü–æ–ª—É—á–∏—Ç—å–î–∞–Ω–Ω—ã–µ',
                'intent': 'data_retrieval',
                'quality': 0.6,
                'complexity': 0.1,
                'maintainability': 0.7
            },
            {
                'code': '–§—É–Ω–∫—Ü–∏—è –†–∞—Å—Å—á–∏—Ç–∞—Ç—å–°—É–º–º—É(–ê, –ë) –í–æ–∑–≤—Ä–∞—Ç –ê + –ë; –ö–æ–Ω–µ—Ü–§—É–Ω–∫—Ü–∏–∏',
                'function_name': '–†–∞—Å—Å—á–∏—Ç–∞—Ç—å–°—É–º–º—É',
                'intent': 'calculation',
                'quality': 0.7,
                'complexity': 0.05,
                'maintainability': 0.9
            },
            # –î–æ–±–∞–≤–∏–º –µ—â–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        ] * 100  # –î—É–±–ª–∏—Ä—É–µ–º –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ dataset
        
        self.examples = samples
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(self.examples)} sample –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    def save_dataset(self, train_ratio: float = 0.8, val_ratio: float = 0.1):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ dataset —Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º
        
        Args:
            train_ratio: –î–æ–ª—è train (default 0.8)
            val_ratio: –î–æ–ª—è validation (default 0.1)
            test_ratio: –û—Å—Ç–∞–ª—å–Ω–æ–µ –¥–ª—è test
        """
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ dataset...")
        
        # Shuffle
        import random
        random.shuffle(self.examples)
        
        # Split
        n = len(self.examples)
        train_size = int(n * train_ratio)
        val_size = int(n * val_ratio)
        
        train_examples = self.examples[:train_size]
        val_examples = self.examples[train_size:train_size + val_size]
        test_examples = self.examples[train_size + val_size:]
        
        # Save
        self._save_json(train_examples, 'train.json')
        self._save_json(val_examples, 'val.json')
        self._save_json(test_examples, 'test.json')
        
        # Stats
        self._save_stats()
        
        logger.info(f"‚úÖ Train: {len(train_examples)}")
        logger.info(f"‚úÖ Val: {len(val_examples)}")
        logger.info(f"‚úÖ Test: {len(test_examples)}")
    
    def _save_json(self, examples: List[Dict], filename: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON"""
        filepath = self.output_dir / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(examples, f, ensure_ascii=False, indent=2)
        logger.info(f"üíæ {filename}: {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    def _save_stats(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        stats_file = self.output_dir / 'stats.json'
        payload = {
            'schema_version': '1.0.0',
            'summary': dict(self.stats),
        }
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(payload, f, indent=2)
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats_file}")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ dataset –¥–ª—è Neural BSL Parser")
    parser.add_argument(
        "--db-url",
        default="postgresql://parser_user:parser_pass_2024@localhost:5433/1c_ai_db",
        help="–°—Ç—Ä–æ–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("./data/neural_training"),
        help="–ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è train/val/test",
    )
    parser.add_argument(
        "--train-ratio",
        type=float,
        default=0.8,
        help="–î–æ–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏",
    )
    parser.add_argument(
        "--val-ratio",
        type=float,
        default=0.1,
        help="–î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏",
    )
    return parser.parse_args()


async def run(args: argparse.Namespace) -> None:
    logger.info("=" * 70)
    logger.info("–ü–û–î–ì–û–¢–û–í–ö–ê TRAINING DATASET –î–õ–Ø NEURAL PARSER")
    logger.info("=" * 70)

    preparer = NeuralDatasetPreparer(args.output_dir)
    await preparer.prepare_from_postgres(db_url=args.db_url)
    preparer.save_dataset(train_ratio=args.train_ratio, val_ratio=args.val_ratio)

    logger.info("=" * 70)
    logger.info("‚úÖ Dataset –≥–æ—Ç–æ–≤!")
    logger.info("=" * 70)
    logger.info("–§–∞–π–ª—ã:")
    logger.info("  - %s", preparer.output_dir / "train.json")
    logger.info("  - %s", preparer.output_dir / "val.json")
    logger.info("  - %s", preparer.output_dir / "test.json")


def main() -> int:
    args = parse_args()
    try:
        asyncio.run(run(args))
    except Exception as err:  # noqa: BLE001
        logger.error("‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ dataset: %s", err)
        return 1
    return 0


if __name__ == "__main__":
    raise SystemExit(main())





