#!/usr/bin/env python3
"""
Massive AST Dataset Builder
–°–æ–∑–¥–∞–µ—Ç –±–æ–ª—å—à–æ–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ BSL

–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:
1. PostgreSQL knowledge_base (50,000+ —Ñ—É–Ω–∫—Ü–∏–π)
2. GitHub –ø—É–±–ª–∏—á–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã
3. –í—Ä—É—á–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ò–∑–≤–ª–µ–∫–∞–µ—Ç 50,000+ –ø—Ä–∏–º–µ—Ä–æ–≤ (vs 500)
- –î–æ–±–∞–≤–ª—è–µ—Ç AST representation
- Semantic enrichment
- Data augmentation
- Quality filtering

–í–µ—Ä—Å–∏—è: 2.0.0
"""

import asyncio
import json
import logging
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import hashlib

sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import asyncpg
    from src.services.configuration_knowledge_base import get_knowledge_base
    from scripts.parsers.bsl_ast_parser import BSLASTParser
except ImportError as e:
    print(f"[ERROR] Missing dependencies: {e}")
    print("Install: pip install asyncpg")
    sys.exit(1)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MassiveASTDatasetBuilder:
    """
    –°–æ–∑–¥–∞–µ—Ç –º–∞—Å—Å–∏–≤–Ω—ã–π dataset —Å AST –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
    
    –¶–µ–ª—å: 50,000+ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 500
    """
    
    def __init__(self, output_dir: str = "./data/bsl_massive_dataset"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.examples = []
        self.stats = {
            'total': 0,
            'from_db': 0,
            'from_github': 0,
            'augmented': 0,
            'filtered_out': 0,
            'categories': {}
        }
        
        # AST parser
        self.ast_parser = BSLASTParser(use_language_server=True)
        
        # KB
        self.kb = get_knowledge_base()
    
    async def build_from_postgres(self, db_url: str = None):
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –í–°–ï 50,000+ —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ PostgreSQL
        
        This is the main dataset source!
        """
        logger.info("üìö –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ PostgreSQL...")
        
        if not db_url:
            db_url = "postgresql://user:password@localhost:5432/1c_ai_db"
        
        try:
            conn = await asyncpg.connect(db_url)
            
            # –ó–∞–ø—Ä–æ—Å –í–°–ï–• —Ñ—É–Ω–∫—Ü–∏–π
            query = """
                SELECT 
                    kb.config_name,
                    kb.module_name,
                    md.function_name,
                    md.code,
                    md.description,
                    md.parameters,
                    md.return_type,
                    md.is_exported,
                    md.region,
                    md.comments,
                    md.examples
                FROM knowledge_base.modules kb
                JOIN knowledge_base.module_details md ON kb.id = md.module_id
                WHERE LENGTH(md.code) > 50          -- –ù–µ —Å–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç—ã–µ
                AND LENGTH(md.code) < 5000          -- –ù–µ —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω—ã–µ
                AND md.function_name IS NOT NULL    -- –¢–æ–ª—å–∫–æ —Ñ—É–Ω–∫—Ü–∏–∏
            """
            
            rows = await conn.fetch(query)
            
            logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(rows)} —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ –ë–î")
            
            for row in rows:
                example = await self._create_training_example_from_db_row(row)
                
                if example and self._quality_filter(example):
                    self.examples.append(example)
                    self.stats['from_db'] += 1
                else:
                    self.stats['filtered_out'] += 1
            
            await conn.close()
            
            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {self.stats['from_db']} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –ë–î")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL: {e}")
            logger.warning("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: DATABASE_URL env variable")
    
    async def _create_training_example_from_db_row(
        self, 
        row: asyncpg.Record
    ) -> Optional[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –∏–∑ —Å—Ç—Ä–æ–∫–∏ –ë–î"""
        
        try:
            code = row['code']
            
            # –ü–∞—Ä—Å–∏–º BSL –∫–æ–¥ –≤ AST
            parse_result = self.ast_parser.parse(code)
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
            category = self._categorize_function(row, parse_result)
            
            # –°–æ–∑–¥–∞–µ–º instruction
            instruction = self._generate_instruction(row, category)
            
            # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Å AST
            example = {
                # –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                'instruction': instruction,
                'input': row.get('description', ''),
                'output': code,
                
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                'config_name': row['config_name'],
                'module_name': row['module_name'],
                'function_name': row['function_name'],
                'category': category,
                'is_exported': row.get('is_exported', False),
                
                # AST –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                'ast': parse_result.get('ast'),
                'control_flow': parse_result.get('control_flow'),
                'data_flow': parse_result.get('data_flow'),
                'complexity': parse_result.get('complexity'),
                'api_usage': parse_result.get('api_usage', []),
                
                # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
                'quality_score': self._calculate_quality_score(row, parse_result),
                'lines_of_code': len(code.split('\n')),
                
                # –•–µ—à –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
                'hash': hashlib.sha256(code.encode()).hexdigest()
            }
            
            self.stats['total'] += 1
            self.stats['categories'][category] = self.stats['categories'].get(category, 0) + 1
            
            return example
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–º–µ—Ä–∞: {e}")
            return None
    
    def _categorize_function(
        self, 
        row: asyncpg.Record, 
        parse_result: Dict
    ) -> str:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ —Ç–∏–ø—É"""
        
        function_name = row['function_name'].lower()
        code = row['code'].lower()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if any(kw in function_name for kw in ['–ø–æ–ª—É—á–∏—Ç—å', '–Ω–∞–π—Ç–∏', '–≤—ã–±—Ä–∞—Ç—å', 'get', 'find']):
            return 'data_retrieval'
        
        elif any(kw in function_name for kw in ['—Å–æ–∑–¥–∞—Ç—å', '–¥–æ–±–∞–≤–∏—Ç—å', '–∑–∞–ø–∏—Å–∞—Ç—å', 'create', 'add']):
            return 'data_creation'
        
        elif any(kw in function_name for kw in ['–æ–±–Ω–æ–≤–∏—Ç—å', '–∏–∑–º–µ–Ω–∏—Ç—å', 'update', 'modify']):
            return 'data_update'
        
        elif any(kw in function_name for kw in ['—É–¥–∞–ª–∏—Ç—å', 'delete', 'remove']):
            return 'data_deletion'
        
        elif any(kw in function_name for kw in ['—Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å', '–≤—ã—á–∏—Å–ª–∏—Ç—å', 'calculate']):
            return 'calculation'
        
        elif any(kw in code for kw in ['–∑–∞–ø—Ä–æ—Å', 'query', 'select']):
            return 'database_query'
        
        elif any(kw in code for kw in ['http', 'rest', 'soap', '–≤–µ–±—Å–µ—Ä–≤–∏—Å']):
            return 'integration'
        
        elif any(kw in code for kw in ['—Ñ–æ—Ä–º–∞', 'form', '—ç–ª–µ–º–µ–Ω—Ç']):
            return 'ui_forms'
        
        elif 'validate' in function_name or '–ø—Ä–æ–≤–µ—Ä–∏—Ç—å' in function_name:
            return 'validation'
        
        else:
            return 'utility'
    
    def _generate_instruction(self, row: asyncpg.Record, category: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è instruction –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞"""
        
        function_name = row['function_name']
        params = row.get('parameters', [])
        
        # –ë–∞–∑–æ–≤—ã–π template
        instruction = f"–°–æ–∑–¥–∞–π —Ñ—É–Ω–∫—Ü–∏—é {function_name}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        if params:
            param_names = ', '.join([p.get('name', '') for p in params])
            instruction += f" —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {param_names}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_context = {
            'data_retrieval': '–¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö',
            'data_creation': '–¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø–∏—Å–∏',
            'data_update': '–¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö',
            'data_deletion': '–¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏',
            'calculation': '–¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∑–Ω–∞—á–µ–Ω–∏–π',
            'database_query': '–¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –∫ –ë–î',
            'integration': '–¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ HTTP/REST',
            'ui_forms': '–¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–æ—Ä–º–∞–º–∏',
            'validation': '–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö',
            'utility': '–¥–ª—è —Å–ª—É–∂–µ–±–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π'
        }
        
        context = category_context.get(category, '')
        if context:
            instruction += f" {context}"
        
        return instruction
    
    def _calculate_quality_score(
        self, 
        row: asyncpg.Record, 
        parse_result: Dict
    ) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ score –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–∞ (0-1)"""
        
        score = 0.0
        
        # 1. –ù–∞–ª–∏—á–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤/–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ (+0.3)
        if row.get('comments') or row.get('description'):
            score += 0.3
        
        # 2. –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (+0.2)
        if row.get('examples'):
            score += 0.2
        
        # 3. –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º–∞—è —Ñ—É–Ω–∫—Ü–∏—è (+0.1)
        if row.get('is_exported'):
            score += 0.1
        
        # 4. –ê–¥–µ–∫–≤–∞—Ç–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å (+0.2)
        complexity = parse_result.get('complexity', {}).get('cyclomatic', 0)
        if 2 <= complexity <= 10:  # Sweet spot
            score += 0.2
        elif complexity == 1:
            score += 0.1  # –°–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–∞—è
        
        # 5. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç API 1–° (+0.1)
        if parse_result.get('api_usage'):
            score += 0.1
        
        # 6. –ë–µ–∑ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ (+0.1)
        if not any(d.get('severity') == 'error' for d in parse_result.get('diagnostics', [])):
            score += 0.1
        
        return min(score, 1.0)
    
    def _quality_filter(self, example: Dict) -> bool:
        """–§–∏–ª—å—Ç—Ä –∫–∞—á–µ—Å—Ç–≤–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–ª–æ—Ö–∏–µ –ø—Ä–∏–º–µ—Ä—ã"""
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π quality score
        if example['quality_score'] < 0.3:
            return False
        
        # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ –¥–ª–∏–Ω–Ω—ã–µ
        if example['lines_of_code'] < 5 or example['lines_of_code'] > 200:
            return False
        
        # –°–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω—ã–µ (–Ω–µ—á–∏—Ç–∞–µ–º—ã–µ)
        complexity = example.get('complexity', {}).get('cyclomatic', 0)
        if complexity > 20:
            return False
        
        return True
    
    async def augment_dataset(self):
        """
        Data augmentation - —Å–æ–∑–¥–∞–Ω–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–π
        
        –£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç dataset –≤ 2-3 —Ä–∞–∑–∞
        """
        logger.info("üîÑ Data augmentation...")
        
        original_count = len(self.examples)
        augmented = []
        
        for example in self.examples[:1000]:  # –ü–µ—Ä–≤—ã–µ 1000 –¥–ª—è augmentation
            # 1. –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            var1 = self._augment_rename_variables(example)
            if var1:
                augmented.append(var1)
            
            # 2. –ò–∑–º–µ–Ω–µ–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
            var2 = self._augment_modify_comments(example)
            if var2:
                augmented.append(var2)
        
        self.examples.extend(augmented)
        self.stats['augmented'] = len(augmented)
        
        logger.info(f"‚úÖ Augmentation: {original_count} ‚Üí {len(self.examples)} (+{len(augmented)})")
    
    def _augment_rename_variables(self, example: Dict) -> Optional[Dict]:
        """Augmentation: –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –≤ production –Ω—É–∂–µ–Ω proper AST rewrite
        code = example['output']
        
        # –ó–∞–º–µ–Ω—è–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ –∏–º–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        replacements = {
            '–†–µ–∑—É–ª—å—Ç–∞—Ç': '–†–µ–∑—É–ª—å—Ç–∞—Ç–í—ã–ø–æ–ª–Ω–µ–Ω–∏—è',
            '–ü–∞—Ä–∞–º–µ—Ç—Ä': '–í—Ö–æ–¥–Ω–æ–π–ü–∞—Ä–∞–º–µ—Ç—Ä',
            '–ó–Ω–∞—á–µ–Ω–∏–µ': '–¢–µ–∫—É—â–µ–µ–ó–Ω–∞—á–µ–Ω–∏–µ'
        }
        
        new_code = code
        for old, new in replacements.items():
            if old in code and new not in code:
                new_code = new_code.replace(old, new)
        
        if new_code != code:
            augmented = example.copy()
            augmented['output'] = new_code
            augmented['hash'] = hashlib.sha256(new_code.encode()).hexdigest()
            augmented['is_augmented'] = True
            return augmented
        
        return None
    
    def _augment_modify_comments(self, example: Dict) -> Optional[Dict]:
        """Augmentation: –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤"""
        # –í production –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è
        return None  # Placeholder
    
    def save_dataset(self, split: bool = True):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ dataset –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        
        Args:
            split: –†–∞–∑–±–∏—Ç—å –Ω–∞ train/val/test
        """
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ dataset ({len(self.examples)} –ø—Ä–∏–º–µ—Ä–æ–≤)...")
        
        # Deduplicate –ø–æ hash
        unique_examples = {}
        for ex in self.examples:
            hash_key = ex['hash']
            if hash_key not in unique_examples:
                unique_examples[hash_key] = ex
        
        examples = list(unique_examples.values())
        logger.info(f"–ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(examples)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        if split:
            # Split: 80% train, 10% val, 10% test
            import random
            random.shuffle(examples)
            
            n = len(examples)
            train_size = int(n * 0.8)
            val_size = int(n * 0.1)
            
            train_examples = examples[:train_size]
            val_examples = examples[train_size:train_size + val_size]
            test_examples = examples[train_size + val_size:]
            
            # Save splits
            self._save_jsonl(train_examples, "train.jsonl")
            self._save_jsonl(val_examples, "validation.jsonl")
            self._save_jsonl(test_examples, "test.jsonl")
            
            logger.info(f"‚úÖ Train: {len(train_examples)}")
            logger.info(f"‚úÖ Val: {len(val_examples)}")
            logger.info(f"‚úÖ Test: {len(test_examples)}")
        else:
            self._save_jsonl(examples, "full.jsonl")
        
        # Save stats
        stats_file = self.output_dir / "dataset_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ Stats: {stats_file}")
    
    def _save_jsonl(self, examples: List[Dict], filename: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSONL —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è Hugging Face"""
        
        output_file = self.output_dir / filename
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for ex in examples:
                # Format –¥–ª—è fine-tuning
                training_text = self._format_for_training(ex)
                
                # JSONL - –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —ç—Ç–æ JSON
                line = json.dumps({'text': training_text}, ensure_ascii=False)
                f.write(line + '\n')
        
        logger.info(f"Saved: {output_file} ({len(examples)} examples)")
    
    def _format_for_training(self, example: Dict) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        –í–∫–ª—é—á–∞–µ—Ç AST –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é!
        """
        
        # Alpaca format + AST
        text = f"""### Instruction:
{example['instruction']}

### Category:
{example['category']}

### Input:
{example['input']}

### Structure (AST):
Complexity: {example['complexity']['cyclomatic']}
API Usage: {len(example['api_usage'])} calls
Control Flow: {len(example['control_flow']['nodes'])} nodes

### Response:
{example['output']}
"""
        
        return text


async def main():
    """Main entry point"""
    
    print("=" * 70)
    print("MASSIVE AST DATASET BUILDER")
    print("–¶–µ–ª—å: 50,000+ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å AST")
    print("=" * 70)
    
    builder = MassiveASTDatasetBuilder()
    
    # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ PostgreSQL (–æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫)
    await builder.build_from_postgres()
    
    # 2. Data augmentation
    if len(builder.examples) > 0:
        await builder.augment_dataset()
    
    # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    if len(builder.examples) > 0:
        builder.save_dataset(split=True)
        
        print("\n" + "=" * 70)
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print("=" * 70)
        print(f"–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(builder.examples)}")
        print(f"–ò–∑ PostgreSQL: {builder.stats['from_db']}")
        print(f"Augmented: {builder.stats['augmented']}")
        print(f"Filtered out: {builder.stats['filtered_out']}")
        
        print("\n–ö–∞—Ç–µ–≥–æ—Ä–∏–∏:")
        for cat, count in builder.stats['categories'].items():
            print(f"  {cat}: {count}")
        
        print(f"\n‚úÖ Dataset –≥–æ—Ç–æ–≤: {builder.output_dir}")
    else:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å dataset")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL")


if __name__ == "__main__":
    asyncio.run(main())




