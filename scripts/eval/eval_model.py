"""
–ü—Ä–æ—Å—Ç–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–Ω–æ–π ML‚Äë–º–æ–¥–µ–ª–∏ 1C AI Stack.

Usage:
    python scripts/eval/eval_model.py --model ./models/demo-model --questions output/dataset/DEMO_qa.jsonl --limit 10
    python scripts/eval/eval_model.py --config-name ERPCPM --save reports/eval/ERPCPM.json
"""

from __future__ import annotations

import argparse
import json
from datetime import datetime, timezone
from pathlib import Path
from statistics import mean
from typing import Any, Dict, List, Optional

from scripts.ml.config_utils import get_config, load_configs, format_config_info


def load_dataset(path: Path, limit: int) -> List[Dict[str, Any]]:
    with path.open("r", encoding="utf-8") as fh:
        lines = fh.readlines()

    samples: List[Dict[str, Any]] = []
    for line in lines:
        line = line.strip()
        if not line:
            continue
        try:
            samples.append(json.loads(line))
        except json.JSONDecodeError:
            continue
        if 0 < limit <= len(samples):
            break
    return samples


def evaluate(
    model_path: Path,
    dataset_path: Path,
    dataset: List[Dict[str, Any]],
    config_name: Optional[str],
) -> Dict[str, Any]:
    if not model_path.exists():
        raise FileNotFoundError(f"Model path not found: {model_path}")

    print(f"üìÅ Model path: {model_path}")
    print(f"üìä Samples to evaluate: {len(dataset)}")
    print("\n‚ö†Ô∏è  Demo evaluator: –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –Ω–∞–ª–∏—á–∏–µ –æ—Ç–≤–µ—Ç–æ–≤.\n")

    missing_answer = 0
    missing_metadata = 0
    answer_lengths: List[int] = []
    question_lengths: List[int] = []

    for sample in dataset:
        answer = sample.get("answer")
        if not answer:
            missing_answer += 1
        else:
            answer_lengths.append(len(str(answer).split()))

        metadata = sample.get("metadata")
        if not metadata:
            missing_metadata += 1

        question = sample.get("question") or sample.get("prompt")
        if question:
            question_lengths.append(len(str(question).split()))

    total = len(dataset)

    print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏:")
    print(f"  ‚Ä¢ –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {total}")
    print(f"  ‚Ä¢ –ë–µ–∑ answer: {missing_answer}")
    print(f"  ‚Ä¢ –ë–µ–∑ metadata: {missing_metadata}")

    if total > 0:
        quality_score = ((total - missing_answer) / total) * 100
        print(f"\n–û—Ü–µ–Ω–∫–∞ (—É—Å–ª–æ–≤–Ω–∞—è): {quality_score:.1f}% –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.")
    else:
        print("\n–î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç–æ–π ‚Äî –Ω–∏—á–µ–≥–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å.")

    summary = {
        "config_name": config_name,
        "model_path": str(model_path),
        "dataset_path": str(dataset_path),
        "generated_at": datetime.now(tz=timezone.utc).isoformat(),
        "total_samples": total,
        "answered_samples": total - missing_answer,
        "missing_answer": missing_answer,
        "missing_metadata": missing_metadata,
        "answer_coverage": 0.0 if total == 0 else (total - missing_answer) / total,
        "metadata_coverage": 0.0 if total == 0 else (total - missing_metadata) / total,
        "avg_answer_tokens": round(mean(answer_lengths), 2) if answer_lengths else 0.0,
        "avg_question_tokens": round(mean(question_lengths), 2) if question_lengths else 0.0,
    }

    return summary


def print_summary(summary: Dict[str, Any]) -> None:
    print("\nüìà Summary")
    print(f"  ‚Ä¢ Answer coverage     : {summary['answer_coverage'] * 100:.1f}%")
    print(f"  ‚Ä¢ Metadata coverage   : {summary['metadata_coverage'] * 100:.1f}%")
    print(f"  ‚Ä¢ Avg answer tokens   : {summary['avg_answer_tokens']}")
    print(f"  ‚Ä¢ Avg question tokens : {summary['avg_question_tokens']}")


def list_configs() -> None:
    for name in sorted(load_configs()):
        print(name)


def main() -> None:
    parser = argparse.ArgumentParser(description="Evaluate 1C AI demo model")
    parser.add_argument("--model", help="–ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∏–ª–∏ —Ñ–∞–π–ª)")
    parser.add_argument("--questions", help="JSONL —Ñ–∞–π–ª —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏/–æ—Ç–≤–µ—Ç–∞–º–∏")
    parser.add_argument("--limit", type=int, help="–°–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–ª–∏ 20)")
    parser.add_argument("--config-name", help="–ò–º—è –Ω–∞–±–æ—Ä–∞ –∏–∑ config/ml_datasets.json")
    parser.add_argument("--save", help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON –æ—Ç—á—ë—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)")
    parser.add_argument("--list-configs", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –≤—ã–π—Ç–∏")

    args = parser.parse_args()

    if args.list_configs:
        list_configs()
        return

    config = None
    if args.config_name:
        config = get_config(args.config_name)
        print(format_config_info(args.config_name, config))

    model_arg = args.model or (config.get("model_host") if config else None)
    questions_arg = args.questions or (config.get("qa_host") if config else None)

    if not model_arg or not questions_arg:
        raise SystemExit("Specify --model and --questions or use --config-name with predefined paths.")

    limit = args.limit
    if limit is None:
        if config and config.get("evaluation_limit"):
            limit = int(config["evaluation_limit"])
        else:
            limit = 20

    model_path = Path(model_arg)
    dataset_path = Path(questions_arg)

    dataset = load_dataset(dataset_path, limit)
    summary = evaluate(model_path, dataset_path, dataset, args.config_name)
    print_summary(summary)

    save_path: Optional[Path] = None
    if args.save:
        save_path = Path(args.save)
    elif config and config.get("eval_report"):
        save_path = Path(config["eval_report"])

    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        with save_path.open("w", encoding="utf-8") as handle:
            json.dump(summary, handle, ensure_ascii=False, indent=2)
        print(f"\nüíæ Report saved to: {save_path}")


if __name__ == "__main__":
    main()

