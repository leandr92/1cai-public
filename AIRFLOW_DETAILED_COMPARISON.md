# üî¨ Apache Airflow vs 1C AI Stack - –î–µ—Ç–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑

**–í–µ—Ä—Å–∏—è:** 1.0 Extended  
**–î–∞—Ç–∞:** 2024-11-05  
**–¶–µ–ª—å:** –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è–º–∏

---

## üìê –¢–µ–∫—É—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ (AS-IS)

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç 1: Celery –¥–ª—è ML Tasks

```mermaid
graph TB
    subgraph "Celery Beat (Scheduler)"
        BEAT[Celery Beat Process<br/>PID: xxx<br/>Memory: 50MB]
        
        SCHEDULE["Beat Schedule:<br/>- retrain-models-daily (2:00)<br/>- update-feature-store (hourly)<br/>- cleanup-old-experiments (1:00)<br/>- check-model-drift (every 30min)<br/>- retrain-underperforming (3:00)"]
    end
    
    subgraph "Redis Broker"
        REDIS_BROKER["Redis DB 1<br/>Port: 6379<br/>Keys:<br/>- celery-task-meta-*<br/>- _kombu.binding.*<br/>Max memory: 2GB"]
    end
    
    subgraph "Celery Workers (3 instances)"
        W1["Worker 1<br/>Concurrency: 4<br/>Pool: prefork<br/>Memory: 800MB"]
        W2["Worker 2<br/>Concurrency: 4<br/>Pool: prefork<br/>Memory: 800MB"]
        W3["Worker 3<br/>Concurrency: 4<br/>Pool: prefork<br/>Memory: 800MB"]
    end
    
    subgraph "Result Backend"
        REDIS_RESULT["Redis DB 2<br/>Result storage<br/>TTL: 24h"]
    end
    
    subgraph "Monitoring"
        FLOWER["Flower UI<br/>Port: 5555<br/>Features:<br/>- Task list<br/>- Worker status<br/>- Basic charts<br/>Memory: 100MB"]
    end

    BEAT --> REDIS_BROKER
    REDIS_BROKER --> W1
    REDIS_BROKER --> W2
    REDIS_BROKER --> W3
    
    W1 --> REDIS_RESULT
    W2 --> REDIS_RESULT
    W3 --> REDIS_RESULT
    
    W1 -.Monitor.-> FLOWER
    W2 -.Monitor.-> FLOWER
    W3 -.Monitor.-> FLOWER

    style BEAT fill:#ff6b6b
    style REDIS_BROKER fill:#dc244c
```

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏:**

```python
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Celery
CELERY_CONFIG = {
    'broker_url': 'redis://localhost:6379/1',
    'result_backend': 'redis://localhost:6379/2',
    'task_serializer': 'json',
    'result_serializer': 'json',
    'accept_content': ['json'],
    'timezone': 'UTC',
    'enable_utc': True,
    'task_track_started': True,
    'task_time_limit': 1800,  # 30 min
    'task_soft_time_limit': 1500,  # 25 min
    'worker_prefetch_multiplier': 1,
    'task_acks_late': True,
}

# Beat schedule
CELERYBEAT_SCHEDULE = {
    'retrain-models-daily': {
        'task': 'workers.ml_tasks.retrain_all_models',
        'schedule': crontab(hour=2, minute=0),
        'options': {'queue': 'ml_heavy'}
    },
    # ... 4 more tasks
}
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
```
‚ùå –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏ - –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã
   retrain-models –¥–æ–ª–∂–µ–Ω –∏–¥—Ç–∏ –ü–û–°–õ–ï update-feature-store
   –Ω–æ —ç—Ç–æ –Ω–µ –≤–∏–¥–Ω–æ –≤ –∫–æ–¥–µ
   
‚ùå –ï—Å–ª–∏ retrain-models —É–ø–∞–¥–µ—Ç –≤ 2:05
   cleanup-old-experiments –≤—Å—ë —Ä–∞–≤–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—Å—è –≤ 1:00 (–Ω–∞ —Å–ª–µ–¥. –¥–µ–Ω—å)
   –¥–∞–∂–µ –µ—Å–ª–∏ –∑–∞–≤–∏—Å–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –≥–æ—Ç–æ–≤—ã
   
‚ùå Flower UI –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∏–µ/–Ω–µ–¥–∞–≤–Ω–∏–µ –∑–∞–¥–∞—á–∏
   –Ω–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏ –∑–∞ –º–µ—Å—è—Ü
   –Ω–µ—Ç Gantt chart
   –Ω–µ—Ç SLA tracking
   
‚ùå Retry –ª–æ–≥–∏–∫–∞ - –≥–ª–æ–±–∞–ª—å–Ω–∞—è
   –Ω–µ–ª—å–∑—è –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å per-task –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–¥–∞
```

---

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç 2: Workflow Orchestrator (TypeScript)

```mermaid
sequenceDiagram
    participant Client
    participant WF as Workflow Orchestrator
    participant DB as Supabase
    participant A1 as Architect Agent
    participant A2 as Developer Agent
    participant A3 as Consultant Agent

    Client->>WF: POST /workflow-orchestrator<br/>{user_task, demo_type}
    WF->>DB: Create demo record
    DB-->>WF: demo_id
    WF->>DB: Create 3 stages
    WF-->>Client: 202 Accepted {demo_id}
    
    Note over WF: Async processing starts
    
    WF->>DB: Update stage 1: processing
    WF->>A1: Call analyze-task
    A1-->>WF: analysis_result
    WF->>DB: Update stage 1: completed
    
    WF->>DB: Update stage 2: processing
    WF->>A2: Call develop-solution<br/>(with analysis_result)
    A2-->>WF: solution_result
    WF->>DB: Update stage 2: completed
    
    WF->>DB: Update stage 3: processing
    WF->>A3: Call provide-consultation<br/>(with solution_result)
    A3-->>WF: consultation_result
    WF->>DB: Update stage 3: completed
    
    WF->>DB: Update demo: completed<br/>progress: 100%
```

**–ö–æ–¥:**
```typescript
async function processWorkflow(demoId, userTask, stages) {
    // Hardcoded –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    analysisResult = await callAgent('analyze-task', {...});
    solutionResult = await callAgent('develop-solution', {
        analysis: analysisResult  // –ü–µ—Ä–µ–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ
    });
    await callAgent('provide-consultation', {
        solution: solutionResult
    });
}
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
```
‚ùå Hardcoded –ø–æ—Ä—è–¥–æ–∫ (1‚Üí2‚Üí3)
   –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, conditional branches
   
‚ùå –ù–µ—Ç retry –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤
   –ï—Å–ª–∏ Developer Agent —É–ø–∞–ª - –≤–µ—Å—å workflow –ø–∞–¥–∞–µ—Ç
   
‚ùå –ù–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
   –ú–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ –ë–î (demo_stages)
   
‚ùå –ù–µ—Ç scheduling
   –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤—Ä—É—á–Ω—É—é
   
‚ùå –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ - —Å–ª–æ–∂–Ω–æ–µ
   –í—Å–µ –∞–≥–µ–Ω—Ç—ã –≤ –æ–¥–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
```

---

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç 3: AI Orchestrator (Real-time Routing)

```mermaid
graph LR
    REQ[User Request] --> CLASS[Query Classifier]
    
    CLASS --> ROUTE{Route Decision}
    
    ROUTE -->|Code Gen| QWEN[Qwen3-Coder]
    ROUTE -->|Search| QDRANT[Qdrant Search]
    ROUTE -->|Graph| NEO4J[Neo4j Query]
    ROUTE -->|Analysis| GPT4[OpenAI GPT-4]
    
    QWEN --> RESP[Response]
    QDRANT --> RESP
    NEO4J --> RESP
    GPT4 --> RESP

    style CLASS fill:#00d4aa
    style ROUTE fill:#ff6b6b
```

**–≠—Ç–æ –ù–ï workflow orchestration** - —ç—Ç–æ intelligent routing –¥–ª—è real-time –∑–∞–ø—Ä–æ—Å–æ–≤.

**Airflow –∑–¥–µ—Å—å –ù–ï –Ω—É–∂–µ–Ω** - —ç—Ç–æ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å (<2s latency —Ç—Ä–µ–±—É–µ—Ç—Å—è).

---

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç 4: Background Jobs (PostgreSQL Queue)

```mermaid
graph TB
    APP[FastAPI App] -->|Insert| QUEUE[(background_jobs table)]
    
    QUEUE -->|Poll every 5s| WORKER[Background Worker<br/>Python script]
    
    WORKER -->|Update status| QUEUE
    
    WORKER --> EXEC[Execute Job:<br/>- Export data<br/>- Send email<br/>- Generate report]
    
    EXEC -->|Success| QUEUE
    EXEC -->|Error| RETRY{Retry?}
    
    RETRY -->|Yes, retry_count < 3| QUEUE
    RETRY -->|No| FAILED[Mark as failed]

    style QUEUE fill:#336791
    style WORKER fill:#ff6b6b
```

**–ö–æ–¥:**
```sql
-- –ü—Ä–æ—Å—Ç–∞—è –æ—á–µ—Ä–µ–¥—å –≤ PostgreSQL
CREATE TABLE background_jobs (
    id UUID PRIMARY KEY,
    job_type VARCHAR(100),
    status VARCHAR(50) DEFAULT 'pending',
    retry_count INTEGER DEFAULT 0,
    max_retries INTEGER DEFAULT 3,
    ...
);

-- Worker poll
SELECT * FROM background_jobs 
WHERE status = 'pending' 
  AND (next_retry_at IS NULL OR next_retry_at < NOW())
ORDER BY created_at
LIMIT 10
FOR UPDATE SKIP LOCKED;
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
```
‚ùå Polling –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥ (–Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ)
‚ùå –ù–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –∑–∞–¥–∞—á
‚ùå –ù–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
‚ùå –†—É—á–Ω–æ–π retry –º–µ—Ö–∞–Ω–∏–∑–º
‚ùå –ù–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞–¥–∞—á
```

---

## üÜö Airflow Architecture (TO-BE)

### –ü–æ–ª–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Apache Airflow

```mermaid
graph TB
    subgraph "Airflow Components"
        WEBSERVER["Airflow Webserver<br/>Port: 8080<br/>Flask app<br/>Memory: 500MB<br/>Features:<br/>- DAG visualization<br/>- Task logs<br/>- Graph view<br/>- Gantt chart<br/>- SLA monitoring"]
        
        SCHEDULER["Airflow Scheduler<br/>Process: daemon<br/>Memory: 300MB<br/>Functions:<br/>- Parse DAG files<br/>- Schedule tasks<br/>- Trigger execution<br/>- SLA checks<br/>Poll interval: 5s"]
        
        EXECUTOR{{"Executor<br/>(CeleryExecutor)"}}
        
        METADATA[(PostgreSQL<br/>Airflow Metadata DB<br/>Tables:<br/>- dag<br/>- dag_run<br/>- task_instance<br/>- task_fail<br/>- sla_miss<br/>- log<br/>- xcom<br/>~30 tables total)]
    end
    
    subgraph "Task Execution Layer"
        BROKER["Redis/RabbitMQ<br/>Task Queue<br/>Queues:<br/>- default<br/>- ml_heavy<br/>- ml_light<br/>- data_sync"]
        
        WORKER1["Celery Worker 1<br/>Queue: ml_heavy<br/>Concurrency: 2<br/>Memory: 2GB"]
        WORKER2["Celery Worker 2<br/>Queue: ml_light<br/>Concurrency: 8<br/>Memory: 1GB"]
        WORKER3["Celery Worker 3<br/>Queue: data_sync<br/>Concurrency: 4<br/>Memory: 512MB"]
    end
    
    subgraph "DAG Storage"
        DAGS["DAG Files<br/>Path: /dags/<br/>Files:<br/>- ml_training_pipeline.py<br/>- data_sync_pipeline.py<br/>- maintenance_pipeline.py<br/>Auto-discovery<br/>Scan interval: 30s"]
    end
    
    subgraph "Logs & XCom"
        LOGS["Task Logs<br/>Storage: S3/Local<br/>Retention: 30 days"]
        XCOM["XCom (data passing)<br/>Storage: Metadata DB<br/>Max size: 48KB"]
    end

    SCHEDULER --> METADATA
    SCHEDULER --> DAGS
    SCHEDULER --> EXECUTOR
    
    EXECUTOR --> BROKER
    BROKER --> WORKER1
    BROKER --> WORKER2
    BROKER --> WORKER3
    
    WORKER1 --> METADATA
    WORKER2 --> METADATA
    WORKER3 --> METADATA
    
    WORKER1 --> LOGS
    WORKER1 --> XCOM
    
    WEBSERVER --> METADATA
    WEBSERVER --> LOGS

    style SCHEDULER fill:#ff6b6b
    style WEBSERVER fill:#00d4aa
    style METADATA fill:#336791
```

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏:**

```python
# airflow.cfg
[core]
dags_folder = /opt/airflow/dags
executor = CeleryExecutor
sql_alchemy_conn = postgresql+psycopg2://airflow:***@postgres:5432/airflow
parallelism = 32  # Max parallel tasks globally
max_active_runs_per_dag = 16
load_examples = False

[scheduler]
scheduler_heartbeat_sec = 5
min_file_process_interval = 30  # Scan DAG folder
dag_dir_list_interval = 300

[celery]
broker_url = redis://redis:6379/0
result_backend = db+postgresql://airflow:***@postgres:5432/airflow
worker_concurrency = 16
worker_prefetch_multiplier = 1

[webserver]
web_server_port = 8080
workers = 4  # Gunicorn workers
worker_class = sync
expose_config = False
authenticate = True
auth_backend = airflow.contrib.auth.backends.password_auth

[logging]
base_log_folder = /opt/airflow/logs
remote_logging = True
remote_base_log_folder = s3://airflow-logs/
logging_level = INFO
```

---

## üîÑ –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö workflow

### ML Training Pipeline

#### –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (Celery):

```python
# src/workers/ml_tasks.py

@celery_app.task(
    name='workers.ml_tasks.retrain_all_models',
    bind=True,
    max_retries=3,
    default_retry_delay=300
)
def retrain_all_models(self):
    """–ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏"""
    try:
        # 1. Update feature store (–Ω—É–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å –≤—Ä—É—á–Ω—É—é –∏–ª–∏ –Ω–∞–¥–µ—è—Ç—å—Å—è —á—Ç–æ –æ—Ç—Ä–∞–±–æ—Ç–∞–ª)
        # 2. Load data
        training_data = load_training_data()
        
        # 3. Train each model
        models = ['classifier_v1', 'regressor_v1', 'embeddings_v1']
        for model_name in models:
            train_single_model(model_name, training_data)
        
        # 4. Evaluate (–≥–¥–µ –≥–∞—Ä–∞–Ω—Ç–∏—è —á—Ç–æ training –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ?)
        
        # 5. Log to MLflow
        
        return {'status': 'success', 'models_trained': len(models)}
    except Exception as e:
        self.retry(exc=e)
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
```
‚ùå –í—Å–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (–º–µ–¥–ª–µ–Ω–Ω–æ!)
   classifier_v1: 15 min
   regressor_v1: 20 min
   embeddings_v1: 10 min
   TOTAL: 45 –º–∏–Ω—É—Ç –≤–º–µ—Å—Ç–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö 20 (–µ—Å–ª–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)

‚ùå –ï—Å–ª–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å —É–ø–∞–ª–∞ - retry –≤—Å–µ—Ö
   –ù—É–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å –≤—Å–µ 3 –º–æ–¥–µ–ª–∏ –∑–∞–Ω–æ–≤–æ

‚ùå –ù–µ—Ç visibility –∫–∞–∫–∞—è –º–æ–¥–µ–ª—å —Å–µ–π—á–∞—Å –æ–±—É—á–∞–µ—Ç—Å—è
   –í–∏–¥–Ω–æ —Ç–æ–ª—å–∫–æ "retrain_all_models: RUNNING"

‚ùå –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ—è–≤–Ω—ã–µ
   retrain_all_models –∑–∞–≤–∏—Å–∏—Ç –æ—Ç update_feature_store
   –Ω–æ –≤ –∫–æ–¥–µ —ç—Ç–æ –Ω–µ –æ—Ç—Ä–∞–∂–µ–Ω–æ
```

---

#### –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç –≤ Airflow:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.branch import BranchPythonOperator
from airflow.sensors.external_task import ExternalTaskSensor
from datetime import datetime, timedelta

with DAG(
    'ml_training_pipeline_v2',
    default_args={
        'owner': '1c-ai-ml-team',
        'depends_on_past': True,  # –ù–µ –∑–∞–ø—É—Å–∫–∞—Ç—å –µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π run –Ω–µ —É—Å–ø–µ—à–µ–Ω
        'email': ['ml-team@1c-ai.dev'],
        'email_on_failure': True,
        'email_on_retry': False,
        'retries': 3,
        'retry_delay': timedelta(minutes=5),
        'execution_timeout': timedelta(hours=2),
    },
    description='ML Training Pipeline with parallel model training',
    schedule_interval='0 2 * * *',  # Daily at 2:00 AM
    start_date=datetime(2024, 1, 1),
    catchup=False,
    max_active_runs=1,  # –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω active run
    tags=['ml', 'training', 'production'],
) as dag:

    # Sensor: Wait for feature store update (if needed)
    wait_for_features = ExternalTaskSensor(
        task_id='wait_for_feature_update',
        external_dag_id='feature_store_update',
        external_task_id='update_complete',
        timeout=3600,  # 1 hour
        poke_interval=60,  # Check every minute
    )
    
    # Task: Load and prepare data
    prepare_data = PythonOperator(
        task_id='prepare_training_data',
        python_callable=load_and_prepare_data,
        pool='data_processing',  # Resource pool
    )
    
    # Task: Validate data quality
    validate_data = PythonOperator(
        task_id='validate_data_quality',
        python_callable=validate_data_quality,
    )
    
    # Branch: Choose models to train based on drift detection
    def choose_models_to_train(**context):
        drift_check = context['ti'].xcom_pull(task_ids='check_model_drift')
        if drift_check['high_drift']:
            return ['train_classifier', 'train_regressor', 'train_embeddings']
        elif drift_check['medium_drift']:
            return ['train_classifier', 'train_regressor']
        else:
            return ['skip_training']
    
    branch_training = BranchPythonOperator(
        task_id='decide_which_models',
        python_callable=choose_models_to_train,
    )
    
    # PARALLEL Training tasks (–∫–ª—é—á–µ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ!)
    train_classifier = PythonOperator(
        task_id='train_classifier',
        python_callable=train_classifier_model,
        pool='ml_training',
        pool_slots=2,  # –ó–∞–Ω–∏–º–∞–µ—Ç 2 —Å–ª–æ—Ç–∞ –∏–∑ pool
        execution_timeout=timedelta(minutes=30),
    )
    
    train_regressor = PythonOperator(
        task_id='train_regressor',
        python_callable=train_regressor_model,
        pool='ml_training',
        pool_slots=2,
        execution_timeout=timedelta(minutes=30),
    )
    
    train_embeddings = PythonOperator(
        task_id='train_embeddings',
        python_callable=train_embeddings_model,
        pool='ml_training',
        pool_slots=1,
        execution_timeout=timedelta(minutes=20),
    )
    
    skip_training = PythonOperator(
        task_id='skip_training',
        python_callable=lambda: print("No training needed")
    )
    
    # Evaluation tasks (–ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è)
    evaluate_all = PythonOperator(
        task_id='evaluate_all_models',
        python_callable=evaluate_models,
        trigger_rule='none_failed',  # –ó–∞–ø—É—Å—Ç–∏—Ç—å –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞
    )
    
    # Model registry update
    update_registry = PythonOperator(
        task_id='update_model_registry',
        python_callable=update_mlflow_registry,
    )
    
    # Cleanup
    cleanup = PythonOperator(
        task_id='cleanup_old_experiments',
        python_callable=cleanup_experiments,
        trigger_rule='all_done',  # –í—Å–µ–≥–¥–∞ –∑–∞–ø—É—Å–∫–∞—Ç—å, –¥–∞–∂–µ –µ—Å–ª–∏ –±—ã–ª–∏ –æ—à–∏–±–∫–∏
    )
    
    # Send notification
    notify = PythonOperator(
        task_id='send_slack_notification',
        python_callable=send_notification,
        trigger_rule='all_done',
    )

    # DAG Dependencies (–≤–∏–∑—É–∞–ª—å–Ω–æ –≤–∏–¥–Ω–æ!)
    wait_for_features >> prepare_data >> validate_data >> branch_training
    
    branch_training >> [train_classifier, train_regressor, train_embeddings, skip_training]
    
    [train_classifier, train_regressor, train_embeddings] >> evaluate_all
    skip_training >> notify
    
    evaluate_all >> update_registry >> cleanup >> notify
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Airflow –≤–µ—Ä—Å–∏–∏:**

```
‚úÖ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
   classifier + regressor + embeddings = –û–î–ù–û–í–†–ï–ú–ï–ù–ù–û
   –í—Ä–µ–º—è: 45 min ‚Üí 20 min (—ç–∫–æ–Ω–æ–º–∏—è 55%!)

‚úÖ Conditional execution
   –û–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –º–æ–¥–µ–ª–∏ –≥–¥–µ –µ—Å—Ç—å drift
   
‚úÖ Explicit dependencies
   –í–∏–¥–Ω–æ –Ω–∞ –≥—Ä–∞—Ñ–µ —á—Ç–æ –æ—Ç —á–µ–≥–æ –∑–∞–≤–∏—Å–∏—Ç
   
‚úÖ Smart retry
   –ï—Å–ª–∏ classifier —É–ø–∞–ª - retry —Ç–æ–ª—å–∫–æ –µ–≥–æ
   regressor –∏ embeddings –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç
   
‚úÖ Visibility –≤ UI
   –í–∏–¥–Ω–æ –∫–∞–∫–∞—è –º–æ–¥–µ–ª—å —Å–µ–π—á–∞—Å –æ–±—É—á–∞–µ—Ç—Å—è
   –°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–æ—à–ª–æ
   –ö–æ–≥–¥–∞ –æ–∂–∏–¥–∞–µ—Ç—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
   
‚úÖ Resource management
   Pools –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—Ç –ø–µ—Ä–µ–≥—Ä—É–∑–∫—É
   
‚úÖ Trigger rules
   cleanup –∑–∞–ø—É—Å—Ç–∏—Ç—Å—è –í–°–ï–ì–î–ê
   evaluate —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã 1 –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞
```

---

## üìä Side-by-Side Comparison: –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä

### –°—Ü–µ–Ω–∞—Ä–∏–π: –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π ML Pipeline

**–ó–∞–¥–∞—á–∏:**
1. Update feature store (10 min)
2. Check model drift (5 min)
3. Train 3 models (15+20+10 = 45 min sequential, 20 min parallel)
4. Evaluate models (10 min)
5. Update registry (2 min)
6. Cleanup old experiments (5 min)

---

#### Celery (—Ç–µ–∫—É—â–µ–µ):

**–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:**
```
02:00 - retrain-models-daily starts
  02:00-02:10 - update_feature_store (–≤ –¥—Ä—É–≥–æ–π –∑–∞–¥–∞—á–µ, –Ω–∞–¥–µ–µ–º—Å—è —á—Ç–æ –æ—Ç—Ä–∞–±–æ—Ç–∞–ª–∞)
  02:10-02:15 - check_model_drift (–≤ –¥—Ä—É–≥–æ–π –∑–∞–¥–∞—á–µ)
  02:15-02:30 - train classifier
  02:30-02:50 - train regressor
  02:50-03:00 - train embeddings
  03:00-03:10 - evaluate
  03:10-03:12 - update registry
  03:12-03:17 - cleanup (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –∑–∞–¥–∞—á–µ –≤ 1:00, –Ω–æ –≤—á–µ—Ä–∞—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ!)

TOTAL: ~1 —á–∞—Å 17 –º–∏–Ω—É—Ç
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- –ó–∞–¥–∞—á–∏ —Ä–∞–∑–±—Ä–æ—Å–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏
- –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ—è–≤–Ω—ã–µ
- Cleanup –Ω–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω
- –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ —É–ø–∞–ª–æ - –Ω–µ—è—Å–Ω–æ —á—Ç–æ –¥–µ–ª–∞—Ç—å

**–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**
```
Flower UI:
  retrain-models-daily: SUCCESS (took 1h 17m)
  
–ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ –≤–Ω—É—Ç—Ä–∏? –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –±–µ–∑ –ª–æ–≥–æ–≤!
```

---

#### Airflow (–ø—Ä–µ–¥–ª–∞–≥–∞–µ–º–æ–µ):

**–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:**
```
02:00 - ml_training_pipeline DAG triggered

02:00-02:10 - [wait_for_features] Sensor wait
02:10-02:15 - [prepare_data] Load data
02:15-02:20 - [validate_data] Validate
02:20-02:22 - [decide_which_models] Check drift ‚Üí train all 3

02:22-02:42 - PARALLEL execution:
              [train_classifier] - 20 min
              [train_regressor]  - 20 min
              [train_embeddings] - 10 min (–∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Ä–∞–Ω—å—à–µ)

02:42-02:52 - [evaluate_all_models] - 10 min
02:52-02:54 - [update_registry] - 2 min
02:54-02:59 - [cleanup] - 5 min
02:59-03:00 - [notify] - 1 min

TOTAL: 1 —á–∞—Å (–≤–º–µ—Å—Ç–æ 1h 17m)
–≠–∫–æ–Ω–æ–º–∏—è: 17 –º–∏–Ω—É—Ç (22%!)
```

**–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ UI:**
```
Airflow UI:
  DAG: ml_training_pipeline
  Run: 2024-11-05 02:00:00
  Status: SUCCESS
  Duration: 1h 0m
  
  Tasks (10):
    ‚úÖ wait_for_features (10m)
    ‚úÖ prepare_data (5m)
    ‚úÖ validate_data (5m)
    ‚úÖ decide_which_models (2m)
    ‚úÖ train_classifier (20m) ‚Üê PARALLEL
    ‚úÖ train_regressor (20m)  ‚Üê PARALLEL
    ‚úÖ train_embeddings (10m) ‚Üê PARALLEL
    ‚úÖ evaluate_all_models (10m)
    ‚úÖ update_registry (2m)
    ‚úÖ cleanup (5m)
    ‚úÖ notify (1m)
  
  Gantt Chart: [–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–≥–¥–∞ —á—Ç–æ –≤—ã–ø–æ–ª–Ω—è–ª–æ—Å—å]
  Graph View: [–≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å —Ü–≤–µ—Ç–∞–º–∏]
  Logs: [–∫–ª–∏–∫ –Ω–∞ –ª—é–±–æ–π task ‚Üí –ª–æ–≥–∏]
```

---

## üìà –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã DAG'–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞

### DAG 1: Data Sync Pipeline (–Ω–æ–≤—ã–π!)

**–ü—Ä–æ–±–ª–µ–º–∞:** –°–µ–π—á–∞—Å –Ω–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –ë–î.

**–†–µ—à–µ–Ω–∏–µ —Å Airflow:**

```python
with DAG(
    'data_sync_1c_to_all_dbs',
    schedule_interval='0 */6 * * *',  # Every 6 hours
    start_date=datetime(2024, 1, 1),
) as dag:

    # Extract from 1C configurations
    extract_1c = PythonOperator(
        task_id='extract_1c_metadata',
        python_callable=extract_from_1c_configs,
    )
    
    # Transform
    transform = PythonOperator(
        task_id='transform_metadata',
        python_callable=transform_metadata_func,
    )
    
    # Load to PostgreSQL
    load_postgres = PostgresOperator(
        task_id='load_to_postgres',
        postgres_conn_id='postgres_1c_ai',
        sql="""
            INSERT INTO metadata (name, type, properties)
            SELECT * FROM temp_metadata
            ON CONFLICT (id) DO UPDATE SET ...
        """
    )
    
    # PARALLEL: Load to graph and vector stores
    load_neo4j = PythonOperator(
        task_id='sync_to_neo4j',
        python_callable=sync_postgres_to_neo4j,
    )
    
    vectorize_qdrant = PythonOperator(
        task_id='vectorize_to_qdrant',
        python_callable=vectorize_and_load_qdrant,
    )
    
    index_elasticsearch = PythonOperator(
        task_id='index_to_elasticsearch',
        python_callable=index_to_elasticsearch,
    )
    
    # Validation
    validate_sync = PythonOperator(
        task_id='validate_sync_integrity',
        python_callable=validate_data_integrity,
    )
    
    # Dependencies
    extract_1c >> transform >> load_postgres
    load_postgres >> [load_neo4j, vectorize_qdrant, index_elasticsearch]
    [load_neo4j, vectorize_qdrant, index_elasticsearch] >> validate_sync
```

**–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ Airflow UI:**
```
extract_1c ‚Üí transform ‚Üí load_postgres ‚Üí ‚î¨‚îÄ‚Üí load_neo4j ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                          ‚îú‚îÄ‚Üí vectorize_qdrant ‚îÄ‚îÄ‚îº‚îÄ‚Üí validate_sync
                                          ‚îî‚îÄ‚Üí index_elasticsearch‚îò
```

**–í—ã–≥–æ–¥—ã:**
- ‚úÖ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ 3 –ë–î (—ç–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏!)
- ‚úÖ –í–∏–¥–Ω–æ –Ω–∞ –≥—Ä–∞—Ñ–µ –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å
- ‚úÖ –ï—Å–ª–∏ Neo4j —É–ø–∞–ª - Qdrant –∏ ES –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç
- ‚úÖ Validation –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö

---

### DAG 2: BSL Dataset Preparation

**–ü—Ä–æ–±–ª–µ–º–∞:** –°–µ–π—á–∞—Å –¥–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤–∏—Ç—Å—è –≤—Ä—É—á–Ω—É—é —Å–∫—Ä–∏–ø—Ç–æ–º.

**–†–µ—à–µ–Ω–∏–µ —Å Airflow:**

```python
with DAG(
    'bsl_dataset_preparation',
    schedule_interval='0 0 * * 0',  # Weekly on Sunday
    start_date=datetime(2024, 1, 1),
) as dag:

    # Fetch from multiple sources (PARALLEL!)
    fetch_postgres = PythonOperator(
        task_id='fetch_from_postgres',
        python_callable=fetch_bsl_from_postgres,
    )
    
    fetch_github = PythonOperator(
        task_id='fetch_from_github',
        python_callable=fetch_bsl_from_github_repos,
        execution_timeout=timedelta(hours=2),
    )
    
    fetch_its = PythonOperator(
        task_id='fetch_from_its_library',
        python_callable=fetch_bsl_from_its,
        execution_timeout=timedelta(hours=1),
    )
    
    # Merge and deduplicate
    merge_datasets = PythonOperator(
        task_id='merge_and_deduplicate',
        python_callable=merge_all_sources,
        trigger_rule='none_failed',  # –î–∞–∂–µ –µ—Å–ª–∏ GitHub —É–ø–∞–ª
    )
    
    # Quality filtering
    filter_quality = PythonOperator(
        task_id='filter_low_quality',
        python_callable=filter_by_quality_score,
    )
    
    # Split train/val/test
    split_dataset = PythonOperator(
        task_id='split_dataset',
        python_callable=split_train_val_test,
    )
    
    # PARALLEL: Save in different formats
    save_jsonl = PythonOperator(
        task_id='save_as_jsonl',
        python_callable=save_jsonl_format,
    )
    
    save_parquet = PythonOperator(
        task_id='save_as_parquet',
        python_callable=save_parquet_format,
    )
    
    # Upload to dataset storage
    upload = PythonOperator(
        task_id='upload_to_huggingface',
        python_callable=upload_to_hf_hub,
    )
    
    # Validate dataset
    validate = PythonOperator(
        task_id='validate_dataset_quality',
        python_callable=validate_final_dataset,
    )
    
    # Dependencies
    [fetch_postgres, fetch_github, fetch_its] >> merge_datasets
    merge_datasets >> filter_quality >> split_dataset
    split_dataset >> [save_jsonl, save_parquet]
    [save_jsonl, save_parquet] >> upload >> validate
```

**–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:**
```
Celery (sequential):
  fetch_postgres: 30 min
  fetch_github: 120 min
  fetch_its: 60 min
  merge: 15 min
  filter: 10 min
  split: 5 min
  save: 20 min
  upload: 30 min
  TOTAL: 290 min (4.8 hours)

Airflow (parallel):
  [fetch_postgres + fetch_github + fetch_its]: 120 min (longest)
  merge: 15 min
  filter: 10 min
  split: 5 min
  [save_jsonl + save_parquet]: 20 min (parallel)
  upload: 30 min
  validate: 5 min
  TOTAL: 205 min (3.4 hours)
  
–≠–ö–û–ù–û–ú–ò–Ø: 85 –º–∏–Ω—É—Ç (29%!)
```

---

### DAG 3: System Maintenance

```python
with DAG(
    'system_maintenance',
    schedule_interval='0 1 * * *',  # Daily at 1:00 AM
    start_date=datetime(2024, 1, 1),
) as dag:

    # Database maintenance (PARALLEL –¥–ª—è –∫–∞–∂–¥–æ–π –ë–î)
    vacuum_postgres = PostgresOperator(
        task_id='vacuum_postgres',
        postgres_conn_id='postgres_1c_ai',
        sql='VACUUM ANALYZE; REINDEX DATABASE 1c_ai_stack;'
    )
    
    cleanup_neo4j = PythonOperator(
        task_id='cleanup_neo4j_orphans',
        python_callable=cleanup_neo4j_orphaned_nodes,
    )
    
    optimize_qdrant = PythonOperator(
        task_id='optimize_qdrant_collections',
        python_callable=optimize_qdrant_indexes,
    )
    
    # Cache cleanup
    cleanup_redis = PythonOperator(
        task_id='cleanup_redis_cache',
        python_callable=cleanup_old_cache_keys,
    )
    
    # Log cleanup (—É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤)
    cleanup_logs = PythonOperator(
        task_id='cleanup_old_logs',
        python_callable=delete_logs_older_than_30_days,
    )
    
    # Health check –ø–æ—Å–ª–µ maintenance
    health_check = PythonOperator(
        task_id='system_health_check',
        python_callable=comprehensive_health_check,
        trigger_rule='all_done',
    )
    
    # Alert if health check failed
    alert_on_failure = PythonOperator(
        task_id='alert_if_unhealthy',
        python_callable=send_alert_to_oncall,
        trigger_rule='one_failed',  # –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –∑–∞–¥–∞—á–∞ —É–ø–∞–ª–∞
    )
    
    # Dependencies
    [vacuum_postgres, cleanup_neo4j, optimize_qdrant, cleanup_redis, cleanup_logs] >> health_check
    health_check >> alert_on_failure
```

**–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ Airflow:**
```
vacuum_postgres ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
cleanup_neo4j ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
optimize_qdrant ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚Üí health_check ‚îÄ‚Üí alert_on_failure
cleanup_redis ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
cleanup_logs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[–í—Å–µ 5 –∑–∞–¥–∞—á –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ] ‚Üí [Health check] ‚Üí [Alert –µ—Å–ª–∏ –Ω—É–∂–Ω–æ]
```

---

## üé® Airflow UI Features (—á–µ–≥–æ –Ω–µ—Ç –≤ Celery)

### 1. DAG Graph View

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DAG: ml_training_pipeline                           ‚îÇ
‚îÇ Status: ‚úÖ Success  Duration: 1h 0m  Next: Tomorrow ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ     wait_for_features                               ‚îÇ
‚îÇ           ‚Üì                                         ‚îÇ
‚îÇ     prepare_data                                    ‚îÇ
‚îÇ           ‚Üì                                         ‚îÇ
‚îÇ     validate_data                                   ‚îÇ
‚îÇ           ‚Üì                                         ‚îÇ
‚îÇ     decide_which_models                             ‚îÇ
‚îÇ           ‚Üì                                         ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ     ‚Üì            ‚Üì           ‚Üì                      ‚îÇ
‚îÇ  train_class  train_reg  train_emb                 ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ           ‚Üì                                         ‚îÇ
‚îÇ     evaluate_all                                    ‚îÇ
‚îÇ           ‚Üì                                         ‚îÇ
‚îÇ     update_registry                                 ‚îÇ
‚îÇ           ‚Üì                                         ‚îÇ
‚îÇ     cleanup                                         ‚îÇ
‚îÇ           ‚Üì                                         ‚îÇ
‚îÇ     notify                                          ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ Colors:                                             ‚îÇ
‚îÇ üü¢ Success  üî¥ Failed  üü° Running  ‚ö™ Not started  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. Gantt Chart

```
Task                 |00:00|01:00|02:00|03:00|
---------------------|-----|-----|-----|-----|
wait_for_features    |‚ñà‚ñà‚ñà‚ñà‚ñà|     |     |     |
prepare_data         |     |‚ñà‚ñà‚ñà‚ñà |     |     |
validate_data        |     | ‚ñà‚ñà‚ñà‚ñà|     |     |
train_classifier     |     |     |‚ñà‚ñà‚ñà‚ñà‚ñà|‚ñà‚ñà‚ñà‚ñà‚ñà|
train_regressor      |     |     |‚ñà‚ñà‚ñà‚ñà‚ñà|‚ñà‚ñà‚ñà‚ñà‚ñà|
train_embeddings     |     |     |‚ñà‚ñà‚ñà‚ñà‚ñà|     |
evaluate_all         |     |     |     |‚ñà‚ñà‚ñà‚ñà |
cleanup              |     |     |     | ‚ñà‚ñà‚ñà |

–õ–µ–≥–∫–æ –≤–∏–¥–µ—Ç—å:
- –ì–¥–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º
- –ì–¥–µ bottleneck'–∏
- –°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–Ω—è–ª –∫–∞–∂–¥—ã–π task
```

### 3. Task Instance Details

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Task: train_classifier                  ‚îÇ
‚îÇ Run: 2024-11-05 02:22:00               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Status: ‚úÖ Success                      ‚îÇ
‚îÇ Duration: 19m 43s                       ‚îÇ
‚îÇ Try Number: 1 / 3                       ‚îÇ
‚îÇ Queue: ml_heavy                         ‚îÇ
‚îÇ Pool: ml_training (2/10 slots used)    ‚îÇ
‚îÇ Executor: CeleryExecutor                ‚îÇ
‚îÇ Hostname: celery-worker-2               ‚îÇ
‚îÇ PID: 42351                              ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ XCom Values:                            ‚îÇ
‚îÇ   model_path: /models/classifier_v1.pkl ‚îÇ
‚îÇ   accuracy: 0.9234                      ‚îÇ
‚îÇ   training_samples: 10000               ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ Logs: [Full Task Log ‚Üì]                ‚îÇ
‚îÇ 2024-11-05 02:22:00 - INFO - Starting  ‚îÇ
‚îÇ 2024-11-05 02:25:00 - INFO - Epoch 1/10‚îÇ
‚îÇ ...                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Celery Flower —Ç–∞–∫–æ–≥–æ –ù–ï –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç!

---

## üíª –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞: –ú–∏–≥—Ä–∞—Ü–∏—è —Å Celery –Ω–∞ Airflow

### Before (Celery):

```python
# src/workers/ml_tasks.py

@celery_app.task(bind=True, max_retries=3)
def retrain_all_models(self):
    """–ü–µ—Ä–µ–æ–±—É—á–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏"""
    try:
        models = ['classifier', 'regressor', 'embeddings']
        results = []
        
        for model_name in models:
            result = train_single_model(model_name)
            results.append(result)
        
        return {'status': 'success', 'models': results}
    except Exception as e:
        self.retry(exc=e, countdown=300)

# –ö–∞–∫ —ç—Ç–æ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ?
beat_schedule = {
    'retrain-models-daily': {
        'task': 'workers.ml_tasks.retrain_all_models',
        'schedule': crontab(hour=2, minute=0),
    }
}
```

**–ü—Ä–æ–±–ª–µ–º—ã:**
- –í—Å–µ –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–Ω–∏—Ä—É—é—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
- –ù–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
- Retry –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –µ—Å–ª–∏ –æ–¥–Ω–∞ —É–ø–∞–ª–∞

---

### After (Airflow):

```python
# dags/ml_training_pipeline.py

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from datetime import datetime, timedelta

# DAG definition
with DAG(
    'ml_training_pipeline',
    default_args={
        'owner': 'ml-team',
        'retries': 3,
        'retry_delay': timedelta(minutes=5),
    },
    description='ML models retraining pipeline',
    schedule_interval='0 2 * * *',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['ml', 'training'],
) as dag:

    # Task functions
    def train_classifier_func(**context):
        """Train classifier model"""
        from ml.training.trainer import ModelTrainer
        
        trainer = ModelTrainer()
        result = trainer.train('classifier_v1')
        
        # Push result to XCom (for next tasks)
        context['ti'].xcom_push(key='model_path', value=result['path'])
        context['ti'].xcom_push(key='accuracy', value=result['accuracy'])
        
        return result
    
    def train_regressor_func(**context):
        """Train regressor model"""
        # Similar logic
        pass
    
    def train_embeddings_func(**context):
        """Train embeddings model"""
        # Similar logic
        pass
    
    def evaluate_all_func(**context):
        """Evaluate all trained models"""
        # Pull results from previous tasks
        ti = context['ti']
        classifier_acc = ti.xcom_pull(task_ids='train_classifier', key='accuracy')
        regressor_acc = ti.xcom_pull(task_ids='train_regressor', key='accuracy')
        embeddings_acc = ti.xcom_pull(task_ids='train_embeddings', key='accuracy')
        
        return {
            'classifier': classifier_acc,
            'regressor': regressor_acc,
            'embeddings': embeddings_acc
        }
    
    # Define tasks
    train_classifier = PythonOperator(
        task_id='train_classifier',
        python_callable=train_classifier_func,
        pool='ml_training',
    )
    
    train_regressor = PythonOperator(
        task_id='train_regressor',
        python_callable=train_regressor_func,
        pool='ml_training',
    )
    
    train_embeddings = PythonOperator(
        task_id='train_embeddings',
        python_callable=train_embeddings_func,
        pool='ml_training',
    )
    
    evaluate = PythonOperator(
        task_id='evaluate_all_models',
        python_callable=evaluate_all_func,
        trigger_rule='none_failed',
    )
    
    # Update model registry in PostgreSQL
    update_registry = PostgresOperator(
        task_id='update_model_registry',
        postgres_conn_id='postgres_1c_ai',
        sql="""
            INSERT INTO ml.model_registry (model_name, version, accuracy, path)
            VALUES 
                ('classifier_v1', '{{ ds }}', {{ ti.xcom_pull(task_ids='train_classifier', key='accuracy') }}, '...'),
                ('regressor_v1', '{{ ds }}', {{ ti.xcom_pull(task_ids='train_regressor', key='accuracy') }}, '...'),
                ('embeddings_v1', '{{ ds }}', {{ ti.xcom_pull(task_ids='train_embeddings', key='accuracy') }}, '...')
        """
    )
    
    # Dependencies
    [train_classifier, train_regressor, train_embeddings] >> evaluate >> update_registry
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
```
‚úÖ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: 45 min ‚Üí 20 min
‚úÖ –í–∏–¥–Ω–æ –Ω–∞ –≥—Ä–∞—Ñ–µ —á—Ç–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
‚úÖ XCom –ø–µ—Ä–µ–¥–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –º–µ–∂–¥—É tasks
‚úÖ Templating –≤ SQL ({{ ti.xcom_pull(...) }})
‚úÖ –ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å - –æ—Ç–¥–µ–ª—å–Ω—ã–π task —Å –ª–æ–≥–∞–º–∏
‚úÖ Retry –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π
```

---

## üìä Resource Comparison

### Memory Usage

| Component | Celery (Current) | Airflow | –†–∞–∑–Ω–∏—Ü–∞ |
|-----------|------------------|---------|---------|
| Scheduler | 50MB (Beat) | 300MB | +250MB |
| Web UI | 100MB (Flower) | 500MB | +400MB |
| Workers (3x) | 2.4GB | 2.4GB | 0 |
| Broker | 100MB (Redis) | 100MB (Redis) | 0 |
| Metadata DB | - | 200MB (PostgreSQL) | +200MB |
| **TOTAL** | **2.65GB** | **3.5GB** | **+850MB** |

### CPU Usage

| Component | Celery | Airflow | –†–∞–∑–Ω–∏—Ü–∞ |
|-----------|--------|---------|---------|
| Scheduler | 5-10% | 10-15% | +5-10% |
| Web UI | 2-5% | 5-10% | +3-5% |
| Workers | 50-90% (when active) | 50-90% | 0 |
| **TOTAL** | **57-105%** | **65-115%** | **+8-10%** |

**–í—ã–≤–æ–¥:** +850MB RAM, +8-10% CPU - –ø—Ä–∏–µ–º–ª–µ–º—ã–π overhead –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.

---

## üéØ –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Airflow

### 1. Dynamic DAG Generation

**–°—Ü–µ–Ω–∞—Ä–∏–π:** –°–æ–∑–¥–∞—Ç—å DAG –¥–ª—è –∫–∞–∂–¥–æ–π 1C –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

```python
# dags/dynamic_config_dags.py

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

# –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –∏–∑ –ë–î
CONFIGURATIONS = [
    {'name': 'ERP', 'schedule': '0 2 * * *'},
    {'name': 'UT', 'schedule': '0 3 * * *'},
    {'name': 'ZUP', 'schedule': '0 4 * * *'},
]

# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º DAG –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
for config in CONFIGURATIONS:
    dag_id = f"sync_config_{config['name'].lower()}"
    
    with DAG(
        dag_id,
        schedule_interval=config['schedule'],
        start_date=datetime(2024, 1, 1),
        catchup=False,
    ) as dag:
        
        extract = PythonOperator(
            task_id='extract',
            python_callable=extract_config,
            op_kwargs={'config_name': config['name']},
        )
        
        sync = PythonOperator(
            task_id='sync_to_dbs',
            python_callable=sync_to_all_dbs,
        )
        
        extract >> sync
    
    # DAG –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è
    globals()[dag_id] = dag
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** 3 DAG'–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–Ω—ã!
- sync_config_erp
- sync_config_ut
- sync_config_zup

–í Celery —Ç–∞–∫–æ–µ —Å–¥–µ–ª–∞—Ç—å —Å–ª–æ–∂–Ω–æ.

---

### 2. Sub-DAGs (–ö–æ–º–ø–æ–∑–∏—Ü–∏—è workflow)

```python
# –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π sub-DAG –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
from airflow.utils.task_group import TaskGroup

def create_model_training_group(model_name):
    with TaskGroup(f"train_{model_name}") as group:
        
        prepare = PythonOperator(
            task_id='prepare_data',
            python_callable=prepare_data_for_model,
            op_kwargs={'model': model_name}
        )
        
        train = PythonOperator(
            task_id='train',
            python_callable=train_model,
            op_kwargs={'model': model_name}
        )
        
        evaluate = PythonOperator(
            task_id='evaluate',
            python_callable=evaluate_model,
            op_kwargs={'model': model_name}
        )
        
        prepare >> train >> evaluate
        
        return group

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º DAG
with DAG('ml_pipeline') as dag:
    start = DummyOperator(task_id='start')
    
    # 3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö sub-DAG'–∞
    classifier_group = create_model_training_group('classifier')
    regressor_group = create_model_training_group('regressor')
    embeddings_group = create_model_training_group('embeddings')
    
    end = DummyOperator(task_id='end')
    
    start >> [classifier_group, regressor_group, embeddings_group] >> end
```

**–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:**
```
start ‚Üí ‚î¨‚îÄ‚Üí [train_classifier: prepare ‚Üí train ‚Üí evaluate]
        ‚îú‚îÄ‚Üí [train_regressor: prepare ‚Üí train ‚Üí evaluate]
        ‚îî‚îÄ‚Üí [train_embeddings: prepare ‚Üí train ‚Üí evaluate]
        ‚Üí end
```

---

### 3. SLA Monitoring & Alerting

```python
with DAG(
    'ml_training_pipeline',
    default_args={
        'sla': timedelta(hours=2),  # Global SLA
    },
    sla_miss_callback=send_sla_miss_alert,
) as dag:

    train_classifier = PythonOperator(
        task_id='train_classifier',
        python_callable=train_func,
        sla=timedelta(minutes=30),  # Task-specific SLA
    )
```

**–ï—Å–ª–∏ task –ø—Ä–µ–≤—ã—à–∞–µ—Ç SLA:**
```
Alert –≤ Slack:
‚ö†Ô∏è SLA MISS!
DAG: ml_training_pipeline
Task: train_classifier
Expected: 30 minutes
Actual: 35 minutes
Status: Still running
Action: Investigate why training is slow
```

–í Celery —Ç–∞–∫–æ–≥–æ –Ω–µ—Ç!

---

### 4. Backfilling Historical Data

**–°—Ü–µ–Ω–∞—Ä–∏–π:** –ù—É–∂–Ω–æ –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü.

**Celery:**
```python
# –ü—Ä–∏–¥–µ—Ç—Å—è –ø–∏—Å–∞—Ç—å —Å–∫—Ä–∏–ø—Ç —Ä—É–∫–∞–º–∏
for date in date_range(start='2024-10-01', end='2024-11-01'):
    retrain_all_models.apply_async(
        kwargs={'execution_date': date}
    )
# –°–ª–æ–∂–Ω–æ –æ—Ç—Å–ª–µ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
```

**Airflow:**
```bash
# One command!
airflow dags backfill \
    ml_training_pipeline \
    --start-date 2024-10-01 \
    --end-date 2024-11-01 \
    --reset-dagruns

# Airflow —Å–∞–º:
# - –°–æ–∑–¥–∞—Å—Ç 31 DAG run (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ –¥–µ–Ω—å)
# - –ó–∞–ø—É—Å—Ç–∏—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∏–ª–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (depends_on_past)
# - –ü–æ–∫–∞–∂–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ UI
# - –°–æ—Ö—Ä–∞–Ω–∏—Ç –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
```

---

### 5. Data Lineage Tracking

```python
# Airflow –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –æ—Ç–∫—É–¥–∞ –¥–∞–Ω–Ω—ã–µ
from airflow.lineage import AUTO

with DAG('data_pipeline') as dag:
    
    extract = PythonOperator(
        task_id='extract',
        python_callable=extract_data,
        outlets=[Dataset('s3://raw-data/metadata.json')],  # Output
    )
    
    transform = PythonOperator(
        task_id='transform',
        python_callable=transform_data,
        inlets=[Dataset('s3://raw-data/metadata.json')],  # Input
        outlets=[Dataset('postgres://metadata')],  # Output
    )
```

**–í UI –≤–∏–¥–Ω–æ:**
```
Data Lineage:
  s3://raw-data/metadata.json ‚Üí [extract] ‚Üí [transform] ‚Üí postgres://metadata
  
  Last updated: 2024-11-05 02:00
  Consumers: [load_to_neo4j, vectorize_to_qdrant]
```

---

## üîß –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä: Full DAG –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞

### –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π Data + ML Pipeline

```python
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.utils.task_group import TaskGroup
from datetime import datetime, timedelta

default_args = {
    'owner': '1c-ai-team',
    'depends_on_past': False,
    'email': ['alerts@1c-ai.dev'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=3),
}

with DAG(
    'complete_ml_data_pipeline',
    default_args=default_args,
    description='Complete data sync and ML training pipeline for 1C AI Stack',
    schedule_interval='0 2 * * *',  # Daily at 2 AM
    start_date=datetime(2024, 1, 1),
    catchup=False,
    max_active_runs=1,
    tags=['production', 'ml', 'data-sync'],
    sla_miss_callback=send_sla_alert,
) as dag:

    # =====================================
    # PHASE 1: DATA EXTRACTION & PREPARATION
    # =====================================
    
    with TaskGroup('data_extraction') as extraction_group:
        
        extract_1c = PythonOperator(
            task_id='extract_1c_configs',
            python_callable=extract_from_1c_configurations,
            pool='data_extraction',
            sla=timedelta(minutes=30),
        )
        
        extract_github = PythonOperator(
            task_id='extract_github_repos',
            python_callable=fetch_from_github_api,
            pool='data_extraction',
            sla=timedelta(minutes=45),
        )
        
        extract_its = PythonOperator(
            task_id='extract_its_docs',
            python_callable=scrape_its_documentation,
            pool='data_extraction',
            sla=timedelta(minutes=60),
        )
        
        # Parallel extraction
        [extract_1c, extract_github, extract_its]
    
    # =====================================
    # PHASE 2: DATA TRANSFORMATION
    # =====================================
    
    merge_data = PythonOperator(
        task_id='merge_all_sources',
        python_callable=merge_and_deduplicate,
        trigger_rule='none_failed',  # –î–∞–∂–µ –µ—Å–ª–∏ GitHub –Ω–µ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ
    )
    
    clean_data = PythonOperator(
        task_id='clean_and_validate',
        python_callable=clean_and_validate_data,
    )
    
    # Check data quality before proceeding
    def check_data_quality_func(**context):
        ti = context['ti']
        data = ti.xcom_pull(task_ids='clean_and_validate')
        
        if data['quality_score'] > 0.8:
            return 'proceed_to_sync'
        else:
            return 'send_quality_alert'
    
    quality_check = BranchPythonOperator(
        task_id='check_data_quality',
        python_callable=check_data_quality_func,
    )
    
    send_alert = PythonOperator(
        task_id='send_quality_alert',
        python_callable=send_low_quality_alert,
    )
    
    proceed = PythonOperator(
        task_id='proceed_to_sync',
        python_callable=lambda: print("Quality OK, proceeding"),
    )
    
    # =====================================
    # PHASE 3: DATABASE SYNCHRONIZATION (PARALLEL)
    # =====================================
    
    with TaskGroup('database_sync') as sync_group:
        
        sync_postgres = PostgresOperator(
            task_id='sync_to_postgres',
            postgres_conn_id='postgres_1c_ai',
            sql='sql/sync_metadata.sql',
            pool='database_writes',
        )
        
        sync_neo4j = PythonOperator(
            task_id='sync_to_neo4j',
            python_callable=sync_graph_database,
            pool='database_writes',
        )
        
        vectorize_qdrant = PythonOperator(
            task_id='vectorize_and_sync_qdrant',
            python_callable=vectorize_and_load_qdrant,
            pool='ml_inference',  # –¢—Ä–µ–±—É–µ—Ç GPU
            execution_timeout=timedelta(hours=1),
        )
        
        index_elasticsearch = PythonOperator(
            task_id='index_to_elasticsearch',
            python_callable=bulk_index_to_es,
            pool='database_writes',
        )
        
        # All in parallel
        [sync_postgres, sync_neo4j, vectorize_qdrant, index_elasticsearch]
    
    # =====================================
    # PHASE 4: DATA VALIDATION
    # =====================================
    
    validate_sync = PythonOperator(
        task_id='validate_data_integrity',
        python_callable=validate_cross_db_integrity,
        trigger_rule='none_failed',
    )
    
    # =====================================
    # PHASE 5: ML MODEL TRAINING (PARALLEL)
    # =====================================
    
    with TaskGroup('ml_training') as training_group:
        
        # Model 1: Classifier
        train_classifier = PythonOperator(
            task_id='train_classifier_model',
            python_callable=train_classifier_with_new_data,
            pool='ml_training',
            pool_slots=2,  # –ó–∞–Ω–∏–º–∞–µ—Ç 2 —Å–ª–æ—Ç–∞
            execution_timeout=timedelta(minutes=30),
        )
        
        # Model 2: Regressor
        train_regressor = PythonOperator(
            task_id='train_regressor_model',
            python_callable=train_regressor_with_new_data,
            pool='ml_training',
            pool_slots=2,
            execution_timeout=timedelta(minutes=40),
        )
        
        # Model 3: Embeddings
        fine_tune_embeddings = PythonOperator(
            task_id='finetune_embeddings_model',
            python_callable=finetune_qwen_embeddings,
            pool='ml_training',
            pool_slots=3,  # –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤
            execution_timeout=timedelta(hours=2),
        )
        
        [train_classifier, train_regressor, fine_tune_embeddings]
    
    # =====================================
    # PHASE 6: MODEL EVALUATION & DEPLOYMENT
    # =====================================
    
    evaluate_models = PythonOperator(
        task_id='evaluate_all_models',
        python_callable=comprehensive_model_evaluation,
        trigger_rule='none_failed',
    )
    
    def decide_deployment(**context):
        """–†–µ—à–∏—Ç—å —Å—Ç–æ–∏—Ç –ª–∏ –¥–µ–ø–ª–æ–∏—Ç—å –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏"""
        ti = context['ti']
        eval_results = ti.xcom_pull(task_ids='evaluate_all_models')
        
        # –ï—Å–ª–∏ accuracy —É–ª—É—á—à–∏–ª–∞—Å—å
        if eval_results['classifier']['accuracy'] > 0.90:
            return 'deploy_to_production'
        else:
            return 'keep_current_models'
    
    deployment_decision = BranchPythonOperator(
        task_id='decide_deployment',
        python_callable=decide_deployment,
    )
    
    deploy_production = PythonOperator(
        task_id='deploy_to_production',
        python_callable=deploy_models_to_prod,
    )
    
    keep_current = PythonOperator(
        task_id='keep_current_models',
        python_callable=lambda: print("Models not good enough, keeping current"),
    )
    
    # Update model registry (always)
    update_registry = PostgresOperator(
        task_id='update_model_registry',
        postgres_conn_id='postgres_1c_ai',
        sql='sql/update_model_registry.sql',
        trigger_rule='none_failed',
    )
    
    # =====================================
    # PHASE 7: CLEANUP & NOTIFICATION
    # =====================================
    
    cleanup = PythonOperator(
        task_id='cleanup_temp_files',
        python_callable=cleanup_training_artifacts,
        trigger_rule='all_done',  # –í—Å–µ–≥–¥–∞, –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
    )
    
    notify_team = PythonOperator(
        task_id='send_summary_to_slack',
        python_callable=send_pipeline_summary,
        trigger_rule='all_done',
    )

    # =====================================
    # DAG DEPENDENCIES
    # =====================================
    
    extraction_group >> merge_data >> clean_data >> quality_check
    quality_check >> [proceed, send_alert]
    
    proceed >> sync_group >> validate_sync >> training_group
    
    training_group >> evaluate_models >> deployment_decision
    deployment_decision >> [deploy_production, keep_current]
    
    [deploy_production, keep_current] >> update_registry >> cleanup >> notify_team
```

**–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ Airflow UI:**

```
[Extraction Group: parallel extract_1c, extract_github, extract_its]
    ‚Üì
merge_data
    ‚Üì
clean_data
    ‚Üì
check_data_quality ‚Üí [proceed | send_alert]
    ‚Üì
[Sync Group: parallel sync_postgres, sync_neo4j, vectorize_qdrant, index_es]
    ‚Üì
validate_sync
    ‚Üì
[Training Group: parallel train_classifier, train_regressor, fine_tune_embeddings]
    ‚Üì
evaluate_models
    ‚Üì
decide_deployment ‚Üí [deploy_to_production | keep_current_models]
    ‚Üì
update_registry
    ‚Üì
cleanup
    ‚Üì
notify_team
```

**–≠—Ç–æ –ù–ï–í–û–ó–ú–û–ñ–ù–û –≤ Celery –±–µ–∑ –Ω–∞–ø–∏—Å–∞–Ω–∏—è custom orchestrator!**

---

## üí∞ ROI Calculation

### Costs

**One-time:**
- –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞: 60 —á–∞—Å–æ–≤ √ó $50/—á–∞—Å = $3,000
- –û–±—É—á–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã: 20 —á–∞—Å–æ–≤ √ó $50/—á–∞—Å = $1,000
- Infrastructure setup: $500
**Total one-time: $4,500**

**Recurring:**
- Infrastructure: +850MB RAM ‚âà +$5/–º–µ—Å—è—Ü (cloud)
- Maintenance: 2 —á–∞—Å–∞/–º–µ—Å—è—Ü √ó $50/—á–∞—Å = $100/–º–µ—Å—è—Ü
**Total monthly: $105**

**Annual: $1,260** (–ø–µ—Ä–≤—ã–π –≥–æ–¥: $5,760)

---

### Benefits

**Time savings:**
- ML Pipeline: 45 min ‚Üí 20 min = 25 min saved daily
  - 25 min √ó 365 days = 152 hours/year
  - 152 hours √ó $50/—á–∞—Å = $7,600/year

- Data Sync: –ù–æ–≤—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª, —Ä–∞–Ω—å—à–µ –Ω–µ –±—ã–ª–æ
  - –≠–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤: ~10 hours/week
  - 10 hours √ó 52 weeks √ó $50/—á–∞—Å = $26,000/year

- Debugging/Troubleshooting: -30% –≤—Ä–µ–º–µ–Ω–∏
  - –°–µ–π—á–∞—Å: ~5 hours/week
  - –≠–∫–æ–Ω–æ–º–∏—è: 1.5 hours/week √ó 52 √ó $50 = $3,900/year

**Total annual savings: $37,500**

**ROI:** 
- Year 1: $37,500 - $5,760 = **$31,740 profit** (550% ROI!)
- Year 2+: $37,500 - $1,260 = **$36,240 profit** (2900% ROI!)

---

## ‚úÖ –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏

| –§—É–Ω–∫—Ü–∏—è | Celery | Airflow | –í–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ |
|---------|--------|---------|----------------------|
| Task scheduling | ‚úÖ Beat | ‚úÖ Scheduler | üî¥ Critical |
| Async execution | ‚úÖ | ‚úÖ | üî¥ Critical |
| Retry logic | ‚úÖ Basic | ‚úÖ Advanced | üü° High |
| Dependencies | ‚ùå Manual | ‚úÖ DAG | üî¥ Critical |
| Parallel execution | ‚ùå –°–ª–æ–∂–Ω–æ | ‚úÖ Easy | üî¥ Critical |
| UI visualization | ‚ö†Ô∏è Flower (basic) | ‚úÖ Rich UI | üü° High |
| Logging | ‚ö†Ô∏è Scattered | ‚úÖ Centralized | üü° High |
| Monitoring | ‚ö†Ô∏è External | ‚úÖ Built-in | üü° High |
| SLA tracking | ‚ùå | ‚úÖ | üü¢ Medium |
| Backfilling | ‚ùå | ‚úÖ | üü¢ Medium |
| Data lineage | ‚ùå | ‚úÖ | üü¢ Medium |
| Conditional logic | ‚ùå | ‚úÖ | üî¥ Critical |
| Resource pools | ‚ùå | ‚úÖ | üü° High |
| XCom (data passing) | ‚ùå | ‚úÖ | üü° High |
| Testing utilities | ‚ö†Ô∏è Basic | ‚úÖ Advanced | üü¢ Medium |

**–û—Ü–µ–Ω–∫–∞ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏:**
- üî¥ Critical (5): **Airflow wins 4/5**
- üü° High (6): **Airflow wins 6/6**
- üü¢ Medium (4): **Airflow wins 4/4**

**–í–µ—Ä–¥–∏–∫—Ç:** Airflow –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Celery –¥–ª—è workflow orchestration

---

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –í–ù–ï–î–†–Ø–¢–¨ AIRFLOW

**–ü–æ—á–µ–º—É:**
1. ‚úÖ **–≠–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏** - –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —Å–æ–∫—Ä–∞—â–∞–µ—Ç ML pipeline –Ω–∞ 55%
2. ‚úÖ **–í–∏–¥–∏–º–æ—Å—Ç—å** - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö workflow
3. ‚úÖ **–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å** - –ª—É—á—à–∏–π retry, dependencies, validation
4. ‚úÖ **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å** - –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ pipeline'—ã
5. ‚úÖ **ROI** - –æ–∫—É–ø–∞–µ—Ç—Å—è –∑–∞ 2 –º–µ—Å—è—Ü–∞

**–ö–∞–∫ –≤–Ω–µ–¥—Ä—è—Ç—å:**
1. **Phase 1 (2 –Ω–µ–¥–µ–ª–∏):** Setup Airflow, –±–∞–∑–æ–≤—ã–π DAG
2. **Phase 2 (3 –Ω–µ–¥–µ–ª–∏):** –ú–∏–≥—Ä–∞—Ü–∏—è ML pipeline —Å Celery
3. **Phase 3 (2 –Ω–µ–¥–µ–ª–∏):** –ù–æ–≤—ã–µ DAG'–∏ (Data Sync, Maintenance)
4. **Phase 4 (ongoing):** –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ –Ω–æ–≤—ã–µ pipeline'—ã

**Timeline:** 2 –º–µ—Å—è—Ü–∞ –¥–æ production

**Effort:** ~80 —á–∞—Å–æ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏

**Payback period:** 2 –º–µ—Å—è—Ü–∞

---

### –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã (–µ—Å–ª–∏ –Ω–µ —Ö–æ—Ç–∏–º Airflow)

1. **Prefect** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Airflow
   - –ü—Ä–æ—â–µ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
   - –õ—É—á—à–µ –¥–ª—è dynamic workflows
   - –ú–µ–Ω—å—à–µ overhead
   - –ù–æ –º–µ–Ω–µ–µ mature

2. **Temporal** - –¥–ª—è distributed workflows
   - –û—á–µ–Ω—å –º–æ—â–Ω—ã–π
   - –•–æ—Ä–æ—à –¥–ª—è microservices
   - –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π

3. **Dagster** - data orchestration
   - –û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ data assets
   - –•–æ—Ä–æ—à –¥–ª—è ML/Data teams
   - –ù–æ–≤—ã–π, –Ω–æ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–π

4. **–û—Å—Ç–∞–≤–∏—Ç—å Celery + —É–ª—É—á—à–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**
   - –î–æ–±–∞–≤–∏—Ç—å custom dashboard
   - –£–ª—É—á—à–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
   - –î–æ–±–∞–≤–∏—Ç—å dependency tracking
   - –ù–æ —ç—Ç–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø–∏—Å–∞—Ç—å "—Å–≤–æ–π Airflow"

**–í—ã–≤–æ–¥:** Apache Airflow - –ª—É—á—à–∏–π –≤—ã–±–æ—Ä –ø–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é –∑—Ä–µ–ª–æ—Å—Ç—å/—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å/community

---

## üìù –§–∏–Ω–∞–ª—å–Ω—ã–µ –≤—ã–≤–æ–¥—ã

### –ß—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å —Å–µ–π—á–∞—Å:

```
‚úÖ Celery - —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ limited
‚úÖ Workflow Orchestrator (TS) - hardcoded, –Ω–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è
‚úÖ AI Orchestrator - —Ö–æ—Ä–æ—à –¥–ª—è real-time routing (–æ—Å—Ç–∞–≤–∏—Ç—å)
‚úÖ Background Jobs - –ø—Ä–∏–º–∏—Ç–∏–≤–Ω–∞—è –æ—á–µ—Ä–µ–¥—å
```

### –ß—Ç–æ –±—É–¥–µ—Ç —Å Airflow:

```
‚úÖ Apache Airflow - –µ–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è batch workflows
‚úÖ AI Orchestrator - –¥–ª—è real-time (–æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å!)
‚úÖ Celery Workers - –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ Airflow (CeleryExecutor)
‚ùå Workflow Orchestrator (TS) - –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ Airflow DAG
‚ùå Background Jobs - –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ Airflow DAG
```

### Hybrid Architecture (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Apache Airflow (Batch Workflows)    ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  - ML Training Pipeline                 ‚îÇ
‚îÇ  - Data Sync Pipeline                   ‚îÇ
‚îÇ  - Maintenance Pipeline                 ‚îÇ
‚îÇ  - BSL Dataset Preparation              ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  Uses: CeleryExecutor                   ‚îÇ
‚îÇ     ‚Üì                                   ‚îÇ
‚îÇ  Celery Workers (for heavy ML tasks)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   AI Orchestrator (Real-time Routing)   ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  - User queries (<2s latency)           ‚îÇ
‚îÇ  - Code generation                      ‚îÇ
‚îÇ  - Semantic search                      ‚îÇ
‚îÇ  - Telegram bot responses               ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  Direct calls to AI services            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ö–∞–∂–¥—ã–π –¥–ª—è —Å–≤–æ–µ–π –∑–∞–¥–∞—á–∏!**

---

**–°—Ç–∞—Ç—É—Å:** –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç  
**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** ‚úÖ –í–Ω–µ–¥—Ä—è—Ç—å Apache Airflow –¥–ª—è batch workflows  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üü° High (–Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, –Ω–æ –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω–æ)  
**Timeline:** 2 –º–µ—Å—è—Ü–∞  
**ROI:** 550% –≤ –ø–µ—Ä–≤—ã–π –≥–æ–¥

