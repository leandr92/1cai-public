# План устойчивости агента при блокировке внешних LLM

## 1. Контекст и риски
- **Регуляторная угроза**: с 1 марта 2026 года Роскомнадзор сможет отключать российский сегмент от глобального интернета, вводить «белые списки» и выборочные шатдауны. Это напрямую затрагивает доступ к зарубежным LLM и API [источник](https://www.svoboda.org/a/ruchnoy-internet/33586177.html).
- **Текущая зависимость**: ключевые роли (`Architect`, `DevOps`, `Technical Writer`) используют `openai-gpt4` как основной или fallback-агент (`src/ai/role_based_router.py`), сервисы (`src/services/openai_code_analyzer.py`, `external/sgr-agent-core`) тянут OpenAI SDK. При недоступности облака происходит деградация большей части «умных» функций.
- **Сценарии отказа**:
  1. Полный разрыв соединения с внешними API (интернет шатдаун, «белый список» без OpenAI).
  2. Ограничение полосы/latency → истечение тайм-аутов.
  3. Блокировка отдельных доменов или TLS-инспекция, приводящая к ошибкам сертификатов.
  4. Требование использовать только «разрешённые» отечественные сервисы.

## 2. Целевая стратегия
1. **Мультипровайдерная архитектура**: расширить пул моделей (Gigachat, Yandex GPT, Qwen Coder, 1C Напарник) и обеспечить независимый роутинг на уровне `role_based_router`.
2. **Локальный inference-стек**: развёрнуть один или несколько LLM on-prem и подготовить автоматическое переключение.
3. **Офлайн-ядро знаний**: сформировать кэш ответов, векторные базы и runbook, чтобы типовые задачи решались без вызова LLM.
4. **Управление трафиком**: использовать туннели/частные каналы там, где это законно, но исходить из сценария полного автономного режима.
5. **Операционная готовность**: внедрить планы реагирования, тесты и мониторинг, чтобы команда заранее знала порядок действий.

## 3. Поставщики и инструменты

| Группа | Инструменты/сервисы | Назначение |
|--------|---------------------|------------|
| Локальные LLM | **vLLM**, **Text Generation Inference**, **Ollama** (для прототипов), **LM Studio** (ручная отладка) | Деплой Qwen 2.5 Coder 7B/14B, Mistral 7B Instruct, Llama 3 8B, Mixtral 8x7B (квантованные GGUF для CPU) |
| Аппаратная платформа | NVIDIA RTX 6000/8000, A100/H100; AMD MI300 (через ROCm); кластер на 4×RTX4090 | Хостинг inference и fine-tune (пока только в планах, закупка требует отдельного решения) |
| MLOps | **Kubernetes + KServe**, **Seldon Core**, **Argo CD**, **MLflow** | Оркестрация деплоев, управление версиями моделей |
| Векторные базы | **Qdrant** (уже в `docker-compose.dev.yml`), **Milvus**, **Weaviate** | Кэширование знаний, поиск похожего контента |
| Embedding-модели | `qwen3-embedding-4b-instruct`, `intfloat/multilingual-e5-large`, `sbert.net/LaBSE` | Генерация векторов для офлайн-поиска |
| Локальные диалоги | **GigaChat API**, **YandexGPT**, **Mistral API (self-host)** | Домашние fallback-провайдеры с белыми списками |
| Секреты | HashiCorp Vault (`infrastructure/vault`), SOPS, Kubernetes Secrets | Безопасное хранение API-ключей и переключение профилей |
| Мониторинг | Prometheus/Grafana (`docker-compose.saas.yml`), Loki, Tempo, Sentry | Наблюдение за задержками, ошибками и переключениями |
| Сеть | WireGuard, Tailscale, Zerotier, Cloudflare Tunnel | Резервные каналы, легальные обходы блокировок |

## 4. Архитектурные изменения
1. **Роутер агентов**  
   - В `src/ai/role_based_router.py` и `config/architecture.yaml` добавить поддержку новых бэкендов (`local-qwen`, `local-mistral`, `gigachat`, `yandex-gpt`).  
   - Ввод `ProviderHealthMonitor`: компонент, фиксирующий статус и время отклика каждого поставщика, публикующий метрики в Prometheus (`ai_provider_status{provider="openai"}`).
   - Конфигурация YAML (пример):  
     ```yaml
     providers:
       local-qwen:
         endpoint: http://llm-gateway:8080/v1
         driver: vllm
         models: ["qwen2.5-coder-7b-instruct", "qwen2.5-coder-32b-instruct"]
         auth: internal-jwt
         capabilities: ["code", "russian"]
     ```

2. **LLM Gateway**  
   - Отдельный сервис (`src/services/llm_gateway.py`), принимающий OpenAI-совместимые запросы и маршрутизирующий их к локальным backends (vLLM, Text Generation Inference, Gigachat REST).  
   - Рекомендация: использовать reverse-proxy (NGINX/Traefik) с circuit breaker, лимитами и ретраями, чтобы снизить нагрузку при деградации.

3. **Локальный inference-кластер**  
   - Минимальная топология:  
     ```
     [Users] -> [API Gateway] -> [LLM Gateway] -> (OpenAI | Gigachat | Yandex | vLLM pods)
                                             -> [Vector Store Qdrant]
     ```
   - Kubernetes-манифесты: Helm-чарт в `infrastructure/helm/llm-gateway`, StatefulSet для vLLM, горизонтальное масштабирование по Prometheus-метрикам.

4. **Knowledge Base Offload**  
   - Pipelines в `scripts/` для парсинга `the-book-of-secret-knowledge` и внутренних docs в векторную базу; cron job обновления.  
   - REST-эндпоинт `GET /knowledge/search` (FastAPI) → агент возвращает конспекты без обращения к LLM.

## 5. Операционная процедура переключения
1. **Обнаружение**  
   - Тайм-аут запросов `openai` → событие в Prometheus Alertmanager.  
   - Авто-обновление статуса провайдера в Redis `ai_provider_status`.
2. **Автопереключение**  
   - API-gateway получает сигнал и обновляет `primary_agent` → `local-qwen`.  
   - Все новые сессии используют локальный backend, активные сессии получают уведомление.
3. **Ручное вмешательство**  
   - DevOps запускает Ansible-плейбук (`infrastructure/ansible/inventory`) для масштабирования кластера и включения дополнительных моделей.  
   - Проверка логов (`monitoring/`) и валидация качества ответов (smoke-тесты).
4. **Восстановление**  
   - После стабилизации внешнего доступа выполняется `gradual roll-back`: часть трафика возвращается на OpenAI, проводится A/B-сравнение.

## 6. План развёртывания (12 недель)

| Этап | Срок | Результаты | Ответственные |
|------|------|------------|---------------|
| **Подготовка** | Недели 1–2 | Аудит кода и конфигураций, выбор моделей, оценка потребностей по железу (закупка отложена до отдельного решения) | Архитектор, ML-инженер |
| **Инфраструктурный пилот** | Недели 3–5 | Деплой vLLM (Qwen 7B), настройка LLM Gateway, интеграция с FastAPI; Prometheus/Grafana дашборды | DevOps, ML-инженер |
| **Знания и кэш** | Недели 4–6 | Создание векторной базы (Qdrant), ETL из `docs/`, `the-book-of-secret-knowledge`, запуск cron обновлений | Data Engineer |
| **Расширение моделей** | Недели 6–8 | Добавление Gigachat/Yandex/GigaChat, fine-tune Mistral для бизнес-русифицированных сценариев | ML-команда |
| **Операционный контур** | Недели 8–10 | Alerting, Ansible-плейбуки, инструкции runbook, подготовка учений «интернет оффлайн» | DevOps, SRE |
| **Учения и оптимизация** | Недели 10–12 | Проведение двух сценариев (полный шатдаун, деградация), корректировка параметров, обновление документации | SRE, продуктовая команда |

## 7. Runbook «Интернет недоступен»
1. **T0**: От Alertmanager поступает уведомление «OpenAI provider down».  
2. **T0 + 5 мин**: Проверить статус туннелей/прокси; удостовериться, что блокировка общая, а не локальная.  
3. **T0 + 10 мин**: Выполнить команду `make switch-llm BACKEND=local-qwen` (готовый скрипт).  
4. **T0 + 15 мин**: Запустить smoke-тесты (`scripts/tests/llm_smoke.py --backend local-qwen`).  
5. **T0 + 30 мин**: Сообщить пользователям о режиме ограниченного функционала, активировать офлайн-runbook.  
6. **T0 + 60 мин**: Провести выборочный аудит ответов, зафиксировать качество (метрики BLEU/ROUGE/точность).  
7. **По факту восстановления**: `make switch-llm BACKEND=openai` → A/B сравнение, отчёт в Confluence/Notion.

## 8. Дополнительные меры
- **Юридическое сопровождение**: зарегистрировать собственные API-узлы в реестрах, подготовить документы о соответствии требованиям к критической информационной инфраструктуре (КИИ).  
- **Безопасность**: все локальные модели запускаются в отдельном сегменте сети, доступ через WireGuard + MFA; логи запросов без персональных данных (`PII Tokenizer`).  
- **Оптимизация затрат**: использовать квантованные модели (AWQ/GPTQ), динамически выключать GPU ночью, вести отчёт по потреблению энергии.  
- **Обновление знаний**: еженедельный cron `scripts/knowledge/update_secret_book.py` — обновление курируемого контента из `the-book-of-secret-knowledge` и внутренних репозиториев.  
- **Резервные каналы**: оценить возможность аренды выделенных каналов связи или использования CDN-провайдеров с разрешением от Роскомнадзора.

## 9. Требуемые артефакты
- Helm-чарт `infrastructure/helm/llm-gateway` + K8s-манифесты vLLM.  
- Конфигурация `config/llm_providers.yaml` с приоритетами и health-check.  
- Runbook в `docs/06-features/DEVOPS_AGENT_OFFLINE_MODE.md`.  
- Авто-тесты на переключение (`tests/integration/test_llm_failover.py`).  
- ETL-скрипт `scripts/knowledge/build_vector_store.py`.  
- Grafana dashboard `monitoring/grafana/dashboards/llm-resilience.json`.  
- Отчёт по пилоту и метрикам качества (BLEU, Rouge-L, pass@k).

---

## 10. Статус внедрения (обновлено)
- ✅ Подготовлен конфиг провайдеров и fallback-матрица (`config/llm_providers.yaml`), скрипт переключения (`scripts/llm/switch_backend.py`), диагностические утилиты и runbook.  
- ✅ `RoleBasedRouter`, базовые ассистенты и `LLMGateway` читают конфигурацию через `LLMProviderManager`, поддерживают динамическое переключение и возвращают метаданные о провайдере и fallback-цепочке; добавлен режим моделирования сценариев (`config/llm_gateway_simulation.yaml`).  
- ✅ Созданы вспомогательные документы: план офлайн-режима, шаблон отчёта, директория для упражнений, конвейер сборки базы знаний.  
- ✅ Подготовлены инструменты моделирования: `scripts/diagnostics/mock_healthcheck.py`, `scripts/tests/run_offline_dry_run.py`, `scripts/knowledge/mock_embedding_builder.py`, интеграционные тесты (`tests/integration/test_llm_gateway_simulation.py`).  
- ⏳ Требуется реализовать реальные вызовы в LLM Gateway, health-check-и и smoke-тесты, обновить ETL (эмбеддинги + Qdrant), автоматизировать сравнение ответов и cron обновления знаний.  
- ⏳ Локальные self-hosted модели остаются в статусе `planned`, закупка железа не выполняется до отдельного решения.

---

**Статус**: документ создан для внутреннего использования, никаких внешних публикаций или пушей не выполнялось.

