# üß† BSL Fine-Tuning Guide

**–û–±—É—á–µ–Ω–∏–µ AI –º–æ–¥–µ–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è 1–°:–ü—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ (BSL)**

> üöß **–°—Ç–∞—Ç—É—Å:** Dataset –≥–æ—Ç–æ–≤, fine-tuning –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ

---

## üéØ –¶–µ–ª—å

–°–æ–∑–¥–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é AI –º–æ–¥–µ–ª—å –¥–ª—è:
- –ì–µ–Ω–µ—Ä–∞—Ü–∏–∏ BSL –∫–æ–¥–∞ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
- –ü–æ–Ω–∏–º–∞–Ω–∏—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ 1–° (–¥–æ–∫—É–º–µ–Ω—Ç—ã, —Ä–µ–≥–∏—Å—Ç—Ä—ã, –∑–∞–ø—Ä–æ—Å—ã)
- –°–ª–µ–¥–æ–≤–∞–Ω–∏—è best practices BSL
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –∫–æ–¥–∞

**–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å:** Qwen3-Coder (32B)  
**Fine-tuned –º–æ–¥–µ–ª—å:** Qwen3-BSL (—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)

---

## üìä Dataset

### –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:

1. **PostgreSQL** (knowledge_base)
   - 50,000+ —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
   - DO, ERP, ZUP, BUH
   - –° –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π –∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏

2. **GitHub** (–ø—É–±–ª–∏—á–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã)
   - 1C-Company/ssl_* –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
   - oscript-library
   - Community –ø—Ä–æ–µ–∫—Ç—ã

3. **–ü–∞—Ç—Ç–µ—Ä–Ω—ã** (–≤—Ä—É—á–Ω—É—é)
   - CRUD –æ–ø–µ—Ä–∞—Ü–∏–∏
   - –†–∞–±–æ—Ç–∞ —Å —Ñ–æ—Ä–º–∞–º–∏
   - HTTP –∑–∞–ø—Ä–æ—Å—ã
   - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
   - Refactoring –ø—Ä–∏–º–µ—Ä—ã

### –§–æ—Ä–º–∞—Ç dataset:

**Alpaca format** (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):
```json
{
  "instruction": "–°–æ–∑–¥–∞–π —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ù–î–°",
  "input": "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: –°—É–º–º–∞, –°—Ç–∞–≤–∫–∞",
  "output": "–§—É–Ω–∫—Ü–∏—è –†–∞—Å—Å—á–∏—Ç–∞—Ç—å–ù–î–°(...)\n...",
  "system": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ BSL..."
}
```

**OpenAI format:**
```json
{
  "messages": [
    {"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ BSL"},
    {"role": "user", "content": "–°–æ–∑–¥–∞–π —Ñ—É–Ω–∫—Ü–∏—é..."},
    {"role": "assistant", "content": "–§—É–Ω–∫—Ü–∏—è ..."}
  ]
}
```

### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:

- Train: 80% –ø—Ä–∏–º–µ—Ä–æ–≤ (~400 –∏–∑ 500)
- Validation: 10% (~50)
- Test: 10% (~50)

**–ö–∞—Ç–µ–≥–æ—Ä–∏–∏:**
- CRUD: 30%
- Forms: 20%
- HTTP/Integration: 15%
- Optimization: 15%
- Refactoring: 10%
- Other: 10%

---

## üõ†Ô∏è –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ Dataset

### –ó–∞–ø—É—Å–∫ builder:

```bash
# –°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–π dataset
python src/ai/copilot/dataset_builder.py

# Output:
# datasets/bsl/
#   ‚îú‚îÄ‚îÄ bsl_alpaca_train.jsonl
#   ‚îú‚îÄ‚îÄ bsl_openai_train.jsonl
#   ‚îú‚îÄ‚îÄ bsl_hf_train.jsonl
#   ‚îú‚îÄ‚îÄ bsl_train.jsonl (train split)
#   ‚îú‚îÄ‚îÄ bsl_val.jsonl (validation)
#   ‚îú‚îÄ‚îÄ bsl_test.jsonl (test)
#   ‚îî‚îÄ‚îÄ dataset_stats.json
```

### –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ dataset –∏–∑ PostgreSQL:

```python
from src.ai.copilot.dataset_builder import BSLDatasetBuilder
from src.database import get_db_connection

async def expand_dataset():
    builder = BSLDatasetBuilder()
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    db = await get_db_connection()
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤
    await builder.build_from_postgres(db)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    builder.save_for_finetuning("alpaca")
    
    print(f"Dataset —Ä–∞—Å—à–∏—Ä–µ–Ω –¥–æ {len(builder.examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
```

### Scraping –∏–∑ GitHub:

```python
builder = BSLDatasetBuilder()

# –°–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö BSL —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤
repos = [
    "1C-Company/ssl_1c_bsl",
    "oscript-library/opm",
    "oscript-library/logos",
    "vanessa-opensource/add"
]

await builder.build_from_github(repos)
```

---

## üöÄ Fine-Tuning Process

### 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install torch transformers datasets accelerate bitsandbytes

# –î–ª—è GPU (NVIDIA)
# CUDA 11.8+
```

### 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è fine-tuning

```python
# training_config.yaml

model:
  base_model: "Qwen/Qwen2.5-Coder-32B-Instruct"
  output_dir: "models/qwen3-bsl"
  
training:
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 8
  learning_rate: 2e-5
  warmup_steps: 100
  
  # LoRA settings (–¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)
  use_lora: true
  lora_rank: 64
  lora_alpha: 128
  lora_dropout: 0.1
  
optimization:
  optimizer: "adamw_torch"
  scheduler: "cosine"
  weight_decay: 0.01
  max_grad_norm: 1.0
  
hardware:
  device: "cuda"  # cuda | cpu | mps
  mixed_precision: "fp16"  # fp16 | bf16 | no
  gradient_checkpointing: true
```

### 3. –ó–∞–ø—É—Å–∫ fine-tuning

```bash
# –° LoRA (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Qwen3-32B)
python src/ai/copilot/lora_fine_tuning.py \
    --dataset datasets/bsl/bsl_alpaca_train.jsonl \
    --base_model Qwen/Qwen2.5-Coder-32B-Instruct \
    --output_dir models/qwen3-bsl-lora \
    --epochs 3 \
    --batch_size 4

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —á–µ—Ä–µ–∑ TensorBoard
tensorboard --logdir models/qwen3-bsl-lora/logs
```

---

## üìà –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–µ—Å—É—Ä—Å–∞–º

### Hardware:

**Minimum (—Å LoRA):**
- GPU: NVIDIA RTX 3090 (24GB VRAM)
- RAM: 32GB
- Disk: 200GB SSD
- Time: ~6-12 —á–∞—Å–æ–≤

**Recommended:**
- GPU: NVIDIA A100 (80GB VRAM)
- RAM: 64GB
- Disk: 500GB NVMe
- Time: ~2-4 —á–∞—Å–∞

**Alternative (–±–µ–∑ GPU):**
- CPU only: –≤–æ–∑–º–æ–∂–Ω–æ –Ω–æ –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ (48+ —á–∞—Å–æ–≤)
- Cloud: Google Colab Pro+ (~$50/–º–µ—Å—è—Ü)
- RunPod: GPU rent (~$0.5/—á–∞—Å)

### Cloud options:

1. **RunPod** (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
   - RTX 4090: $0.34/hour
   - A100 80GB: $1.89/hour
   - Pre-configured PyTorch templates

2. **Google Colab Pro+**
   - A100 access: $50/–º–µ—Å—è—Ü
   - 500 compute units
   - Easy setup

3. **AWS SageMaker**
   - ml.g5.12xlarge: ~$7/hour
   - Managed training
   - Automatic scaling

---

## üéì Fine-Tuning –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

### LoRA (Low-Rank Adaptation):

**–ó–∞—á–µ–º:**
- –û–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ 1-2% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
- –¢—Ä–µ–±—É–µ—Ç—Å—è –≤ 10 —Ä–∞–∑ –º–µ–Ω—å—à–µ VRAM
- –ë—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ
- –ú–µ–Ω—å—à–µ —Ä–∏—Å–∫ catastrophic forgetting

**–ù–∞—Å—Ç—Ä–æ–π–∫–∏:**
```python
lora_config = {
    "r": 64,  # Rank (16-128, –±–æ–ª—å—à–µ = —Ç–æ—á–Ω–µ–µ –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
    "lora_alpha": 128,  # Scaling factor (–æ–±—ã—á–Ω–æ 2*r)
    "lora_dropout": 0.1,  # Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"]
}
```

### Learning Rate:

```python
# –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
learning_rates = {
    "full_finetuning": 1e-5,  # –û—á–µ–Ω—å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ
    "lora": 2e-4,  # –ú–æ–∂–Ω–æ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–µ–µ
    "qlora": 5e-4  # Quantized LoRA
}
```

### Batch Size:

```python
# –ü–æ–¥–±–æ—Ä –ø–æ–¥ –≤–∞—à—É GPU
batch_sizes = {
    "24GB VRAM": 4,  # RTX 3090, 4090
    "40GB VRAM": 8,  # A100 40GB
    "80GB VRAM": 16  # A100 80GB
}

# Gradient accumulation –¥–ª—è —ç–º—É–ª—è—Ü–∏–∏ –±–æ–ª—å—à–µ–≥–æ batch
effective_batch = batch_size * gradient_accumulation_steps
# –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: 32-64
```

---

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è

### –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è:

1. **Loss** (–æ—Å–Ω–æ–≤–Ω–æ–µ)
   - Train loss –¥–æ–ª–∂–Ω–∞ –ø–∞–¥–∞—Ç—å
   - Val loss –¥–æ–ª–∂–Ω–∞ –ø–∞–¥–∞—Ç—å (–∏–ª–∏ stabilize)
   - –ï—Å–ª–∏ val loss —Ä–∞—Å—Ç–µ—Ç ‚Üí overfitting

2. **Perplexity**
   - –î–æ–ª–∂–Ω–∞ —Å–Ω–∏–∂–∞—Ç—å—Å—è
   - < 10 = –æ—Ç–ª–∏—á–Ω–æ
   - < 20 = —Ö–æ—Ä–æ—à–æ
   - > 50 = –ø–ª–æ—Ö–æ

3. **Code Quality** (–∫–∞—Å—Ç–æ–º–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
   - –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å
   - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å
   - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ best practices

### TensorBoard:

```bash
tensorboard --logdir models/qwen3-bsl-lora/logs

# Metrics:
# - train/loss
# - val/loss
# - train/perplexity
# - val/perplexity
# - learning_rate
# - grad_norm
```

---

## ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏

### –ü–æ—Å–ª–µ fine-tuning:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
model = AutoModelForCausalLM.from_pretrained("models/qwen3-bsl-lora")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Coder-32B-Instruct")

# –¢–µ—Å—Ç
prompt = "–°–æ–∑–¥–∞–π —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å–∫–∏–¥–∫–∏ –ø–æ –æ–±—ä–µ–º—É"
result = model.generate(prompt)

print(result)
```

### –¢–µ—Å—Ç–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏:

1. **–ë–∞–∑–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è**
   - –ü—Ä–æ—Å—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (CRUD)
   - –ü—Ä–æ—Ü–µ–¥—É—Ä—ã
   - –ó–∞–ø—Ä–æ—Å—ã

2. **–ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞**
   - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º BSL
   - –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
   - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

3. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**
   - –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–¥–∞
   - –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
   - –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ N+1 –∑–∞–ø—Ä–æ—Å–æ–≤

4. **–°–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏**
   - –ë–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞
   - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
   - Multi-step —Ä–µ—à–µ–Ω–∏—è

---

## üîÑ –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ

### –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è:

```
1. –°–æ–±—Ä–∞—Ç—å dataset ‚Üí Fine-tune
    ‚Üì
2. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö
    ‚Üì
3. –°–æ–±—Ä–∞—Ç—å feedback (—á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–ª–æ—Ö–æ)
    ‚Üì
4. –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –≤ dataset
    ‚Üì
5. –ü–æ–≤—Ç–æ—Ä–∏—Ç—å fine-tuning
```

### –°–±–æ—Ä feedback:

```python
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–æ—Ö–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤
{
  "user_query": "–°–æ–∑–¥–∞–π —Ñ—É–Ω–∫—Ü–∏—é...",
  "generated_code": "...",
  "user_rating": 2,  # 1-5
  "issues": ["–ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫", "–ù–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å"],
  "timestamp": "2024-11-05T12:00:00Z"
}

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ dataset –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
```

---

## üí° Best Practices

### 1. Quality > Quantity

- –õ—É—á—à–µ 500 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —á–µ–º 5000 –ø–ª–æ—Ö–∏—Ö
- –ö–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—Ä–æ–≤–µ—Ä–µ–Ω —ç–∫—Å–ø–µ—Ä—Ç–æ–º
- –£–¥–∞–ª—è—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–∏–º–µ—Ä—ã

### 2. Diversity

- –†–∞–∑–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–¥–∞—á
- –†–∞–∑–Ω—ã–µ —Å—Ç–∏–ª–∏ –∫–æ–¥–∞
- –†–∞–∑–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
- –†–∞–∑–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (DO, ERP, ZUP)

### 3. Documentation

- –ö–∞–∂–¥—ã–π example —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏
- –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ—á–µ–º—É –∫–æ–¥ —Ö–æ—Ä–æ—à–∏–π
- –û–ø–∏—Å–∞–Ω–∏–µ best practices

### 4. Validation

- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ real-world –∑–∞–¥–∞—á–∞—Ö
- A/B testing —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
- User feedback

---

## üì¶ –ì–æ—Ç–æ–≤—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö

### –ü—É–±–ª–∏—á–Ω—ã–µ BSL datasets:

1. **1C SSL Examples**
   - –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø–æ–¥—Å–∏—Å—Ç–µ–º
   - 1000+ —Ñ—É–Ω–∫—Ü–∏–π
   - –•–æ—Ä–æ—à–æ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ

2. **–ò–¢–° Examples**
   - –ü—Ä–∏–º–µ—Ä—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
   - –†–µ–∞–ª—å–Ω—ã–µ –∫–µ–π—Å—ã
   - Best practices

3. **Community Examples**
   - GitHub projects
   - Infostart code snippets
   - Stack Overflow (1c tag)

---

## üöÄ Deployment

### –ü–æ—Å–ª–µ fine-tuning:

1. **Quantization** (–¥–ª—è production)
```bash
# 8-bit quantization –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
python scripts/quantize_model.py \
    --model models/qwen3-bsl-lora \
    --output models/qwen3-bsl-8bit
```

2. **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ AI Orchestrator**
```python
# src/ai/orchestrator.py

# –î–æ–±–∞–≤–∏—Ç—å fine-tuned –º–æ–¥–µ–ª—å
QWEN_BSL_MODEL = "models/qwen3-bsl-lora"

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è code generation
if query_type == "code_generation":
    model = QWEN_BSL_MODEL  # Instead of base Qwen3
```

3. **A/B Testing**
```python
# –°—Ä–∞–≤–Ω–∏—Ç—å base vs fine-tuned
results = ab_test(
    queries=test_queries,
    model_a="Qwen/Qwen2.5-Coder-32B",
    model_b="models/qwen3-bsl-lora"
)

# Metrics: code quality, user rating, execution correctness
```

---

## üìà –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è

### –ü–æ—Å–ª–µ fine-tuning:

| Metric | Base Model | Fine-tuned | Improvement |
|--------|------------|------------|-------------|
| **–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å** | 85% | 98% | +15% |
| **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å** | 70% | 90% | +29% |
| **Best practices** | 60% | 85% | +42% |
| **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∫–æ–¥–∞** | 40% | 95% | +138% |
| **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** | 50% | 80% | +60% |
| **User satisfaction** | 3.5/5 | 4.5/5 | +29% |

---

## üîÆ Roadmap

### Q1 2025:
- [x] Dataset preparation (500 examples)
- [ ] Fine-tune –±–∞–∑–æ–≤—ã–π Qwen3-BSL
- [ ] A/B testing
- [ ] Integration –≤ production

### Q2 2025:
- [ ] –†–∞—Å—à–∏—Ä–∏—Ç—å dataset –¥–æ 2000+ examples
- [ ] Fine-tune –Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏–∫–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π (DO, ERP)
- [ ] Multi-task learning (code + docs + optimization)
- [ ] Community contributions

### Q3 2025:
- [ ] Dataset 5000+ examples
- [ ] Specialized models –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
- [ ] Continuous learning from user feedback
- [ ] State-of-the-art BSL generation

---

## üìû –ü–æ–º–æ—â—å –∏ –≤–æ–ø—Ä–æ—Å—ã

**–ù—É–∂–Ω–∞ –ø–æ–º–æ—â—å —Å fine-tuning?**

- GitHub: [Issues](https://github.com/DmitrL-dev/1cai-public/issues) —Å —Ç–µ–≥–æ–º `ml`
- Discussions: [GitHub Discussions](https://github.com/DmitrL-dev/1cai-public/discussions)

**–•–æ—Ç–∏—Ç–µ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è –ø—Ä–∏–º–µ—Ä–∞–º–∏?**

–°–æ–∑–¥–∞–π—Ç–µ PR —Å –≤–∞—à–∏–º–∏ examples –≤:
```
datasets/community/your_username/
```

**Reward:** Premium –ø–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ 1 –≥–æ–¥ + contributor badge

---

**–í–µ—Ä—Å–∏—è:** 1.0  
**–î–∞—Ç–∞:** 2024-11-05  
**–°—Ç–∞—Ç—É—Å:** üöß Dataset –≥–æ—Ç–æ–≤, fine-tuning –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ

