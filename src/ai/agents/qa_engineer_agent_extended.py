"""
QA Engineer AI Agent Extended
AI ассистент для тестировщиков с полным функционалом
"""

import os
import re
import json
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from pathlib import Path
import random

logger = logging.getLogger(__name__)


class SmartTestGenerator:
    """AI-powered генератор тестов"""
    
    def __init__(self):
        self.qwen_client = None  # TODO: Integration with Qwen3-Coder
        self.test_templates = self._load_test_templates()
    
    def _load_test_templates(self) -> Dict:
        """Шаблоны тестов"""
        return {
            "unit_test_bsl": """
// Тест: {test_name}
// Модуль: {module_name}

Процедура {test_name}() Экспорт
    
    // Arrange (подготовка)
    {arrange_code}
    
    // Act (действие)
    {act_code}
    
    // Assert (проверка)
    {assert_code}
    
КонецПроцедуры
""",
            "vanessa_bdd": """
# language: ru

Функционал: {feature_name}
    Как {actor}
    Я хочу {action}
    Чтобы {business_value}

Контекст:
    Дано {context}

Сценарий: {scenario_name}
    Когда {when_step}
    Тогда {then_step}
    И {and_step}
""",
            "negative_test": """
Процедура {test_name}_НегативныйТест() Экспорт
    
    // Тест на обработку {error_case}
    
    ОжидаемаяОшибка = "{error_message}";
    
    Попытка
        {code_that_should_fail}
        ВызватьИсключение("Ожидалась ошибка, но код выполнился успешно");
    Исключение
        Если НЕ СтрНайти(ОписаниеОшибки(), ОжидаемаяОшибка) > 0 Тогда
            ВызватьИсключение("Получена неожиданная ошибка: " + ОписаниеОшибки());
        КонецЕсли;
    КонецПопытки;
    
КонецПроцедуры
"""
        }
    
    async def generate_tests_for_function(
        self,
        function_code: str,
        function_name: str,
        module_type: str = "common_module"
    ) -> Dict[str, Any]:
        """
        AI генерация тестов для функции
        
        Args:
            function_code: Код функции BSL
            function_name: Название функции
            module_type: Тип модуля (common_module, server, client)
        
        Returns:
            {
                "unit_tests": [...],
                "vanessa_bdd": "...",
                "edge_cases": [...],
                "negative_tests": [...],
                "coverage_estimate": "85%"
            }
        """
        logger.info(f"Generating tests for function: {function_name}")
        
        # Analyze function
        params = self._extract_parameters(function_code)
        return_type = self._detect_return_type(function_code)
        complexity = self._calculate_complexity(function_code)
        
        # Generate unit tests
        unit_tests = []
        
        # Normal case
        unit_tests.append(self._generate_unit_test(
            test_name=f"Тест_{function_name}_НормальныйСценарий",
            module_name=function_name,
            arrange="// Подготовка нормальных данных",
            act=f"Результат = {function_name}({', '.join([p['name'] for p in params])});",
            assert_code="// Проверка корректности результата"
        ))
        
        # Edge cases
        edge_cases = []
        if any(p['type'] == 'string' for p in params):
            edge_cases.append({
                "case": "Пустая строка",
                "test_name": f"Тест_{function_name}_ПустаяСтрока"
            })
        if any(p['type'] == 'number' for p in params):
            edge_cases.append({
                "case": "Ноль",
                "test_name": f"Тест_{function_name}_Ноль"
            })
            edge_cases.append({
                "case": "Отрицательное число",
                "test_name": f"Тест_{function_name}_ОтрицательноеЧисло"
            })
        if any(p['type'] == 'array' for p in params):
            edge_cases.append({
                "case": "Пустой массив",
                "test_name": f"Тест_{function_name}_ПустойМассив"
            })
        
        # Negative tests
        negative_tests = []
        if params:
            negative_tests.append({
                "name": f"Тест_{function_name}_НекорректныйТип",
                "error_case": "некорректный тип параметра",
                "error_message": "Неверный тип параметра"
            })
        
        # Vanessa BDD
        vanessa_bdd = self._generate_vanessa_scenario(function_name, params)
        
        # Coverage estimate
        test_count = len(unit_tests) + len(edge_cases) + len(negative_tests)
        coverage_estimate = min(50 + test_count * 10, 95)
        
        return {
            "function_name": function_name,
            "unit_tests": unit_tests,
            "vanessa_bdd": vanessa_bdd,
            "edge_cases": edge_cases,
            "negative_tests": negative_tests,
            "coverage_estimate": f"{coverage_estimate}%",
            "test_count": test_count,
            "complexity": complexity
        }
    
    def _extract_parameters(self, code: str) -> List[Dict]:
        """Извлечение параметров функции"""
        # Simplified parameter extraction
        params = []
        
        # Match function signature
        func_match = re.search(r'Функция\s+\w+\s*\((.*?)\)', code, re.IGNORECASE | re.DOTALL)
        if func_match:
            params_str = func_match.group(1)
            for param in params_str.split(','):
                param = param.strip()
                if param:
                    # Detect type from name
                    param_type = "any"
                    if "Строка" in param or "Текст" in param:
                        param_type = "string"
                    elif "Число" in param or "Количество" in param or "Сумма" in param:
                        param_type = "number"
                    elif "Массив" in param or "Список" in param:
                        param_type = "array"
                    elif "Булево" in param or "Флаг" in param:
                        param_type = "boolean"
                    
                    params.append({
                        "name": param.split('=')[0].strip(),
                        "type": param_type
                    })
        
        return params
    
    def _detect_return_type(self, code: str) -> str:
        """Определение типа возвращаемого значения"""
        if re.search(r'Возврат\s+\d+', code):
            return "number"
        elif re.search(r'Возврат\s+"', code):
            return "string"
        elif re.search(r'Возврат\s+(Истина|Ложь)', code, re.IGNORECASE):
            return "boolean"
        else:
            return "any"
    
    def _calculate_complexity(self, code: str) -> int:
        """Расчет цикломатической сложности"""
        complexity = 1  # Base complexity
        
        # Count decision points
        complexity += len(re.findall(r'\bЕсли\b', code, re.IGNORECASE))
        complexity += len(re.findall(r'\bИначеЕсли\b', code, re.IGNORECASE))
        complexity += len(re.findall(r'\bДля\b', code, re.IGNORECASE))
        complexity += len(re.findall(r'\bПока\b', code, re.IGNORECASE))
        complexity += len(re.findall(r'\bПопытка\b', code, re.IGNORECASE))
        
        return complexity
    
    def _generate_unit_test(
        self,
        test_name: str,
        module_name: str,
        arrange: str,
        act: str,
        assert_code: str
    ) -> str:
        """Генерация unit теста"""
        return self.test_templates["unit_test_bsl"].format(
            test_name=test_name,
            module_name=module_name,
            arrange_code=arrange,
            act_code=act,
            assert_code=assert_code
        )
    
    def _generate_vanessa_scenario(self, function_name: str, params: List[Dict]) -> str:
        """Генерация Vanessa BDD сценария"""
        return self.test_templates["vanessa_bdd"].format(
            feature_name=f"Тестирование функции {function_name}",
            actor="QA инженер",
            action=f"протестировать функцию {function_name}",
            business_value="убедиться в корректной работе",
            context="я запускаю сценарий открытия TestClient",
            scenario_name=f"Базовый тест {function_name}",
            when_step=f"я вызываю функцию {function_name}",
            then_step="функция выполняется без ошибок",
            and_step="результат соответствует ожиданиям"
        )


class TestCoverageAnalyzer:
    """Анализатор покрытия тестами (интеграция с SonarQube/Vanessa)"""
    
    def __init__(self):
        self.sonar_api_url = os.getenv("SONARQUBE_URL", "http://localhost:9000")
        self.sonar_token = os.getenv("SONARQUBE_TOKEN", "")
    
    async def analyze_coverage(
        self,
        config_name: str,
        test_results: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        Реальный анализ покрытия тестами
        
        Args:
            config_name: Название конфигурации
            test_results: Результаты выполнения тестов (от Vanessa или другого инструмента)
        
        Returns:
            Детальный анализ покрытия
        """
        logger.info(f"Analyzing test coverage for: {config_name}")
        
        # Mock coverage data (в реальности - из SonarQube API)
        coverage_data = {
            "overall_coverage": 0.72,  # 72%
            "by_module": {
                "ПродажиСервер": 0.85,
                "СкладСервер": 0.65,
                "БухгалтерияСервер": 0.78,
                "ОбщегоНазначения": 0.55,
                "РаботаСФайлами": 0.40
            },
            "by_type": {
                "server_modules": 0.75,
                "common_modules": 0.60,
                "form_modules": 0.45
            },
            "uncovered_functions": [
                {
                    "function": "РассчитатьСложнуюСумму",
                    "module": "ПродажиСервер",
                    "complexity": 15,
                    "lines": 45,
                    "priority": "high"
                },
                {
                    "function": "ПроверитьДоступКФайлу",
                    "module": "РаботаСФайлами",
                    "complexity": 8,
                    "lines": 20,
                    "priority": "medium"
                }
            ]
        }
        
        # Calculate test gaps
        test_gaps = []
        if coverage_data["overall_coverage"] < 0.8:
            test_gaps.append("Общее покрытие ниже 80%")
        
        for module, coverage in coverage_data["by_module"].items():
            if coverage < 0.7:
                test_gaps.append(f"Модуль {module}: покрытие {coverage:.0%} < 70%")
        
        # Recommendations
        recommendations = self._generate_coverage_recommendations(coverage_data)
        
        # Priority functions to test
        priority_functions = sorted(
            coverage_data["uncovered_functions"],
            key=lambda x: (x["priority"] == "high", x["complexity"]),
            reverse=True
        )
        
        return {
            "config_name": config_name,
            "overall_coverage": coverage_data["overall_coverage"],
            "coverage_grade": self._get_coverage_grade(coverage_data["overall_coverage"]),
            "by_module": coverage_data["by_module"],
            "by_type": coverage_data["by_type"],
            "uncovered_functions_count": len(coverage_data["uncovered_functions"]),
            "priority_functions": priority_functions[:10],
            "test_gaps": test_gaps,
            "recommendations": recommendations,
            "timestamp": datetime.now().isoformat()
        }
    
    def _get_coverage_grade(self, coverage: float) -> str:
        """Оценка покрытия"""
        if coverage >= 0.9:
            return "A (Excellent)"
        elif coverage >= 0.8:
            return "B (Good)"
        elif coverage >= 0.7:
            return "C (Acceptable)"
        elif coverage >= 0.6:
            return "D (Poor)"
        else:
            return "F (Critical)"
    
    def _generate_coverage_recommendations(self, coverage_data: Dict) -> List[str]:
        """Генерация рекомендаций"""
        recommendations = []
        
        overall = coverage_data["overall_coverage"]
        if overall < 0.8:
            missing_tests = int((0.8 - overall) * 100)
            recommendations.append(
                f"Добавить ~{missing_tests} тестов для достижения 80% покрытия"
            )
        
        # Uncovered functions
        high_priority_count = sum(
            1 for f in coverage_data["uncovered_functions"]
            if f["priority"] == "high"
        )
        if high_priority_count > 0:
            recommendations.append(
                f"Протестировать {high_priority_count} критичных функций без покрытия"
            )
        
        # Low coverage modules
        low_coverage_modules = [
            module for module, cov in coverage_data["by_module"].items()
            if cov < 0.6
        ]
        if low_coverage_modules:
            recommendations.append(
                f"Усилить тестирование модулей: {', '.join(low_coverage_modules)}"
            )
        
        return recommendations


class BugPatternAnalyzer:
    """Анализатор паттернов багов (ML)"""
    
    def __init__(self):
        self.bug_categories = [
            "null_pointer",
            "boundary_error",
            "concurrency",
            "logic_error",
            "integration_error",
            "performance",
            "security"
        ]
    
    async def analyze_bug_patterns(
        self,
        bug_history: List[Dict]
    ) -> Dict[str, Any]:
        """
        Анализ паттернов багов с ML
        
        Args:
            bug_history: [
                {
                    "id": "BUG-123",
                    "module": "ПродажиСервер",
                    "function": "РассчитатьСумму",
                    "category": "null_pointer",
                    "severity": "high",
                    "fixed_date": "2025-10-15"
                }
            ]
        
        Returns:
            Анализ с hotspots и предсказаниями
        """
        logger.info("Analyzing bug patterns")
        
        # Group by module
        bugs_by_module = {}
        for bug in bug_history:
            module = bug.get("module", "Unknown")
            if module not in bugs_by_module:
                bugs_by_module[module] = []
            bugs_by_module[module].append(bug)
        
        # Find hotspots
        hotspots = []
        for module, bugs in bugs_by_module.items():
            if len(bugs) > 5:  # Threshold
                hotspots.append({
                    "module": module,
                    "bug_count": len(bugs),
                    "bug_density": len(bugs) / 1000,  # Simplified: per 1000 lines
                    "predicted_bugs": int(len(bugs) * 0.3),  # Simplified prediction
                    "recommendation": "Рефакторинг + больше unit тестов",
                    "risk_score": min(len(bugs) / 10, 1.0)
                })
        
        # Common patterns
        bug_counts_by_category = {}
        for bug in bug_history:
            category = bug.get("category", "unknown")
            bug_counts_by_category[category] = bug_counts_by_category.get(category, 0) + 1
        
        total_bugs = len(bug_history)
        common_patterns = [
            {
                "pattern": category,
                "count": count,
                "percentage": int((count / total_bugs) * 100) if total_bugs > 0 else 0
            }
            for category, count in sorted(
                bug_counts_by_category.items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]
        ]
        
        # Risk prediction
        risk_predictions = []
        for module in bugs_by_module.keys():
            bug_rate = len(bugs_by_module[module]) / max(len(bug_history), 1)
            if bug_rate > 0.15:  # > 15% of all bugs
                risk_predictions.append({
                    "area": module,
                    "risk_score": min(bug_rate * 5, 1.0),
                    "recommended_action": "Добавить integration tests и code review"
                })
        
        return {
            "total_bugs_analyzed": total_bugs,
            "hotspots": sorted(hotspots, key=lambda x: x["bug_count"], reverse=True),
            "common_patterns": common_patterns,
            "risk_predictions": risk_predictions,
            "recommendations": self._generate_bug_recommendations(hotspots, common_patterns),
            "timestamp": datetime.now().isoformat()
        }
    
    def _generate_bug_recommendations(
        self,
        hotspots: List[Dict],
        patterns: List[Dict]
    ) -> List[str]:
        """Генерация рекомендаций"""
        recommendations = []
        
        if hotspots:
            top_hotspot = hotspots[0]
            recommendations.append(
                f"Критично: модуль {top_hotspot['module']} - {top_hotspot['bug_count']} багов. Рекомендуется рефакторинг."
            )
        
        if patterns:
            top_pattern = patterns[0]
            recommendations.append(
                f"Самый частый тип багов: {top_pattern['pattern']} ({top_pattern['percentage']}%). Добавить специфичные тесты."
            )
        
        recommendations.append(
            "Внедрить статический анализ кода (SonarQube, BSL LS)"
        )
        
        return recommendations


class PerformanceTestGenerator:
    """Генератор performance тестов (K6, JMeter)"""
    
    async def generate_k6_test(
        self,
        api_endpoints: List[str],
        load_profile: Dict
    ) -> Dict[str, Any]:
        """
        Генерация K6 performance теста
        
        Args:
            api_endpoints: ["/api/orders", "/api/products"]
            load_profile: {
                "users": 1000,
                "duration": "30m",
                "ramp_up": "5m"
            }
        
        Returns:
            {
                "k6_script": "...",
                "expected_metrics": {...}
            }
        """
        users = load_profile.get("users", 100)
        duration = load_profile.get("duration", "10m")
        ramp_up = load_profile.get("ramp_up", "2m")
        
        k6_script = f"""
import http from 'k6/http';
import {{ check, sleep }} from 'k6';

export let options = {{
  stages: [
    {{ duration: '{ramp_up}', target: {users} }}, // Ramp-up
    {{ duration: '{duration}', target: {users} }}, // Stay at peak
    {{ duration: '2m', target: 0 }}, // Ramp-down
  ],
  thresholds: {{
    'http_req_duration': ['p(95)<500'], // 95% of requests < 500ms
    'http_req_failed': ['rate<0.01'],   // Error rate < 1%
  }},
}};

export default function () {{
  // Test endpoints
"""
        
        for endpoint in api_endpoints:
            k6_script += f"""
  let res{api_endpoints.index(endpoint)} = http.get('{{{{__ENV.BASE_URL}}}}{endpoint}');
  check(res{api_endpoints.index(endpoint)}, {{
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  }});
"""
        
        k6_script += """
  sleep(1);
}
"""
        
        return {
            "k6_script": k6_script,
            "load_profile": load_profile,
            "expected_metrics": {
                "rps": users / 2,  # Simplified
                "response_time_p95": "< 500ms",
                "error_rate": "< 1%",
                "concurrent_users": users
            },
            "run_command": f"k6 run --vus {users} --duration {duration} test.js"
        }


class QAEngineerAgentExtended:
    """
    Расширенный QA Engineer AI ассистент
    
    Возможности:
    - Smart Test Generation (AI)
    - Real Coverage Analysis (SonarQube/Vanessa)
    - Bug Pattern Analysis (ML)
    - Performance Test Generation (K6/JMeter)
    """
    
    def __init__(self):
        self.test_generator = SmartTestGenerator()
        self.coverage_analyzer = TestCoverageAnalyzer()
        self.bug_analyzer = BugPatternAnalyzer()
        self.perf_generator = PerformanceTestGenerator()
        
        logger.info("QA Engineer Agent Extended initialized")
    
    async def generate_tests(
        self,
        function_code: str,
        function_name: str
    ) -> Dict[str, Any]:
        """AI генерация тестов для функции"""
        return await self.test_generator.generate_tests_for_function(
            function_code,
            function_name
        )
    
    async def analyze_coverage(
        self,
        config_name: str
    ) -> Dict[str, Any]:
        """Анализ покрытия тестами"""
        return await self.coverage_analyzer.analyze_coverage(config_name)
    
    async def analyze_bugs(
        self,
        bug_history: List[Dict]
    ) -> Dict[str, Any]:
        """Анализ паттернов багов"""
        return await self.bug_analyzer.analyze_bug_patterns(bug_history)
    
    async def generate_performance_test(
        self,
        endpoints: List[str],
        load_profile: Dict
    ) -> Dict[str, Any]:
        """Генерация performance теста"""
        return await self.perf_generator.generate_k6_test(endpoints, load_profile)


